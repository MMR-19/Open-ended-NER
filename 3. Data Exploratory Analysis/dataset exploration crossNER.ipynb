{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore CrossNER dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "raw_datasets = {\n",
    "    \"conll\": load_dataset(\"...\"),\n",
    "    \"politics\": load_dataset(\"...\"),\n",
    "    \"science\": load_dataset(\"...\"),\n",
    "    \"music\": load_dataset(\"...\"),\n",
    "    \"literature\": load_dataset(\"...\"),\n",
    "    \"ai\": load_dataset(\"...\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the folder to the Python path\n",
    "sys.path.append(os.path.abspath(\"../0. Helpers\"))\n",
    "sys.path.append(os.path.abspath(\"../2. Data Processing/_dataset_entities\"))\n",
    "\n",
    "# Import libraries\n",
    "from datasetProcessing import tokens_to_sentence, tokens_to_entities\n",
    "from entities_crossNER import entity_names, entity_names_parsed\n",
    "\n",
    "# get the entity names\n",
    "start_of_entity_indices = [i for i in range(len(entity_names)) if (entity_names[i].startswith(\"B-\") or entity_names[i].startswith(\"U-\"))]\n",
    "entity_index_to_name = {i: entity_names[i].split(\"-\")[1] for i in range(len(entity_names)) if entity_names[i] != \"O\"}\n",
    "entity_index_to_name[0] = \"O\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entity processing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_names = raw_datasets[\"conll\"][\"train\"].features[\"ner_tags\"].feature.names\n",
    "\n",
    "def extract_actual_entities(indices):\n",
    "    # get entity names for indices\n",
    "    ind_entities = [entity_names[i] for i in indices]\n",
    "    \n",
    "    # remove B- and I- prefixes\n",
    "    no_prefix = [entity[2:] if entity.startswith(\"B-\") or entity.startswith(\"I-\") else entity for entity in ind_entities]\n",
    "\n",
    "    # return unique\n",
    "    return sorted(list(set(no_prefix)))\n",
    "\n",
    "print(extract_actual_entities(range(len(entity_names))))\n",
    "print(len(extract_actual_entities(range(len(entity_names)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get metrics for each subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    sentences = 0\n",
    "    tokens = 0\n",
    "    entities = 0\n",
    "\n",
    "    def __init__(self, sentences, tokens, entities):\n",
    "        self.sentences = sentences\n",
    "        self.tokens = tokens\n",
    "        self.entities = entities\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.sentences} sentences, {self.tokens} tokens, {self.entities} different entities\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_datasets = {}\n",
    "for key in raw_datasets.keys():\n",
    "    metrics_datasets[key] = {}\n",
    "\n",
    "for key, dataset in raw_datasets.items():\n",
    "\n",
    "    total_examples = 0\n",
    "    total_tokens = 0\n",
    "    total_entity_list = []\n",
    "\n",
    "    for split in dataset.keys():\n",
    "        \n",
    "        # examples\n",
    "        split_instances = dataset[split].num_rows\n",
    "        total_examples += split_instances\n",
    "\n",
    "        split_tokens = 0\n",
    "        split_entity_list = []\n",
    "\n",
    "        for instance in dataset[split]:\n",
    "\n",
    "            split_tokens += len(instance['tokens'])\n",
    "            split_entity_list += instance['ner_tags']\n",
    "        \n",
    "        # tokens\n",
    "        total_tokens += split_tokens\n",
    "\n",
    "        # entities\n",
    "        total_entity_list += split_entity_list\n",
    "        split_entities_count = int((len(list(set(split_entity_list))) - 1)/2)\n",
    "\n",
    "        # add split metrics\n",
    "        metrics_datasets[key][split] = Metrics(split_instances, split_tokens, split_entities_count)\n",
    "\n",
    "    # add total metrics\n",
    "    total_entities_count = int((len(list(set(total_entity_list))) - 1)/2)\n",
    "    metrics_datasets[key][\"total\"] = Metrics(total_examples, total_tokens, total_entities_count)\n",
    "\n",
    "    # add set of all entities\n",
    "    metrics_datasets[key][\"entities\"] = extract_actual_entities(list(set(total_entity_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the header with fixed-width columns\n",
    "print(f\"{'Dataset':<12} {'Split':<15} {'Sentences':<12} {'Tokens':<12} {'Entities':<12} {'Avg Tokens/Sentence':<12}\")\n",
    "print()\n",
    "\n",
    "# Print the table rows with fixed-width columns\n",
    "for key, dataset in metrics_datasets.items():\n",
    "    for split, metrics in dataset.items():\n",
    "        if split != \"entities\" and split != \"total\":\n",
    "            avg_tokens_per_sentence = metrics.tokens / metrics.sentences if metrics.sentences > 0 else 0\n",
    "            print(f\"{key:<12} {split:<15} {metrics.sentences:<12} {metrics.tokens:<12} {metrics.entities:<12} {round(avg_tokens_per_sentence,1):<12}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tokens per Entity\")\n",
    "\n",
    "for key, dataset in raw_datasets.items():\n",
    "\n",
    "    entity_length = []\n",
    "\n",
    "    for instance in dataset['train']:\n",
    "        \n",
    "        tokens = instance['tokens']\n",
    "        ner_tags = instance['ner_tags']\n",
    "        entities = tokens_to_entities(tokens, ner_tags, entity_names_parsed, start_of_entity_indices, entity_index_to_name)\n",
    "\n",
    "        entity_length.extend([len(entity.tokens) for entity in entities])\n",
    "    \n",
    "    min1 = np.min(entity_length)\n",
    "    max2 = np.max(entity_length)\n",
    "    avg = np.mean(entity_length)\n",
    "\n",
    "    print(f\"{key}: min={min1}, avg={round(avg,2)}, max={max2}\")\n",
    "\n",
    "    # save as file\n",
    "    np.save(f'entity_length_crossner_{key}.npy', np.array(entity_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Entities for each dataset\")\n",
    "\n",
    "for key, dataset in metrics_datasets.items():\n",
    "    if \"entities\" in dataset:\n",
    "        print(f\"{key:<12} {dataset['entities']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entities distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"ai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def count_entities(dataset_split):\n",
    "    class_spans = defaultdict(int)\n",
    "    class_instances = defaultdict(int)\n",
    "\n",
    "    for instance in dataset_split:\n",
    "        tokens = instance['tokens']\n",
    "        ner_tags = instance['ner_tags']\n",
    "        entities = tokens_to_entities(tokens, ner_tags, entity_names_parsed, start_of_entity_indices, entity_index_to_name)\n",
    "        \n",
    "        instance_entity_classes = set()\n",
    "        for ent in entities:\n",
    "            class_spans[ent.entity] += 1\n",
    "            instance_entity_classes.add(ent.entity)\n",
    "\n",
    "        for ent_class in instance_entity_classes:\n",
    "            class_instances[ent_class] += 1\n",
    "\n",
    "    all_entities = list(set(list(class_spans.keys()) + list(class_instances.keys())))\n",
    "    all_entities.sort()\n",
    "\n",
    "    return all_entities, dict(class_spans), dict(class_instances)\n",
    "\n",
    "train_entities, train_class_spans, train_class_instances = count_entities(raw_datasets[topic]['train'])\n",
    "test_entities, test_class_spans, test_class_instances = count_entities(raw_datasets[topic]['test'])\n",
    "\n",
    "all_entities = list(set(train_entities + test_entities))\n",
    "all_entities.sort()\n",
    "\n",
    "for ent in all_entities:\n",
    "    train_instances = train_class_instances.get(ent, 0)\n",
    "    test_instances = test_class_instances.get(ent, 0)\n",
    "    train_spans = train_class_spans.get(ent, 0)\n",
    "    test_spans = test_class_spans.get(ent, 0)\n",
    "    print(f\"{ent:<21} &  & {train_instances:<15} & {train_spans:<11} &  & {test_instances:<14} & {test_spans:<10} \\\\\\\\\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find examples with specific criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# except_entities = [\"Organisation\", \"Product\", \"Person\", \"Misc\", \"Algorithm\", \"Field\", \"Task\"]\n",
    "# contains_entities = ['Country', 'Event', 'Person', 'Protein']\n",
    "\n",
    "criteria = []\n",
    "for i, instance in enumerate(raw_datasets[topic][\"test\"]):\n",
    "    true_entities = tokens_to_entities(instance['tokens'], instance['ner_tags'], entity_names_parsed, start_of_entity_indices, entity_index_to_name)\n",
    "    sentence = tokens_to_sentence(instance['tokens'])\n",
    "    criterium = {\n",
    "        \"index\": i,\n",
    "        \"sentence\": sentence,\n",
    "        \"total_count\": len(true_entities),\n",
    "        \"unique_count\": len(set([entity.entity for entity in true_entities])),\n",
    "        # \"except\": all(entity.entity not in except_entities for entity in true_entities),\n",
    "        # \"contains_any\": any(entity.entity in contains_entities for entity in true_entities),\n",
    "        # \"contains_strict\": all(name in [entity.entity for entity in true_entities] for name in contains_entities),\n",
    "        # \"contains_unique_count\": len(set(name for name in contains_entities if name in [entity.entity for entity in true_entities])),\n",
    "        \"sentence_contains\": \"genetic algorithm\" in sentence\n",
    "    }\n",
    "\n",
    "    criteria.append(criterium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for crit in criteria:\n",
    "    # if crit[\"unique\"] == 1 and crit[\"total\"] == 2 and crit[\"except\"] and len(crit[\"sentence\"]) < 150:\n",
    "    if crit[\"sentence_contains\"]:\n",
    "        print(f\"index: {crit['index']}, Total: {crit['total_count']}, Unique: {crit['unique_count']}, Sentence Length: {len(crit['sentence'])}\")\n",
    "        print(\"                                                              \", crit[\"sentence\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process specific examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ai: instances = [(\"validation\", 7), (\"validation\", 12), (\"train\", 1), (\"train\", 15), (\"train\", 58)]\n",
    "# literature: instances = [(\"validation\", 5), (\"validation\", 13), (\"train\", 57), (\"train\", 90), (\"train\", 75)]\n",
    "# music: instances = [(\"validation\", 17), (\"validation\", 19), (\"train\", 30), (\"train\", 54), (\"train\", 20)]\n",
    "# politics: instances = [(\"validation\", 2), (\"validation\", 3), (\"train\", 111), (\"train\", 136), (\"train\", 83)]\n",
    "# science: instances = [(\"validation\", 3), (\"validation\", 5), (\"train\", 7), (\"train\", 90), (\"train\", 86)]\n",
    "instances = [(\"validation\", 3), (\"validation\", 5), (\"train\", 7), (\"train\", 90), (\"train\", 86)]\n",
    "\n",
    "for idx, instance_idx in enumerate(instances):\n",
    "    instance = raw_datasets[topic][instance_idx[0]][instance_idx[1]]\n",
    "    sentence = tokens_to_sentence(instance['tokens'])\n",
    "    true_entities = tokens_to_entities(instance['tokens'], instance['ner_tags'], entity_names_parsed, start_of_entity_indices, entity_index_to_name)\n",
    "    \n",
    "    entities_str = \", \".join([f\"{{'span': '{entity.span}', 'entity': '{entity.entity}'}}\" for entity in true_entities])\n",
    "    print(f\"Example #{idx + 1}: {sentence}\")\n",
    "    print(f\"Expected output: 'entities: [{entities_str}]'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inconsitencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = [(\"test\", 114)]\n",
    "\n",
    "for idx, instance_idx in enumerate(instances):\n",
    "    instance = raw_datasets[topic][instance_idx[0]][instance_idx[1]]\n",
    "    sentence = tokens_to_sentence(instance['tokens'])\n",
    "    true_entities = tokens_to_entities(instance['tokens'], instance['ner_tags'], entity_names_parsed, start_of_entity_indices, entity_index_to_name)\n",
    "    \n",
    "    entities_str = \", \".join([f\"{{'span': '{entity.span}', 'entity': '{entity.entity}'}}\" for entity in true_entities])\n",
    "    print(f\"Example #{idx + 1}: {sentence}\")\n",
    "    print(f\"Example #{idx + 1}: {instance['tokens']}\")\n",
    "    print(f\"Expected output: 'entities: [{entities_str}]'\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
