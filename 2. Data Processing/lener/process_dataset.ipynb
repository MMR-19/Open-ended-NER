{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f544ab6a",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bb7c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, ClassLabel, Sequence, concatenate_datasets\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the folder to the Python path\n",
    "sys.path.append(os.path.abspath(\"../../0. Helpers\"))\n",
    "\n",
    "from datasetProcessing import tokens_to_entities\n",
    "from datasetBalancedSplit import balanced_multilabel_sample, entity_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125643aa",
   "metadata": {},
   "source": [
    "Read txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe61c325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(file_path):\n",
    "    \n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    current_sentence = []\n",
    "    current_labels = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "\n",
    "            # check to save sentence\n",
    "            if line == \"\":\n",
    "                if current_sentence != []:\n",
    "                    sentences.append(current_sentence)\n",
    "                    labels.append(current_labels)\n",
    "                    current_sentence = []\n",
    "                    current_labels = []\n",
    "                \n",
    "            # add word and label to current sentence\n",
    "            else:\n",
    "                word, label = line.split()\n",
    "                current_sentence.append(word)\n",
    "                current_labels.append(label)\n",
    "\n",
    "        if current_sentence != []:\n",
    "                sentences.append(current_sentence)\n",
    "                labels.append(current_labels)\n",
    "                current_sentence = []\n",
    "                current_labels = []\n",
    "\n",
    "    return sentences, labels\n",
    "\n",
    "\n",
    "# test dev\n",
    "sentences, labels = process_dataset(\"...\")\n",
    "\n",
    "# print results\n",
    "for sentence, label in zip(sentences, labels):\n",
    "    print(\" \".join(sentence))\n",
    "    print(label)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e058ffd",
   "metadata": {},
   "source": [
    "Create HF dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574a506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Hugging Face dataset\n",
    "dev_sentences, dev_labels = process_dataset(\"...\")\n",
    "test_sentences, test_labels = process_dataset(\"...\")\n",
    "train_sentences, train_labels = process_dataset(\"...\")\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': Dataset.from_dict({\n",
    "        'id': list(range(1, len(train_sentences) + 1)),\n",
    "        'tokens': train_sentences,\n",
    "        'ner_tags': train_labels\n",
    "    }),\n",
    "    'validation': Dataset.from_dict({\n",
    "        'id': list(range(1, len(dev_sentences) + 1)),\n",
    "        'tokens': dev_sentences,\n",
    "        'ner_tags': dev_labels\n",
    "    }),\n",
    "    'test': Dataset.from_dict({\n",
    "        'id': list(range(1, len(test_sentences) + 1)),\n",
    "        'tokens': test_sentences,\n",
    "        'ner_tags': test_labels\n",
    "    }),\n",
    "})\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba1a4c5",
   "metadata": {},
   "source": [
    "Confirm all entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f28c906",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = set()\n",
    "\n",
    "for labels in [train_labels, dev_labels, test_labels]:\n",
    "    for label_list in labels:\n",
    "        for label in label_list:\n",
    "            entities.add(label)\n",
    "\n",
    "print(\"Entities:\", entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a7659f",
   "metadata": {},
   "source": [
    "Process into default index labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce67b9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entities\n",
    "entity_names = [\"O\", \"B-PESSOA\", \"I-PESSOA\", \"B-ORGANIZACAO\", \"I-ORGANIZACAO\", \"B-LOCAL\", \"I-LOCAL\", \"B-TEMPO\", \"I-TEMPO\", \"B-LEGISLACAO\", \"I-LEGISLACAO\", \"B-JURISPRUDENCIA\", \"I-JURISPRUDENCIA\"]\n",
    "entity_names_parsed = {\"PESSOA\": \"Pessoa\", \"ORGANIZACAO\": \"Organização\", \"LOCAL\": \"Localização\", \"TEMPO\": \"Tempo\", \"LEGISLACAO\": \"Legislação\", \"JURISPRUDENCIA\": \"Jurisprudência\", \"O\": \"-\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f3fe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping between ner_tag and index\n",
    "tag_to_index = {tag: idx for idx, tag in enumerate(entity_names)}\n",
    "\n",
    "# Update the dataset with the index of each ner_tag\n",
    "def update_ner_tags(example):\n",
    "    example[\"ner_tags\"] = [tag_to_index[tag] for tag in example[\"ner_tags\"]]\n",
    "    return example\n",
    "\n",
    "# Apply the mapping to the entire dataset\n",
    "dataset = dataset.map(update_ner_tags)\n",
    "\n",
    "# Update the features with the list of ner_tags\n",
    "dataset = dataset.cast_column(\"ner_tags\", Sequence(feature = ClassLabel(names = entity_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45be08e6",
   "metadata": {},
   "source": [
    "Confirmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a19efdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)\n",
    "print(); print(\"example\"); print(dataset[\"train\"][0])\n",
    "print(); print(\"entities\"); print(dataset[\"train\"].features[\"ner_tags\"].feature.names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c2ef62",
   "metadata": {},
   "source": [
    "Save dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bd7efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save_to_disk('lener_hf_original')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e513f8ad",
   "metadata": {},
   "source": [
    "Split into train and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76c931b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_low = copy.copy(dataset)\n",
    "\n",
    "# merge test and validation\n",
    "dataset_low[\"test\"] = concatenate_datasets([dataset_low[\"test\"], dataset_low[\"validation\"]])\n",
    "dataset_low.pop(\"validation\")\n",
    "\n",
    "print(dataset_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377d39a8",
   "metadata": {},
   "source": [
    "Remove zero-entity instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809b28e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove instances with no entities\n",
    "def filter_no_entities(example):\n",
    "    return any(tag != tag_to_index[\"O\"] for tag in example[\"ner_tags\"])\n",
    "\n",
    "dataset_low = dataset_low.filter(filter_no_entities)\n",
    "\n",
    "print(dataset_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d9004d",
   "metadata": {},
   "source": [
    "Balanced version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61d4931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from entities_leNER import entity_names, entity_names_parsed\n",
    "\n",
    "# get the entity names\n",
    "start_of_entity_indices = [i for i in range(len(entity_names)) if (entity_names[i].startswith(\"B-\") or entity_names[i].startswith(\"U-\"))]\n",
    "entity_index_to_name = {i: entity_names[i].split(\"-\")[1] for i in range(len(entity_names)) if entity_names[i] != \"O\"}\n",
    "entity_index_to_name[0] = \"O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8778bc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_entity_map = entity_map(dataset_low[\"train\"], entity_names_parsed, start_of_entity_indices, entity_index_to_name); print()\n",
    "test_entity_map = entity_map(dataset_low[\"test\"], entity_names_parsed, start_of_entity_indices, entity_index_to_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25dd248",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_idx = balanced_multilabel_sample(train_entity_map, 200)\n",
    "new_test_idx = balanced_multilabel_sample(test_entity_map, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c543be82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the new datasets based on the new indices\n",
    "dataset_low[\"train\"] = dataset_low[\"train\"].select(new_train_idx)\n",
    "dataset_low[\"test\"] = dataset_low[\"test\"].select(new_test_idx)\n",
    "\n",
    "print(dataset_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8d2fce",
   "metadata": {},
   "source": [
    "Confirm old vs new distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d85a07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_entity_map = entity_map(dataset_low[\"train\"], entity_names_parsed, start_of_entity_indices, entity_index_to_name); print()\n",
    "new_test_entity_map = entity_map(dataset_low[\"test\"], entity_names_parsed, start_of_entity_indices, entity_index_to_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c46a5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(\"Instances with at least one entity of class:\")\n",
    "\n",
    "# Flatten all class lists and count frequencies\n",
    "entity_counts = Counter(cls for classes in train_entity_map.values() for cls in classes)\n",
    "print(\"Old train\", entity_counts)\n",
    "\n",
    "new_entity_counts = Counter(cls for classes in new_train_entity_map.values() for cls in classes)\n",
    "print(\"New train\", new_entity_counts)\n",
    "\n",
    "new_test_counts = Counter(cls for classes in new_test_entity_map.values() for cls in classes)\n",
    "print(\"New test\", new_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca840457",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classes_train = {cls:0 for classes in new_train_entity_map.values() for cls in classes}\n",
    "for instance in dataset_low[\"train\"]:\n",
    "    entity_list = tokens_to_entities(instance[\"tokens\"], instance[\"ner_tags\"], entity_names_parsed, start_of_entity_indices, entity_index_to_name)\n",
    "    for entity in entity_list:\n",
    "        all_classes_train[entity.entity] += 1\n",
    "\n",
    "print(\"Total entities train:\")\n",
    "print(all_classes_train)\n",
    "\n",
    "all_classes_test = {cls:0 for classes in new_test_entity_map.values() for cls in classes}\n",
    "for instance in dataset_low[\"test\"]:\n",
    "    entity_list = tokens_to_entities(instance[\"tokens\"], instance[\"ner_tags\"], entity_names_parsed, start_of_entity_indices, entity_index_to_name)\n",
    "    for entity in entity_list:\n",
    "        all_classes_test[entity.entity] += 1\n",
    "\n",
    "print(\"Total entities test:\")\n",
    "print(all_classes_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f146a4e",
   "metadata": {},
   "source": [
    "Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb21f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_low.save_to_disk('lener_hf_low')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
