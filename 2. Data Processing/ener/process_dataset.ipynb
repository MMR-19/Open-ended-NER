{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f544ab6a",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bb7c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, ClassLabel, Sequence\n",
    "import csv\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the folder to the Python path\n",
    "sys.path.append(os.path.abspath(\"../../0. Helpers\"))\n",
    "\n",
    "from datasetProcessing import tokens_to_entities\n",
    "from datasetBalancedSplit import balanced_multilabel_sample, entity_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125643aa",
   "metadata": {},
   "source": [
    "Read txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe61c325",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"...\"\n",
    "\n",
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "current_sentence = []\n",
    "current_labels = []\n",
    "\n",
    "with open(file_path, 'r', newline='', encoding='utf-8') as file:\n",
    "    prev_label = \"O\" # Reset label\n",
    "\n",
    "    reader = csv.reader(file)\n",
    "\n",
    "    for row in reader:\n",
    "        word, label = row\n",
    "        \n",
    "        # check to save sentence\n",
    "        if word == \"\":\n",
    "            if current_sentence != []:\n",
    "                sentences.append(current_sentence)\n",
    "                labels.append(current_labels)\n",
    "                current_sentence = []\n",
    "                current_labels = []\n",
    "            \n",
    "            prev_label = \"O\" # Reset label\n",
    "        \n",
    "        # add word and label to current sentence\n",
    "        else:\n",
    "\n",
    "            # get BIO label\n",
    "            if label == \"O\":\n",
    "                # Non-entity (O)\n",
    "                bio_label = \"O\"\n",
    "            else:\n",
    "                if prev_label != \"O\" and label == prev_label:\n",
    "                    # Inside of an entity (I-)\n",
    "                    bio_label = f\"I-{label}\"\n",
    "                else:\n",
    "                    # Beginning of an entity (B-)\n",
    "                    bio_label = f\"B-{label}\"\n",
    "\n",
    "            # final processing of B-B, B-I and I-I\n",
    "            bio_label = bio_label.replace(\"B-B-\", \"B-\")\n",
    "            bio_label = bio_label.replace(\"B-I-\", \"B-\")\n",
    "            bio_label = bio_label.replace(\"I-I-\", \"I-\")\n",
    "\n",
    "            current_sentence.append(word)\n",
    "            current_labels.append(bio_label)\n",
    "\n",
    "            prev_label = label  # Update\n",
    "\n",
    "    if current_sentence != []:\n",
    "            sentences.append(current_sentence)\n",
    "            labels.append(current_labels)\n",
    "            current_sentence = []\n",
    "            current_labels = []\n",
    "\n",
    "# print results\n",
    "for sentence, label_list in zip(sentences, labels):\n",
    "    print(\" \".join(sentence))\n",
    "    print(label_list)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e058ffd",
   "metadata": {},
   "source": [
    "Create HF dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574a506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Hugging Face dataset\n",
    "dataset = DatasetDict({\n",
    "    'train': Dataset.from_dict({\n",
    "        'id': list(range(1, len(sentences) + 1)),\n",
    "        'tokens': sentences,\n",
    "        'ner_tags': labels\n",
    "    })\n",
    "})\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba1a4c5",
   "metadata": {},
   "source": [
    "Confirm all entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f28c906",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = set()\n",
    "for label_list in labels:\n",
    "    for label in label_list:\n",
    "        entities.add(label)\n",
    "\n",
    "print(\"Entities:\", entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a7659f",
   "metadata": {},
   "source": [
    "Process into default index labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce67b9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entities\n",
    "entity_names = [\"O\", \"B-PERSON\", \"I-PERSON\", \"B-COURT\", \"I-COURT\", \"B-BUSINESS\", \"I-BUSINESS\", \"B-GOVERNMENT\", \"I-GOVERNMENT\", \"B-LOCATION\", \"I-LOCATION\", \"B-LEGISLATION/ACT\", \"I-LEGISLATION/ACT\", \"B-MISCELLANEOUS\", \"I-MISCELLANEOUS\"]\n",
    "entity_names_parsed = {\"PERSON\": \"Person\", \"COURT\": \"Court\", \"BUSINESS\": \"Business\", \"GOVERNMENT\": \"Government\", \"LOCATION\": \"Location\", \"LEGISLATION/ACT\": \"Legislation/Act\", \"MISCELLANEOUS\": \"Miscellaneous \", \"O\": \"-\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f3fe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping between ner_tag and index\n",
    "tag_to_index = {tag: idx for idx, tag in enumerate(entity_names)}\n",
    "\n",
    "# Update the dataset with the index of each ner_tag\n",
    "def update_ner_tags(example):\n",
    "    example[\"ner_tags\"] = [tag_to_index[tag] for tag in example[\"ner_tags\"]]\n",
    "    return example\n",
    "\n",
    "# Apply the mapping to the entire dataset\n",
    "dataset = dataset.map(update_ner_tags)\n",
    "\n",
    "# Update the features with the list of ner_tags\n",
    "dataset = dataset.cast_column(\"ner_tags\", Sequence(feature = ClassLabel(names = entity_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1bbd35",
   "metadata": {},
   "source": [
    "Confirmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f60e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)\n",
    "print(); print(\"example\"); print(dataset[\"train\"][0])\n",
    "print(); print(\"entities\"); print(dataset[\"train\"].features[\"ner_tags\"].feature.names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c2ef62",
   "metadata": {},
   "source": [
    "Save dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bd7efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save_to_disk('ener_hf_original')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92afad1",
   "metadata": {},
   "source": [
    "Split into train and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b49b9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_low = copy.copy(dataset)\n",
    "\n",
    "# merge test and validation\n",
    "dataset_low[\"original\"] = dataset_low[\"train\"]\n",
    "dataset_low.pop(\"train\")\n",
    "\n",
    "dataset_low[\"train\"] = dataset_low[\"original\"]\n",
    "dataset_low[\"test\"] = dataset_low[\"original\"]\n",
    "\n",
    "print(dataset_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ab7045",
   "metadata": {},
   "source": [
    "Balanced version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabea3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from entities_eNER import entity_names, entity_names_parsed\n",
    "\n",
    "# get the entity names\n",
    "start_of_entity_indices = [i for i in range(len(entity_names)) if (entity_names[i].startswith(\"B-\") or entity_names[i].startswith(\"U-\"))]\n",
    "entity_index_to_name = {i: entity_names[i].split(\"-\")[1] for i in range(len(entity_names)) if entity_names[i] != \"O\"}\n",
    "entity_index_to_name[0] = \"O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7251d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_entity_map = entity_map(dataset_low[\"train\"], entity_names_parsed, start_of_entity_indices, entity_index_to_name); print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d598711",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_idx = balanced_multilabel_sample(train_entity_map, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b28f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the new datasets based on the new indices\n",
    "dataset_low[\"train\"] = dataset_low[\"train\"].select(new_train_idx)\n",
    "\n",
    "print(dataset_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40de5590",
   "metadata": {},
   "source": [
    "Exclude new_train examples from new_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a31655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices to exclude\n",
    "exclude_indices = new_train_idx\n",
    "\n",
    "# Compute the complement\n",
    "include_indices = [i for i in range(len(dataset_low[\"test\"])) if i not in exclude_indices]\n",
    "\n",
    "# Select all except 1, 2, 3\n",
    "dataset_low[\"test\"] = dataset_low[\"test\"].select(include_indices)\n",
    "\n",
    "print(dataset_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fadb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_entity_map = entity_map(dataset_low[\"test\"], entity_names_parsed, start_of_entity_indices, entity_index_to_name)\n",
    "new_test_idx = balanced_multilabel_sample(test_entity_map, 600)\n",
    "dataset_low[\"test\"] = dataset_low[\"test\"].select(new_test_idx)\n",
    "\n",
    "dataset_low.pop(\"original\")\n",
    "print(dataset_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47932b2c",
   "metadata": {},
   "source": [
    "Confirm old vs new distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6043fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_entity_map = entity_map(dataset_low[\"train\"], entity_names_parsed, start_of_entity_indices, entity_index_to_name); print()\n",
    "new_test_entity_map = entity_map(dataset_low[\"test\"], entity_names_parsed, start_of_entity_indices, entity_index_to_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec978ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(\"Instances with at least one entity of class:\")\n",
    "\n",
    "# Flatten all class lists and count frequencies\n",
    "entity_counts = Counter(cls for classes in train_entity_map.values() for cls in classes)\n",
    "print(\"Old train\", entity_counts)\n",
    "\n",
    "new_entity_counts = Counter(cls for classes in new_train_entity_map.values() for cls in classes)\n",
    "print(\"New train\", new_entity_counts)\n",
    "\n",
    "new_test_counts = Counter(cls for classes in new_test_entity_map.values() for cls in classes)\n",
    "print(\"New test\", new_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db717c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_classes_train = {cls:0 for classes in new_train_entity_map.values() for cls in classes}\n",
    "for instance in dataset_low[\"train\"]:\n",
    "    entity_list = tokens_to_entities(instance[\"tokens\"], instance[\"ner_tags\"], entity_names_parsed, start_of_entity_indices, entity_index_to_name)\n",
    "    for entity in entity_list:\n",
    "        all_classes_train[entity.entity] += 1\n",
    "\n",
    "print(\"Total entities train:\")\n",
    "print(all_classes_train)\n",
    "\n",
    "all_classes_test = {cls:0 for classes in new_test_entity_map.values() for cls in classes}\n",
    "for instance in dataset_low[\"test\"]:\n",
    "    entity_list = tokens_to_entities(instance[\"tokens\"], instance[\"ner_tags\"], entity_names_parsed, start_of_entity_indices, entity_index_to_name)\n",
    "    for entity in entity_list:\n",
    "        all_classes_test[entity.entity] += 1\n",
    "\n",
    "print(\"Total entities test:\")\n",
    "print(all_classes_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64a3332",
   "metadata": {},
   "source": [
    "Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37a9108",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_low.save_to_disk('ener_hf_low')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
