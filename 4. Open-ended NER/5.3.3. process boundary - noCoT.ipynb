{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "9f41c990",
            "metadata": {},
            "source": [
                "Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b513c9fe",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Libraries\n",
                "import numpy as np\n",
                "import sys\n",
                "import os\n",
                "import json\n",
                "from pathlib import Path\n",
                "import re\n",
                "import litellm \n",
                "from pydantic import BaseModel\n",
                "import pandas as pd\n",
                "\n",
                "# add path to the dataset entities\n",
                "sys.path.append(os.path.abspath(\"../0. Helpers\"))\n",
                "sys.path.append(os.path.abspath(\"../2. Data Processing/_dataset_entities\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a38a8d5d",
            "metadata": {},
            "source": [
                "Topic"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6ec88d30",
            "metadata": {},
            "outputs": [],
            "source": [
                "potential_tokens_folder_output = \"adapted_noCoT\" # output\n",
                "potential_tokens_folder_input = \"adapted\" # input\n",
                "results_category = \"boundary\"\n",
                "\n",
                "class Config:\n",
                "    def __init__(self, lang, n):\n",
                "        self.lang = lang\n",
                "        self.n = n\n",
                "\n",
                "all_configs = {\n",
                "    \"ai\": Config(\"en\", 10),\n",
                "    \"literature\": Config(\"en\", 10),\n",
                "    \"music\": Config(\"en\", 10),\n",
                "    \"politics\": Config(\"en\", 20),\n",
                "    \"science\": Config(\"en\", 20),\n",
                "    \"multinerd_en\": Config(\"en\", 20),\n",
                "    \"multinerd_pt\": Config(\"pt\", 20),\n",
                "    \"ener\": Config(\"en\", 20),\n",
                "    \"lener\": Config(\"pt\", 20),\n",
                "    \"neuralshift\": Config(\"pt\", 20)\n",
                "}"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "628bf226",
            "metadata": {},
            "source": [
                "Prompt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "186a96d2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare LLM environment\n",
                "os.environ[\"AZURE_API_KEY\"] = \"...\"\n",
                "os.environ[\"AZURE_API_BASE\"] = \"...\"\n",
                "\n",
                "class result(BaseModel):\n",
                "  span: str\n",
                "  entity: str\n",
                "\n",
                "# class LLM_Entity_List(BaseModel):\n",
                "#   reflection: str\n",
                "#   result: LLM_Entity\n",
                "\n",
                "# class LLM_Output(BaseModel):\n",
                "#   candidate_token: str\n",
                "#   rationale: str\n",
                "#   updates: str\n",
                "#   entities: list[LLM_Entity]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0e42ba6e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# def get_prompt_prefix(topic):\n",
                "\n",
                "#     entity_info = \"\"\n",
                "\n",
                "#     # Entity info = point + span\n",
                "#     point_dict = json.load(open(f\"entity_info/point_entities/span/{topic}/train/_point_span_4.json\", \"r\", encoding=\"utf-8\"))\n",
                "#     for entity, clusters in point_dict.items():\n",
                "#         entity_info += f\"- \\\"{entity}\\\" e.g. {', '.join(clusters)}\\n\"\n",
                "\n",
                "#     # Split by topic\n",
                "\n",
                "#     # AI\n",
                "#     if topic == \"ai\":\n",
                "#         prompt_prefix = f\"\"\"Use the following set of possible entity labels:\n",
                "# {entity_info}\n",
                "\n",
                "# Dates, times, abstract concepts, adjectives and verbs are NOT entities.\n",
                "# Be sure to prioritize more specific entities, such as \"researcher\" over \"person\", \"conference\" over \"location\" and \"university\" over \"organisation\", when it makes sense.\n",
                "# \"\"\"\n",
                "#     # LITERATURE\n",
                "#     elif topic == \"literature\":\n",
                "#         prompt_prefix = f\"\"\"Use the following set of possible entity labels:\n",
                "# {entity_info}\n",
                "\n",
                "# Dates, times, abstract concepts, adjectives and verbs are NOT entities.\n",
                "# Be sure to prioritize more specific entities, such as \"writer\" over \"person\", when it makes sense.\n",
                "# \"\"\"\n",
                "    \n",
                "#     # MUSIC\n",
                "#     elif topic == \"music\":\n",
                "#         prompt_prefix = f\"\"\"Use the following set of possible entity labels:\n",
                "# {entity_info}\n",
                "\n",
                "# Dates, times, abstract concepts, adjectives and verbs are NOT entities.\n",
                "# Be sure to prioritize more specific entities, such as \"musical artist\" over \"person\" and \"band\" over \"organisation\", when it makes sense.\n",
                "# \"\"\"\n",
                "\n",
                "#     # POLITICS\n",
                "#     elif topic == \"politics\":\n",
                "#         prompt_prefix = f\"\"\"Use the following set of possible entity labels:\n",
                "# {entity_info}\n",
                "\n",
                "# Dates, times, abstract concepts, adjectives and verbs are NOT entities.\n",
                "# Be sure to prioritize more specific entities, such as \"politician\" over \"person\" and \"political party\" over \"organisation\", when it makes sense.\n",
                "# \"\"\"\n",
                "        \n",
                "#     # SCIENCE\n",
                "#     elif topic == \"science\":\n",
                "#         prompt_prefix = f\"\"\"Use the following set of possible entity labels:\n",
                "# {entity_info}\n",
                "\n",
                "# Dates, times, abstract concepts, adjectives and verbs are NOT entities.\n",
                "# Abstract scientific concepts can be entities if they have a name associated with them.\n",
                "# Be sure to prioritize more specific entities, such as \"scientist\" over \"person\" and \"university\" over \"organisation\" or \"location\", when it makes sense.\n",
                "# \"\"\"\n",
                "\n",
                "#     # MULTINERD PT\n",
                "#     elif topic == \"multinerd_pt\":\n",
                "#         prompt_prefix = f\"\"\"Usa o seguinte conjunto de tipos possíveis de entidade:\n",
                "# {entity_info}\n",
                "\n",
                "# Datas, horas, conceitos abstratos, adjetivos e verbos NÃO são entidades.\n",
                "# Se uma entidade não se encaixar em nenhum dos tipos acima, não a incluas na resposta.\n",
                "# \"\"\"\n",
                "        \n",
                "#     # MULTINERD EN\n",
                "#     elif topic == \"multinerd_en\":\n",
                "#         prompt_prefix = f\"\"\"Use the following set of possible entity labels:\n",
                "# {entity_info}\n",
                "# Dates, times, abstract concepts, adjectives and verbs are NOT entities.\n",
                "# \"\"\"\n",
                "        \n",
                "#     # E-NER\n",
                "#     elif topic == \"ener\":\n",
                "#         prompt_prefix = f\"\"\"Use the following set of possible entity labels:\n",
                "# {entity_info}\n",
                "# Dates, times, abstract concepts, adjectives and verbs are NOT entities.\n",
                "# \"\"\"\n",
                "        \n",
                "#     # LeNER-Br + NEURALSHIFT\n",
                "#     elif (topic == \"lener\" or topic == \"neuralshift\"):\n",
                "#         prompt_prefix = f\"\"\"Usa o seguinte conjunto de tipos possíveis de entidade:\n",
                "# {entity_info}\n",
                "\n",
                "# Conceitos abstratos, adjetivos e verbos NÃO são entidades.\n",
                "# Se uma entidade não se encaixar em nenhum dos tipos acima, não a incluas na resposta.\n",
                "# \"\"\"\n",
                "        \n",
                "#     # validation\n",
                "#     if entity_info == \"\" or prompt_prefix == \"\":\n",
                "#         raise ValueError(f\"Error retrieving entity info for topic {topic}.\")\n",
                "\n",
                "#     return prompt_prefix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7ff13a1e",
            "metadata": {},
            "outputs": [],
            "source": [
                "################## FINAL PROMPT ##################\n",
                "def final_prompt(lang, sentence, token, boundary, entity_str, examples):\n",
                "\n",
                "    # validations\n",
                "    if lang not in [\"en\", \"pt\"]:\n",
                "        raise ValueError(f\"Language {lang} not supported.\")\n",
                "    \n",
                "    if boundary not in [\"inside\", \"outside\"]:\n",
                "        raise ValueError(f\"Boundary {boundary} not supported.\")\n",
                "\n",
                "    # example string\n",
                "    examples_string = \"\"\n",
                "    for example in examples:\n",
                "        examples_string += f\"Input: {example['sentence']}\\nOutput entity: '{example['entity']}'\\n\\n\"\n",
                "\n",
                "############# BOUNDARY INSIDE #############\n",
                "\n",
                "    if boundary == \"inside\":\n",
                "        # reflect on token being part of entity when it is probable to be context\n",
                "\n",
                "        if lang == \"en\":\n",
                "            return f\"\"\"\n",
                "<input_text>\n",
                "{sentence}\n",
                "</input_text>\n",
                "\n",
                "<predicted_entity>\n",
                "{entity_str}\n",
                "</predicted_entity>\n",
                "\n",
                "<boundary_token>\n",
                "{token}\n",
                "</boundary_token>\n",
                "\n",
                "<current_status>\n",
                "part of the entity\n",
                "</current_status>\n",
                "\n",
                "1. Calibrate the boundary of the predicted entity by evaluating the inside boundary token \"{token}\".\n",
                "\n",
                "2. The boundary token is a token that was recognized as being part of the entity by the NER system. However, based on training data, it has a high likelihood of being part of the context of an entity, but not actually inside the entity. Note that the training data might not be representative, therefore use your best judgment to determine whether the token (and surroundings) should be removed from the entity or not.\n",
                "\n",
                "3. Consider the following (negative) examples carefully:\n",
                "<examples>\n",
                "{examples_string}\n",
                "</examples>\n",
                "\n",
                "4. Always output the entity. Either the same as it was given, or a modified version of it (with the new refined boundary)!\n",
                "\n",
                "5. Always respect the <input_text> structure, classifying continuous spans.\n",
                "\n",
                "Output Format:\n",
                "- result: modified entity or original entity if no change is needed \n",
                "\n",
                "Return a JSON, focusing on the entity \"{entity_str}\" and the boundary token \"{token}\" within the input text:\n",
                "\"{sentence}\"\n",
                "\"\"\"\n",
                "        \n",
                "        elif lang == \"pt\":\n",
                "            return f\"\"\"\n",
                "<input_text>\n",
                "{sentence}\n",
                "</input_text>\n",
                "\n",
                "<predicted_entity>\n",
                "{entity_str}\n",
                "</predicted_entity>\n",
                "\n",
                "<boundary_token>\n",
                "{token}\n",
                "</boundary_token>\n",
                "\n",
                "<current_status>\n",
                "dentro da entidade\n",
                "</current_status>\n",
                "\n",
                "1. Calibra a fronteira da entidade prevista avaliando o token de fronteira \"{token}\".\n",
                "\n",
                "2. O token de fronteira é um token que foi reconhecido como parte da entidade pelo sistema NER. No entanto, com base nos dados de treino, ele tem uma alta probabilidade de fazer parte do contexto (à volta) de uma entidade, mas não realmente dentro da entidade. Nota que os dados de treino podem não ser representativos, portanto usa o teu melhor julgamento para determinar se o token (e arredores) deve ser removido da entidade ou não.\n",
                "\n",
                "3. Considera cuidadosamente os seguintes exemplos (negativos):\n",
                "<examples>\n",
                "{examples_string}\n",
                "</examples>\n",
                "\n",
                "4. Devolve sempre a entidade. Quer seja a mesma que foi dada, ou uma versão modificada (com a nova fronteira refinada)!\n",
                "\n",
                "5. Respeita sempre a estrutura do <input_text>, classificando spans de forma contínua.\n",
                "\n",
                "Formato de Output:\n",
                "- result: entidade modificada ou a original se nenhuma alteração for necessária\n",
                "\n",
                "Devolve um JSON, focando-te na entidade \"{entity_str}\" e no token de fronteira \"{token}\" dentro do texto de input:\n",
                "\"{sentence}\"\n",
                "\"\"\"\n",
                "\n",
                "############# BOUNDARY OUTSIDE #############\n",
                "\n",
                "    elif boundary == \"outside\":\n",
                "\n",
                "        if lang == \"en\":\n",
                "            return f\"\"\"\n",
                "<input_text>\n",
                "{sentence}\n",
                "</input_text>\n",
                "\n",
                "<predicted_entity>\n",
                "{entity_str}\n",
                "</predicted_entity>\n",
                "\n",
                "<boundary_token>\n",
                "{token}\n",
                "</boundary_token>\n",
                "\n",
                "<current_status>\n",
                "outside of the entity\n",
                "</current_status>\n",
                "\n",
                "1. Calibrate the boundary of the predicted entity by evaluating the context (outside) boundary token \"{token}\".\n",
                "\n",
                "2. The boundary token is a token that was not recognized as being part of the entity by the NER system. However, based on training data, it has a high likelihood of being part of the entity, but could have been left out. Note that the training data might not be representative, therefore use your best judgment to determine whether the token (and surroundings) should be added to the entity or not.\n",
                "\n",
                "3. Consider the following (positive) examples carefully:\n",
                "<examples>\n",
                "{examples_string}\n",
                "</examples>\n",
                "\n",
                "4. Always output the entity. Either the same as it was given, or a modified version of it (with the new refined boundary)!\n",
                "\n",
                "5. Always respect the <input_text> structure, classifying continuous spans.\n",
                "\n",
                "Output Format:\n",
                "- result: modified entity or original entity if no change is needed \n",
                "\n",
                "Return a JSON, focusing on the entity \"{entity_str}\" and the boundary token \"{token}\" within the input text:\n",
                "\"{sentence}\"\n",
                "\"\"\"\n",
                "        \n",
                "        elif lang == \"pt\":\n",
                "            return f\"\"\"\n",
                "<input_text>\n",
                "{sentence}\n",
                "</input_text>\n",
                "\n",
                "<predicted_entity>\n",
                "{entity_str}\n",
                "</predicted_entity>\n",
                "\n",
                "<boundary_token>\n",
                "{token}\n",
                "</boundary_token>\n",
                "\n",
                "<current_status>\n",
                "fora da entidade\n",
                "</current_status>\n",
                "\n",
                "1. Calibra a fronteira da entidade prevista avaliando o token (externo) de fronteira \"{token}\".\n",
                "\n",
                "2. O token de fronteira é um token que não foi reconhecido como parte da entidade pelo sistema NER. No entanto, com base nos dados de treino, ele tem uma alta probabilidade de fazer parte da entidade, mas pode ter sido deixado de fora. Nota que os dados de treino podem não ser representativos, portanto usa o teu melhor julgamento para determinar se o token (e arredores) deve ser adicionado à entidade ou não.\n",
                "\n",
                "3. Considera cuidadosamente os seguintes exemplos (positivos):\n",
                "<examples>\n",
                "{examples_string}\n",
                "</examples>\n",
                "\n",
                "4. Devolve sempre a entidade. Quer seja a mesma que foi dada, ou uma versão modificada (com a nova fronteira refinada)!\n",
                "\n",
                "5. Respeita sempre a estrutura do <input_text>, classificando spans de forma contínua.\n",
                "\n",
                "Output Format:\n",
                "- result: entidade modificada ou a original se nenhuma alteração for necessária\n",
                "\n",
                "Devolve um JSON, focando-te na entidade \"{entity_str}\" e no token de fronteira \"{token}\" dentro do texto de input:\n",
                "\"{sentence}\"\n",
                "\"\"\""
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0c078cf4",
            "metadata": {},
            "source": [
                "LLM Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b5bc09fb",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Call LLM\n",
                "def safe_llm_call(prompt, system, instance):\n",
                "    try:\n",
                "\n",
                "        response = litellm.completion(\n",
                "            model = \"azure/gpt-4o-mini\",\n",
                "            messages = [\n",
                "                {\"role\": \"system\", \"content\": system},\n",
                "                {\"role\": \"user\", \"content\": prompt},\n",
                "            ],\n",
                "\n",
                "            temperature = 0.1,\n",
                "            response_format = result,\n",
                "\n",
                "            # stream = False,\n",
                "            # top_p = 1,\n",
                "        )\n",
                "\n",
                "        # extract LLM predictions\n",
                "        return response.choices[0].message[\"content\"]\n",
                "\n",
                "    except Exception as e:\n",
                "        print(f\"\\n❌❌ LLM call failed: {e}\\n\")\n",
                "        print(f\"\\nExample: {instance}\")\n",
                "        raise"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3f9de219",
            "metadata": {},
            "outputs": [],
            "source": [
                "def system_prompt(lang):\n",
                "    if lang == \"en\":\n",
                "        return \"You are a named entity recognition (NER) system. Your task is to refine the boundaries of entities mentioned in the <input_text>, taking into account the <boundary_token> provided. Always output an entity with refined boundaries (or the original entity if no change is needed). Respond with JSON.\"\n",
                "    elif lang == \"pt\":\n",
                "        return \"És um sistema de reconhecimento de entidades (NER). A tua tarefa é refinar as fronteiras das entidades mencionadas no <input_text>, tendo em conta o <boundary_token> fornecido. Devolve sempre uma entidade com fronteiras refinadas (ou a entidade original se nenhuma alteração for necessária). Responde com JSON.\""
            ]
        },
        {
            "cell_type": "markdown",
            "id": "42258627",
            "metadata": {},
            "source": [
                "Call LLM for each token (reflection)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c8941205",
            "metadata": {},
            "outputs": [],
            "source": [
                "def process_token(topic, lang, file_name, token_info, sentence):\n",
                "\n",
                "    token = token_info['token']\n",
                "    entity_str = token_info['entity_str']\n",
                "    boundary = token_info['boundary']\n",
                "    examples = token_info['examples']\n",
                "\n",
                "    prompt = final_prompt(lang, sentence, token, boundary, entity_str, examples)\n",
                "\n",
                "    # Save prompt to txt file\n",
                "    prompt_file_path = f\"results/error_reflection/{topic}/{results_category}/{potential_tokens_folder_output}/prompts/prompt_{file_name}.txt\"\n",
                "    with open(prompt_file_path, \"w\", encoding=\"utf-8\") as f:\n",
                "        f.write(prompt)\n",
                "\n",
                "    # call the LLM\n",
                "    try:\n",
                "        llm_response = safe_llm_call(prompt, system_prompt(lang), sentence)\n",
                "\n",
                "        if llm_response is None:\n",
                "            print(f\"❌ LLM response is None for sentence: {sentence}\")\n",
                "            return\n",
                "        \n",
                "        # Save response to txt file\n",
                "        response_file_path = f\"results/error_reflection/{topic}/{results_category}/{potential_tokens_folder_output}/responses/response_{file_name}.txt\"\n",
                "        with open(response_file_path, \"a\", encoding=\"utf-8\") as f:\n",
                "            f.write(json.dumps(llm_response, ensure_ascii=False, indent=4))\n",
                "\n",
                "        # parse json and return new entities\n",
                "        llm_json = json.loads(llm_response)\n",
                "        new_llm_entity = llm_json\n",
                "\n",
                "        return {\n",
                "            \"original_entity_str\": entity_str,\n",
                "            \"new_llm_entity\": new_llm_entity\n",
                "        }\n",
                "\n",
                "    except Exception as e:\n",
                "        print(f\"❌ Error on sentence: {e}\")\n",
                "        return"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0f7bd750",
            "metadata": {},
            "source": [
                "Evaluate all tokens in folder"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "42384f93",
            "metadata": {},
            "outputs": [],
            "source": [
                "for topic, config in all_configs.items():\n",
                "    \n",
                "    n = config.n\n",
                "    lang = config.lang\n",
                "    print(f\"\\n\\nProcessing topic: {topic} (n={n}, lang={lang})\")\n",
                "    \n",
                "    input_path = Path(f\"error_reflection/{results_category}/{topic}/{potential_tokens_folder_input}\")\n",
                "\n",
                "    # ensure folder exists\n",
                "    os.makedirs(f\"results/error_reflection/{topic}/{results_category}/{potential_tokens_folder_output}\", exist_ok=True)\n",
                "    os.makedirs(f\"results/error_reflection/{topic}/{results_category}/{potential_tokens_folder_output}/prompts\", exist_ok=True)\n",
                "    os.makedirs(f\"results/error_reflection/{topic}/{results_category}/{potential_tokens_folder_output}/responses\", exist_ok=True)\n",
                "\n",
                "    all_data = list(input_path.glob(\"*.json\"))\n",
                "    for i, file_path in enumerate(all_data):\n",
                "        \n",
                "        print(f\"\\r\\tProcessing instance {i+1}/{len(all_data)}\", end='', flush=True)\n",
                "\n",
                "        # load file\n",
                "        with open(file_path, mode='r', encoding=\"utf-8\") as f:\n",
                "            data = json.loads(f.read())\n",
                "\n",
                "        sentence = data[\"sentence\"]\n",
                "        inside_boundary_tokens = data[\"inside_boundary_tokens\"]\n",
                "        outside_boundary_tokens = data[\"outside_boundary_tokens\"]\n",
                "\n",
                "        new_entities = []\n",
                "        for token_info in inside_boundary_tokens + outside_boundary_tokens:\n",
                "            reflected_token_entities = process_token(topic, lang, file_path.stem, token_info, sentence)\n",
                "            if reflected_token_entities:\n",
                "                new_entities.append(reflected_token_entities)\n",
                "\n",
                "        # save updated file with new entities\n",
                "        for entity in new_entities:\n",
                "            entity[\"boundary_token\"] = token_info['token']\n",
                "            entity[\"boundary\"] = token_info['boundary']\n",
                "\n",
                "        # save results to file\n",
                "        result_json = {\n",
                "            \"sentence\": sentence,\n",
                "            \"reflected_entities\": new_entities\n",
                "        }\n",
                "\n",
                "        result_file_path = f\"results/error_reflection/{topic}/{results_category}/{potential_tokens_folder_output}/{file_path.stem}.json\"\n",
                "        with open(result_file_path, \"w\", encoding=\"utf-8\") as f:\n",
                "            f.write(json.dumps(result_json, ensure_ascii=False, indent=4))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
