Example #1: The most common way is using the so-called ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measure.
Expected output: 'entities: [{'span': 'ROUGE', 'entity': 'Metrics'}, {'span': 'Recall-Oriented Understudy for Gisting Evaluation', 'entity': 'Metrics'}]'

Example #2: The NIST metric is based on the BLEU metric, but with some alterations.
Expected output: 'entities: [{'span': 'NIST metric', 'entity': 'Metrics'}, {'span': 'BLEU metric', 'entity': 'Metrics'}]'

Example #3: The 2004 SSIM paper has been cited over 20,000 times according to Google Scholar, It also received the IEEE Signal Processing Society Sustained Impact Award for 2016, indicative of a paper having an unusually high impact for at least 10 years following its publication.
Expected output: 'entities: [{'span': 'SSIM', 'entity': 'Metrics'}, {'span': 'Google Scholar', 'entity': 'Product'}, {'span': 'IEEE Signal Processing Society Sustained Impact Award', 'entity': 'Misc'}]'

Example #4: As with BLEU, the basic unit of evaluation is the sentence, the algorithm first creates an alignment (see illustrations) between two sentence s, the candidate translation string, and the reference translation string.
Expected output: 'entities: [{'span': 'BLEU', 'entity': 'Metrics'}]'

Example #5: The information retrieval metrics such as precision and recall or DCG are useful to assess the quality of a recommendation method.
Expected output: 'entities: [{'span': 'information retrieval', 'entity': 'Task'}, {'span': 'precision', 'entity': 'Metrics'}, {'span': 'recall', 'entity': 'Metrics'}, {'span': 'DCG', 'entity': 'Metrics'}]'

