Example #1: Speech recognition and speech synthesis deal with how spoken language can be understood or created using computers.
Expected output: 'entities: [{'span': 'Speech recognition', 'entity': 'Task'}, {'span': 'speech synthesis', 'entity': 'Task'}]'

Example #2: The input is called speech recognition and the output is called speech synthesis.
Expected output: 'entities: [{'span': 'speech recognition', 'entity': 'Task'}, {'span': 'speech synthesis', 'entity': 'Task'}]'

Example #3: Some less widely spoken languages use the open-source eSpeak synthesizer for their speech; producing a robotic, awkward voice that may be difficult to understand.
Expected output: 'entities: [{'span': 'eSpeak synthesizer', 'entity': 'Product'}]'

Example #4: The speech synthesis is verging on being completely indistinguishable from a real human's voice with the 2016 introduction of the voice editing and generation software Adobe Voco, a prototype slated to be a part of the Adobe Creative Suite and DeepMind WaveNet, a prototype from Google.
Expected output: 'entities: [{'span': 'speech synthesis', 'entity': 'Task'}, {'span': 'Adobe Voco', 'entity': 'Product'}, {'span': 'Adobe Creative Suite', 'entity': 'Product'}, {'span': 'DeepMind', 'entity': 'Organisation'}, {'span': 'WaveNet', 'entity': 'Product'}, {'span': 'Google', 'entity': 'Organisation'}]'

Example #5: The technique used in creating eigenfaces and using them for recognition is also used outside of face recognition: handwriting recognition, lip reading, voice recognition, sign language / hand gestures interpretation and medical imaging analysis.
Expected output: 'entities: [{'span': 'eigenfaces', 'entity': 'Misc'}, {'span': 'face recognition', 'entity': 'Task'}, {'span': 'handwriting recognition', 'entity': 'Task'}, {'span': 'lip reading', 'entity': 'Task'}, {'span': 'voice recognition', 'entity': 'Task'}, {'span': 'sign language', 'entity': 'Task'}, {'span': 'hand gestures interpretation', 'entity': 'Task'}, {'span': 'medical imaging analysis', 'entity': 'Field'}]'

