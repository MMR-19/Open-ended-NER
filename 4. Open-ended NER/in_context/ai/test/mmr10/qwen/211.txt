Example #1: Sigmoid function Cross entropy loss is used for predicting K independent probability values in math 0,1 / math.
Expected output: 'entities: [{'span': 'Sigmoid function Cross entropy loss', 'entity': 'Metrics'}]'

Example #2: Linear-fractional programming (LFP) is a generalization of linear programming (LP).
Expected output: 'entities: [{'span': 'Linear-fractional programming', 'entity': 'Algorithm'}, {'span': 'LFP', 'entity': 'Algorithm'}, {'span': 'linear programming', 'entity': 'Algorithm'}, {'span': 'LP', 'entity': 'Algorithm'}]'

Example #3: or equivalently using DCG notation:
Expected output: 'entities: [{'span': 'DCG', 'entity': 'Metrics'}]'

Example #4: The NIST metric is based on the BLEU metric, but with some alterations.
Expected output: 'entities: [{'span': 'NIST metric', 'entity': 'Metrics'}, {'span': 'BLEU metric', 'entity': 'Metrics'}]'

Example #5: RapidMiner provides learning schemes, models and algorithms and can be extended using R and Python scripts. David Norris, Bloor Research, November 13, 2013.
Expected output: 'entities: [{'span': 'RapidMiner', 'entity': 'Product'}, {'span': 'R', 'entity': 'Programming Language'}, {'span': 'Python', 'entity': 'Programming Language'}, {'span': 'David Norris', 'entity': 'Researcher'}, {'span': 'Bloor Research', 'entity': 'Organisation'}]'

Example #6: Logo was created in 1967 at Bolt, Beranek and Newman (BBN), a Cambridge, Massachusetts research firm, by Wally Feurzeig, Cynthia Solomon, and Seymour Papert.
Expected output: 'entities: [{'span': 'Logo', 'entity': 'Programming Language'}, {'span': 'Bolt, Beranek and Newman', 'entity': 'Organisation'}, {'span': 'BBN', 'entity': 'Organisation'}, {'span': 'Cambridge', 'entity': 'University'}, {'span': 'Massachusetts', 'entity': 'University'}, {'span': 'Wally Feurzeig', 'entity': 'Researcher'}, {'span': 'Cynthia Solomon', 'entity': 'Researcher'}, {'span': 'Seymour Papert', 'entity': 'Researcher'}]'

Example #7: The parameters β are typically estimated by maximum likelihood.
Expected output: 'entities: [{'span': 'maximum likelihood', 'entity': 'Metrics'}]'

Example #8: If we use least squares to fit a function in the form of a hyperplane ŷ = a + β supT / sup x to the data (x sub i / sub, y sub i / sub) sub 1 ≤ i ≤ n / sub, we could then assess the fit using the mean squared error (MSE).
Expected output: 'entities: [{'span': 'least squares', 'entity': 'Algorithm'}, {'span': 'mean squared error', 'entity': 'Metrics'}, {'span': 'MSE', 'entity': 'Metrics'}]'

Example #9: One can use the OSD algorithm to derive math O (\ sqrt { T }) / math regret bounds for the online version of Support vector machine for classification, which use the hinge loss math v _ t (w) = \ max \ { 0, 1 - y _ t (w \ cdot x _ t) \ } / math
Expected output: 'entities: [{'span': 'OSD algorithm', 'entity': 'Algorithm'}, {'span': 'Support vector machine', 'entity': 'Algorithm'}, {'span': 'classification', 'entity': 'Task'}, {'span': 'hinge loss', 'entity': 'Metrics'}]'

Example #10: To allow for multiple entities, a separate Hinge loss is computed for each capsule.
Expected output: 'entities: [{'span': 'Hinge loss', 'entity': 'Metrics'}]'

