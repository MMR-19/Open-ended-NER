Example #1: One can use the OSD algorithm to derive math O (\ sqrt { T }) / math regret bounds for the online version of Support vector machine for classification, which use the hinge loss math v _ t (w) = \ max \ { 0, 1 - y _ t (w \ cdot x _ t) \ } / math
Expected output: 'entities: [{'span': 'OSD algorithm', 'entity': 'Algorithm'}, {'span': 'Support vector machine', 'entity': 'Algorithm'}, {'span': 'classification', 'entity': 'Task'}, {'span': 'hinge loss', 'entity': 'Metrics'}]'

Example #2: Linear-fractional programming (LFP) is a generalization of linear programming (LP).
Expected output: 'entities: [{'span': 'Linear-fractional programming', 'entity': 'Algorithm'}, {'span': 'LFP', 'entity': 'Algorithm'}, {'span': 'linear programming', 'entity': 'Algorithm'}, {'span': 'LP', 'entity': 'Algorithm'}]'

Example #3: Examples of supervised learning are Naive Bayes classifier, Support vector machine, mixtures of Gaussians, and network.
Expected output: 'entities: [{'span': 'supervised learning', 'entity': 'Field'}, {'span': 'Naive Bayes classifier', 'entity': 'Algorithm'}, {'span': 'Support vector machine', 'entity': 'Algorithm'}, {'span': 'mixtures of Gaussians', 'entity': 'Algorithm'}, {'span': 'network', 'entity': 'Algorithm'}]'

Example #4: An optimal value for math \ alpha / math can be found by using a line search algorithm, that is, the magnitude of math \ alpha / math is determined by finding the value that minimizes S, usually using a line search in the interval math0 \ alpha 1 / math or a backtracking line search such as Armijo-line search.
Expected output: 'entities: [{'span': 'line search algorithm', 'entity': 'Algorithm'}, {'span': 'line search', 'entity': 'Algorithm'}, {'span': 'backtracking line search', 'entity': 'Algorithm'}, {'span': 'Armijo-line search', 'entity': 'Algorithm'}]'

Example #5: The sigmoid function s and derivatives used in the package were originally included in the package, from version 0.8.0 onwards, these were released in a separate R package sigmoid, with the intention to enable more general use.
Expected output: 'entities: [{'span': 'sigmoid function', 'entity': 'Algorithm'}, {'span': 'R', 'entity': 'Programming Language'}, {'span': 'sigmoid', 'entity': 'Algorithm'}]'

Example #6: , C. Papageorgiou and T. Poggio, A Trainable Pedestrian Detection system, International Journal of Computer Vision (IJCV), pages 1: 15-33, 2000 others uses local features like histogram of oriented gradients N. Dalal, B. Triggs, Histograms of oriented gradients for human detection, IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), pages 1: 886-893, 2005 descriptors.
Expected output: 'entities: [{'span': 'C. Papageorgiou', 'entity': 'Researcher'}, {'span': 'T. Poggio', 'entity': 'Researcher'}, {'span': 'Trainable Pedestrian Detection system', 'entity': 'Product'}, {'span': 'International Journal of Computer Vision', 'entity': 'Conference'}, {'span': 'IJCV', 'entity': 'Conference'}, {'span': 'histogram of oriented gradients', 'entity': 'Algorithm'}, {'span': 'N. Dalal', 'entity': 'Researcher'}, {'span': 'B. Triggs', 'entity': 'Researcher'}, {'span': 'Histograms of oriented gradients', 'entity': 'Algorithm'}, {'span': 'human detection', 'entity': 'Task'}, {'span': 'IEEE Computer Society Conference on Computer Vision and Pattern Recognition', 'entity': 'Conference'}, {'span': 'CVPR', 'entity': 'Conference'}]'

Example #7: The most common way is using the so-called ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measure.
Expected output: 'entities: [{'span': 'ROUGE', 'entity': 'Metrics'}, {'span': 'Recall-Oriented Understudy for Gisting Evaluation', 'entity': 'Metrics'}]'

Example #8: Variants of the back-propagation algorithm as well as unsupervised methods by Geoff Hinton and colleagues at the University of Toronto can be used to train deep, highly nonlinear neural architectures, { { cite journal
Expected output: 'entities: [{'span': 'back-propagation algorithm', 'entity': 'Algorithm'}, {'span': 'unsupervised methods', 'entity': 'Misc'}, {'span': 'Geoff Hinton', 'entity': 'Researcher'}, {'span': 'University of Toronto', 'entity': 'University'}]'

Example #9: A fast method for computing maximum likelihood estimates for the probit model was proposed by Ronald Fisher as an appendix to Bliss ' work in 1935.
Expected output: 'entities: [{'span': 'maximum likelihood', 'entity': 'Metrics'}, {'span': 'probit model', 'entity': 'Algorithm'}, {'span': 'Ronald Fisher', 'entity': 'Researcher'}, {'span': 'Bliss', 'entity': 'Researcher'}]'

Example #10: It also created flexible intelligent AGV applications, designing the Motivity control system used by RMT Robotics to develop its ADAM iAGV (Self-Guided Vehicle), used for complex pick and place operations, in conjunction with gantry systems and industrial robot arms, used in first-tier auto supply factories to move products from process to process in non-linear layouts.
Expected output: 'entities: [{'span': 'AGV', 'entity': 'Product'}, {'span': 'Motivity control system', 'entity': 'Product'}, {'span': 'RMT Robotics', 'entity': 'Organisation'}, {'span': 'ADAM iAGV', 'entity': 'Product'}, {'span': 'gantry systems', 'entity': 'Product'}, {'span': 'industrial robot arms', 'entity': 'Product'}, {'span': 'non-linear layouts', 'entity': 'Misc'}]'

