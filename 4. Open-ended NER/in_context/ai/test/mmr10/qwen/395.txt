Example #1: This math \ theta ^ { * } / math is normally estimated using a Maximum Likelihood (math \ theta ^ { * } = \ theta ^ { ML } / math) or Maximum A Posteriori (math \ theta ^ { * } = \ theta ^ { MAP } / math) procedure.
Expected output: 'entities: [{'span': 'Maximum Likelihood', 'entity': 'Algorithm'}, {'span': 'Maximum A Posteriori', 'entity': 'Algorithm'}, {'span': 'MAP', 'entity': 'Algorithm'}]'

Example #2: The technique used in creating eigenfaces and using them for recognition is also used outside of face recognition: handwriting recognition, lip reading, voice recognition, sign language / hand gestures interpretation and medical imaging analysis.
Expected output: 'entities: [{'span': 'eigenfaces', 'entity': 'Misc'}, {'span': 'face recognition', 'entity': 'Task'}, {'span': 'handwriting recognition', 'entity': 'Task'}, {'span': 'lip reading', 'entity': 'Task'}, {'span': 'voice recognition', 'entity': 'Task'}, {'span': 'sign language', 'entity': 'Task'}, {'span': 'hand gestures interpretation', 'entity': 'Task'}, {'span': 'medical imaging analysis', 'entity': 'Field'}]'

Example #3: As with BLEU, the basic unit of evaluation is the sentence, the algorithm first creates an alignment (see illustrations) between two sentence s, the candidate translation string, and the reference translation string.
Expected output: 'entities: [{'span': 'BLEU', 'entity': 'Metrics'}]'

Example #4: Neuroevolution is commonly used as part of the reinforcement learning paradigm, and it can be contrasted with conventional deep learning techniques that use gradient descent on a neural network with a fixed topology.
Expected output: 'entities: [{'span': 'Neuroevolution', 'entity': 'Misc'}, {'span': 'reinforcement learning', 'entity': 'Field'}, {'span': 'deep learning', 'entity': 'Field'}, {'span': 'gradient descent', 'entity': 'Algorithm'}, {'span': 'neural network', 'entity': 'Algorithm'}]'

Example #5: Although used mainly by statisticians and other practitioners requiring an environment for statistical computation and software development, R can also operate as a general matrix calculation toolbox - with performance benchmarks comparable to GNU Octave or MATLAB.
Expected output: 'entities: [{'span': 'R', 'entity': 'Programming Language'}, {'span': 'GNU Octave', 'entity': 'Programming Language'}, {'span': 'MATLAB', 'entity': 'Product'}]'

Example #6: In digital signal processing and information theory, the normalized sinc function is commonly defined for by
Expected output: 'entities: [{'span': 'digital signal processing', 'entity': 'Field'}, {'span': 'information theory', 'entity': 'Field'}, {'span': 'normalized sinc function', 'entity': 'Algorithm'}]'

Example #7: The term computational linguistics itself was first coined by David Hays, a founding member of both the Association for Computational Linguistics and the International Committee on Computational Linguistics (ICCL).
Expected output: 'entities: [{'span': 'computational linguistics', 'entity': 'Field'}, {'span': 'David Hays', 'entity': 'Researcher'}, {'span': 'Association for Computational Linguistics', 'entity': 'Conference'}, {'span': 'International Committee on Computational Linguistics', 'entity': 'Organisation'}, {'span': 'ICCL', 'entity': 'Organisation'}]'

Example #8: He discusses Breadth-first search and Depth-first search techniques, but eventually concludes that the results represent expert system s that incarnate a lot of technical knowledge but don 't shine much light on the mental processes that humans use to solve such puzzles.
Expected output: 'entities: [{'span': 'Breadth-first search', 'entity': 'Algorithm'}, {'span': 'Depth-first search', 'entity': 'Algorithm'}, {'span': 'expert system', 'entity': 'Product'}]'

Example #9: or equivalently using DCG notation:
Expected output: 'entities: [{'span': 'DCG', 'entity': 'Metrics'}]'

Example #10: To allow for multiple entities, a separate Hinge loss is computed for each capsule.
Expected output: 'entities: [{'span': 'Hinge loss', 'entity': 'Metrics'}]'

