Example #1: An autoencoder is a type of artificial neural network used to learn Feature learning in an unsupervised learning manner.
Expected output: 'entities: [{'span': 'autoencoder', 'entity': 'Algorithm'}, {'span': 'artificial neural network', 'entity': 'Algorithm'}, {'span': 'Feature learning', 'entity': 'Task'}, {'span': 'unsupervised learning', 'entity': 'Field'}]'

Example #2: If we use least squares to fit a function in the form of a hyperplane ŷ = a + β supT / sup x to the data (x sub i / sub, y sub i / sub) sub 1 ≤ i ≤ n / sub, we could then assess the fit using the mean squared error (MSE).
Expected output: 'entities: [{'span': 'least squares', 'entity': 'Algorithm'}, {'span': 'mean squared error', 'entity': 'Metrics'}, {'span': 'MSE', 'entity': 'Metrics'}]'

Example #3: To allow for multiple entities, a separate Hinge loss is computed for each capsule.
Expected output: 'entities: [{'span': 'Hinge loss', 'entity': 'Metrics'}]'

Example #4: Since the early 1990s, it has been recommended by several authorities, including the Audio Engineering Society that measurements of dynamic range be made with an audio signal present, which is then filtered out in the noise floor measurement used in determining dynamic range. This avoids questionable measurements based on the use of blank media, or muting circuits.
Expected output: 'entities: [{'span': 'Audio Engineering Society', 'entity': 'Organisation'}, {'span': 'audio signal', 'entity': 'Misc'}, {'span': 'noise floor measurement', 'entity': 'Metrics'}]'

Example #5: If the signal is further ergodic, all sample paths exhibits the same time-average and thus mathR _ x ^ { n / T _ 0 } (\ tau) = \ widehat { R } _ x ^ { n / T _ 0 } (\ tau) / math in mean square error sense.
Expected output: 'entities: [{'span': 'mean square error', 'entity': 'Metrics'}]'

