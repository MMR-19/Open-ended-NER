Example #1: In 2002 Hutter, with Jürgen Schmidhuber and Shane Legg, developed and published a mathematical theory of artificial general intelligence based on idealised intelligent agents and reward-motivated reinforcement learning.
Expected output: 'entities: [{'span': 'Hutter', 'entity': 'Researcher'}, {'span': 'Jürgen Schmidhuber', 'entity': 'Researcher'}, {'span': 'Shane Legg', 'entity': 'Researcher'}, {'span': 'artificial general intelligence', 'entity': 'Field'}, {'span': 'intelligent agents', 'entity': 'Misc'}, {'span': 'reinforcement learning', 'entity': 'Field'}]'

Example #2: 59, pp. 2547-2553, Oct. 2011 In one dimensional polynomial-based memory (or memoryless) DPD, in order to solve for the digital pre-distorter polynomials coefficients and minimize the mean squared error (MSE), the distorted output of the nonlinear system must be over-sampled at a rate that enables the capture of the nonlinear products of the order of the digital pre-distorter.
Expected output: 'entities: [{'span': 'one dimensional polynomial-based memory', 'entity': 'Misc'}, {'span': 'DPD', 'entity': 'Misc'}, {'span': 'mean squared error', 'entity': 'Metrics'}, {'span': 'MSE', 'entity': 'Metrics'}]'

Example #3: DeepFace is a deep learning facial recognition system created by a research group at Facebook.
Expected output: 'entities: [{'span': 'DeepFace', 'entity': 'Product'}, {'span': 'deep learning', 'entity': 'Field'}, {'span': 'facial recognition', 'entity': 'Task'}, {'span': 'Facebook', 'entity': 'Organisation'}]'

Example #4: Linear predictive coding (LPC), a form of speech coding, began development with the work Fumitada Itakura of Nagoya University and Shuzo Saito of Nippon Telegraph and Telephone (NTT) in 1966.
Expected output: 'entities: [{'span': 'Linear predictive coding', 'entity': 'Algorithm'}, {'span': 'LPC', 'entity': 'Algorithm'}, {'span': 'speech coding', 'entity': 'Task'}, {'span': 'Fumitada Itakura', 'entity': 'Researcher'}, {'span': 'Nagoya University', 'entity': 'University'}, {'span': 'Shuzo Saito', 'entity': 'Researcher'}, {'span': 'Nippon Telegraph and Telephone', 'entity': 'University'}, {'span': 'NTT', 'entity': 'University'}]'

Example #5: One can use the OSD algorithm to derive math O (\ sqrt { T }) / math regret bounds for the online version of Support vector machine for classification, which use the hinge loss math v _ t (w) = \ max \ { 0, 1 - y _ t (w \ cdot x _ t) \ } / math
Expected output: 'entities: [{'span': 'OSD algorithm', 'entity': 'Algorithm'}, {'span': 'Support vector machine', 'entity': 'Algorithm'}, {'span': 'classification', 'entity': 'Task'}, {'span': 'hinge loss', 'entity': 'Metrics'}]'

