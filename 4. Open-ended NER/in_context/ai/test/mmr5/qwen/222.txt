Example #1: Artificial neural networks are computational models that excel at machine learning and pattern recognition.
Expected output: 'entities: [{'span': 'Artificial neural networks', 'entity': 'Algorithm'}, {'span': 'machine learning', 'entity': 'Field'}, {'span': 'pattern recognition', 'entity': 'Field'}]'

Example #2: Haralick is a Fellow of IEEE for his contributions in computer vision and image processing and a Fellow of the International Association for Pattern Recognition (IAPR) for his contributions in pattern recognition, image processing, and for service to IAPR.
Expected output: 'entities: [{'span': 'Haralick', 'entity': 'Researcher'}, {'span': 'IEEE', 'entity': 'Organisation'}, {'span': 'computer vision', 'entity': 'Field'}, {'span': 'image processing', 'entity': 'Field'}, {'span': 'International Association for Pattern Recognition', 'entity': 'Organisation'}, {'span': 'IAPR', 'entity': 'Organisation'}, {'span': 'pattern recognition', 'entity': 'Field'}]'

Example #3: Descendants of the CLIPS language include Jess (rule-based portion of CLIPS rewritten in Java, it later grew up in different direction), JESS was originally inspired
Expected output: 'entities: [{'span': 'CLIPS', 'entity': 'Programming Language'}, {'span': 'Jess', 'entity': 'Programming Language'}, {'span': 'Java', 'entity': 'Programming Language'}, {'span': 'JESS', 'entity': 'Programming Language'}]'

Example #4: A number of groups and companies are researching pose estimation, including groups at Brown University, Carnegie Mellon University, MPI Saarbruecken, Stanford University, the University of California, San Diego, the University of Toronto, the École Centrale Paris, ETH Zurich, National University of Sciences and Technology (NUST), and the University of California, Irvine.
Expected output: 'entities: [{'span': 'pose estimation', 'entity': 'Task'}, {'span': 'Brown University', 'entity': 'University'}, {'span': 'Carnegie Mellon University', 'entity': 'University'}, {'span': 'MPI Saarbruecken', 'entity': 'University'}, {'span': 'Stanford University', 'entity': 'University'}, {'span': 'University of California, San Diego', 'entity': 'University'}, {'span': 'University of Toronto', 'entity': 'University'}, {'span': 'École Centrale Paris', 'entity': 'University'}, {'span': 'ETH Zurich', 'entity': 'University'}, {'span': 'National University of Sciences and Technology', 'entity': 'University'}, {'span': 'NUST', 'entity': 'University'}, {'span': 'University of California, Irvine', 'entity': 'University'}]'

Example #5: Feature extraction and dimension reduction can be combined in one step using Principal Component Analysis (PCA), linear discriminant analysis (LDA), or canonical correlation analysis (CCA) techniques as a pre-processing step, followed by clustering by k -NN on feature vectors in reduced-dimension space.
Expected output: 'entities: [{'span': 'Feature extraction', 'entity': 'Task'}, {'span': 'dimension reduction', 'entity': 'Task'}, {'span': 'Principal Component Analysis', 'entity': 'Algorithm'}, {'span': 'PCA', 'entity': 'Algorithm'}, {'span': 'linear discriminant analysis', 'entity': 'Algorithm'}, {'span': 'LDA', 'entity': 'Algorithm'}, {'span': 'canonical correlation analysis', 'entity': 'Algorithm'}, {'span': 'CCA', 'entity': 'Algorithm'}, {'span': 'pre-processing', 'entity': 'Misc'}, {'span': 'k -NN', 'entity': 'Algorithm'}]'

