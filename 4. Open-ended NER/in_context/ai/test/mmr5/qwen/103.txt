Example #1: The parameters Î² are typically estimated by maximum likelihood.
Expected output: 'entities: [{'span': 'maximum likelihood', 'entity': 'Metrics'}]'

Example #2: An autoencoder is a type of artificial neural network used to learn Feature learning in an unsupervised learning manner.
Expected output: 'entities: [{'span': 'autoencoder', 'entity': 'Algorithm'}, {'span': 'artificial neural network', 'entity': 'Algorithm'}, {'span': 'Feature learning', 'entity': 'Task'}, {'span': 'unsupervised learning', 'entity': 'Field'}]'

Example #3: The NIST metric is based on the BLEU metric, but with some alterations.
Expected output: 'entities: [{'span': 'NIST metric', 'entity': 'Metrics'}, {'span': 'BLEU metric', 'entity': 'Metrics'}]'

Example #4: Since 2002, perceptron training has become popular in the field of natural language processing for such tasks as part-of-speech tagging and syntactic parsing (Collins, 2002).
Expected output: 'entities: [{'span': 'natural language processing', 'entity': 'Field'}, {'span': 'part-of-speech tagging', 'entity': 'Task'}, {'span': 'syntactic parsing', 'entity': 'Task'}, {'span': 'Collins', 'entity': 'Researcher'}]'

Example #5: One can use the OSD algorithm to derive math O (\ sqrt { T }) / math regret bounds for the online version of Support vector machine for classification, which use the hinge loss math v _ t (w) = \ max \ { 0, 1 - y _ t (w \ cdot x _ t) \ } / math
Expected output: 'entities: [{'span': 'OSD algorithm', 'entity': 'Algorithm'}, {'span': 'Support vector machine', 'entity': 'Algorithm'}, {'span': 'classification', 'entity': 'Task'}, {'span': 'hinge loss', 'entity': 'Metrics'}]'

