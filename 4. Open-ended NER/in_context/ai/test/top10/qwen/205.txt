Example #1: Artificial neural networks are computational models that excel at machine learning and pattern recognition.
Expected output: 'entities: [{'span': 'Artificial neural networks', 'entity': 'Algorithm'}, {'span': 'machine learning', 'entity': 'Field'}, {'span': 'pattern recognition', 'entity': 'Field'}]'

Example #2: In the late 1980s, two Netherlands universities, University of Groningen and University of Twente, jointly began a project called Knowledge Graphs, which are semantic networks but with the added constraint that edges are restricted to be from a limited set of possible relations, to facilitate algebras on the graph.
Expected output: 'entities: [{'span': 'Netherlands', 'entity': 'Country'}, {'span': 'University of Groningen', 'entity': 'University'}, {'span': 'University of Twente', 'entity': 'University'}, {'span': 'Knowledge Graphs', 'entity': 'Product'}, {'span': 'semantic networks', 'entity': 'Algorithm'}]'

Example #3: Self-organizing maps differ from other artificial neural networks as they apply competitive learning as opposed to error-correction learning such as backpropagation with gradient descent), and in the sense that they use a neighborhood function to preserve the topological properties of the input space.
Expected output: 'entities: [{'span': 'artificial neural networks', 'entity': 'Algorithm'}, {'span': 'competitive learning', 'entity': 'Algorithm'}, {'span': 'error-correction learning', 'entity': 'Algorithm'}, {'span': 'backpropagation', 'entity': 'Algorithm'}, {'span': 'gradient descent', 'entity': 'Algorithm'}, {'span': 'topological properties', 'entity': 'Misc'}]'

Example #4: An autoencoder is a type of artificial neural network used to learn Feature learning in an unsupervised learning manner.
Expected output: 'entities: [{'span': 'autoencoder', 'entity': 'Algorithm'}, {'span': 'artificial neural network', 'entity': 'Algorithm'}, {'span': 'Feature learning', 'entity': 'Task'}, {'span': 'unsupervised learning', 'entity': 'Field'}]'

Example #5: Between 2009 and 2012, the recurrent neural network s and deep feedforward neural network s developed in the research group of Jürgen Schmidhuber at the Swiss AI Lab IDSIA have won eight international competitions in pattern recognition and machine learning.
Expected output: 'entities: [{'span': 'recurrent neural network', 'entity': 'Algorithm'}, {'span': 'deep feedforward neural network', 'entity': 'Algorithm'}, {'span': 'Jürgen Schmidhuber', 'entity': 'Researcher'}, {'span': 'Swiss AI Lab IDSIA', 'entity': 'Organisation'}, {'span': 'pattern recognition', 'entity': 'Field'}, {'span': 'machine learning', 'entity': 'Field'}]'

Example #6: Since 2002, perceptron training has become popular in the field of natural language processing for such tasks as part-of-speech tagging and syntactic parsing (Collins, 2002).
Expected output: 'entities: [{'span': 'natural language processing', 'entity': 'Field'}, {'span': 'part-of-speech tagging', 'entity': 'Task'}, {'span': 'syntactic parsing', 'entity': 'Task'}, {'span': 'Collins', 'entity': 'Researcher'}]'

Example #7: Examples of supervised learning are Naive Bayes classifier, Support vector machine, mixtures of Gaussians, and network.
Expected output: 'entities: [{'span': 'supervised learning', 'entity': 'Field'}, {'span': 'Naive Bayes classifier', 'entity': 'Algorithm'}, {'span': 'Support vector machine', 'entity': 'Algorithm'}, {'span': 'mixtures of Gaussians', 'entity': 'Algorithm'}, {'span': 'network', 'entity': 'Algorithm'}]'

Example #8: Speech recognition and speech synthesis deal with how spoken language can be understood or created using computers.
Expected output: 'entities: [{'span': 'Speech recognition', 'entity': 'Task'}, {'span': 'speech synthesis', 'entity': 'Task'}]'

Example #9: The first attempt at end-to-end ASR was with Connectionist Temporal Classification (CTC) -based systems introduced by Alex Graves of Google DeepMind and Navdeep Jaitly of the University of Toronto in 2014.
Expected output: 'entities: [{'span': 'end-to-end ASR', 'entity': 'Task'}, {'span': 'Connectionist Temporal Classification', 'entity': 'Algorithm'}, {'span': 'CTC', 'entity': 'Algorithm'}, {'span': 'Alex Graves', 'entity': 'Researcher'}, {'span': 'Google DeepMind', 'entity': 'Organisation'}, {'span': 'Navdeep Jaitly', 'entity': 'Researcher'}, {'span': 'University of Toronto', 'entity': 'University'}]'

Example #10: The task of recognizing named entities in text is Named Entity Recognition while the task of determining the identity of the named entities mentioned in text is called Entity Linking.
Expected output: 'entities: [{'span': 'recognizing named entities in text', 'entity': 'Task'}, {'span': 'Named Entity Recognition', 'entity': 'Task'}, {'span': 'Entity Linking', 'entity': 'Task'}]'

