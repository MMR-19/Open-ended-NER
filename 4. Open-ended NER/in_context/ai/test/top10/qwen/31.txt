Example #1: Speech recognition and speech synthesis deal with how spoken language can be understood or created using computers.
Expected output: 'entities: [{'span': 'Speech recognition', 'entity': 'Task'}, {'span': 'speech synthesis', 'entity': 'Task'}]'

Example #2: The input is called speech recognition and the output is called speech synthesis.
Expected output: 'entities: [{'span': 'speech recognition', 'entity': 'Task'}, {'span': 'speech synthesis', 'entity': 'Task'}]'

Example #3: The technique used in creating eigenfaces and using them for recognition is also used outside of face recognition: handwriting recognition, lip reading, voice recognition, sign language / hand gestures interpretation and medical imaging analysis.
Expected output: 'entities: [{'span': 'eigenfaces', 'entity': 'Misc'}, {'span': 'face recognition', 'entity': 'Task'}, {'span': 'handwriting recognition', 'entity': 'Task'}, {'span': 'lip reading', 'entity': 'Task'}, {'span': 'voice recognition', 'entity': 'Task'}, {'span': 'sign language', 'entity': 'Task'}, {'span': 'hand gestures interpretation', 'entity': 'Task'}, {'span': 'medical imaging analysis', 'entity': 'Field'}]'

Example #4: The speech synthesis is verging on being completely indistinguishable from a real human's voice with the 2016 introduction of the voice editing and generation software Adobe Voco, a prototype slated to be a part of the Adobe Creative Suite and DeepMind WaveNet, a prototype from Google.
Expected output: 'entities: [{'span': 'speech synthesis', 'entity': 'Task'}, {'span': 'Adobe Voco', 'entity': 'Product'}, {'span': 'Adobe Creative Suite', 'entity': 'Product'}, {'span': 'DeepMind', 'entity': 'Organisation'}, {'span': 'WaveNet', 'entity': 'Product'}, {'span': 'Google', 'entity': 'Organisation'}]'

Example #5: Modern Windows desktop systems can use SAPI 4 and SAPI 5 components to support speech synthesis and speech.
Expected output: 'entities: [{'span': 'Windows desktop systems', 'entity': 'Product'}, {'span': 'SAPI 4', 'entity': 'Product'}, {'span': 'SAPI 5', 'entity': 'Product'}, {'span': 'speech synthesis', 'entity': 'Task'}, {'span': 'speech', 'entity': 'Task'}]'

Example #6: Haralick is a Fellow of IEEE for his contributions in computer vision and image processing and a Fellow of the International Association for Pattern Recognition (IAPR) for his contributions in pattern recognition, image processing, and for service to IAPR.
Expected output: 'entities: [{'span': 'Haralick', 'entity': 'Researcher'}, {'span': 'IEEE', 'entity': 'Organisation'}, {'span': 'computer vision', 'entity': 'Field'}, {'span': 'image processing', 'entity': 'Field'}, {'span': 'International Association for Pattern Recognition', 'entity': 'Organisation'}, {'span': 'IAPR', 'entity': 'Organisation'}, {'span': 'pattern recognition', 'entity': 'Field'}]'

Example #7: Some less widely spoken languages use the open-source eSpeak synthesizer for their speech; producing a robotic, awkward voice that may be difficult to understand.
Expected output: 'entities: [{'span': 'eSpeak synthesizer', 'entity': 'Product'}]'

Example #8: He is a Fellow of the American Association for the Advancement of Science, Association for the Advancement Artificial Intelligence, and Cognitive Science Society, and an editor of the J. Automated Reasoning, J. Learning Sciences, and J. Applied Ontology.
Expected output: 'entities: [{'span': 'American Association for the Advancement of Science', 'entity': 'Organisation'}, {'span': 'Association for the Advancement Artificial Intelligence', 'entity': 'Conference'}, {'span': 'Cognitive Science Society', 'entity': 'Organisation'}, {'span': 'J. Automated Reasoning', 'entity': 'Conference'}, {'span': 'J. Learning Sciences', 'entity': 'Conference'}, {'span': 'J. Applied Ontology', 'entity': 'Conference'}]'

Example #9: Applications include object recognition, robotic mapping and navigation, image stitching, 3D modeling, gesture recognition, video tracking, individual identification of wildlife and match moving.
Expected output: 'entities: [{'span': 'object recognition', 'entity': 'Task'}, {'span': 'robotic mapping', 'entity': 'Task'}, {'span': 'navigation', 'entity': 'Task'}, {'span': 'image stitching', 'entity': 'Task'}, {'span': '3D modeling', 'entity': 'Task'}, {'span': 'gesture recognition', 'entity': 'Task'}, {'span': 'video tracking', 'entity': 'Task'}, {'span': 'individual identification of wildlife', 'entity': 'Task'}, {'span': 'match moving', 'entity': 'Task'}]'

Example #10: During the 1990s, encouraged by successes in speech recognition and speech synthesis, research began into speech translation with the development of the German Verbmobil project.
Expected output: 'entities: [{'span': 'speech recognition', 'entity': 'Task'}, {'span': 'speech synthesis', 'entity': 'Task'}, {'span': 'speech translation', 'entity': 'Task'}, {'span': 'German', 'entity': 'Misc'}, {'span': 'Verbmobil project', 'entity': 'Misc'}]'

