Example #1: tity contains a collection of visualization tools and algorithms for data analysis and predictive modeling, together with graphical user interfaces for easy access to these functions. but the more recent fully Java -based version (Weka 3), for which development started in 1997, is now used in many different application areas, in particular for educational purposes and research.
Expected output: 'entities: [{'span': 'tity', 'entity': 'Product'}, {'span': 'data analysis', 'entity': 'Field'}, {'span': 'predictive modeling', 'entity': 'Task'}, {'span': 'graphical user interfaces', 'entity': 'Misc'}, {'span': 'Java', 'entity': 'Programming Language'}, {'span': 'Weka 3', 'entity': 'Product'}]'

Example #2: Self-organizing maps differ from other artificial neural networks as they apply competitive learning as opposed to error-correction learning such as backpropagation with gradient descent), and in the sense that they use a neighborhood function to preserve the topological properties of the input space.
Expected output: 'entities: [{'span': 'artificial neural networks', 'entity': 'Algorithm'}, {'span': 'competitive learning', 'entity': 'Algorithm'}, {'span': 'error-correction learning', 'entity': 'Algorithm'}, {'span': 'backpropagation', 'entity': 'Algorithm'}, {'span': 'gradient descent', 'entity': 'Algorithm'}, {'span': 'topological properties', 'entity': 'Misc'}]'

Example #3: Since 2002, perceptron training has become popular in the field of natural language processing for such tasks as part-of-speech tagging and syntactic parsing (Collins, 2002).
Expected output: 'entities: [{'span': 'natural language processing', 'entity': 'Field'}, {'span': 'part-of-speech tagging', 'entity': 'Task'}, {'span': 'syntactic parsing', 'entity': 'Task'}, {'span': 'Collins', 'entity': 'Researcher'}]'

Example #4: Feature extraction and dimension reduction can be combined in one step using principal component analysis (PCA), linear discriminant analysis (LDA), canonical correlation analysis (CCA), or non-negative matrix factorization (NMF) techniques as a pre-processing step followed by clustering by K-NN on feature vectors in reduced-dimension space.
Expected output: 'entities: [{'span': 'Feature extraction', 'entity': 'Task'}, {'span': 'dimension reduction', 'entity': 'Task'}, {'span': 'principal component analysis', 'entity': 'Algorithm'}, {'span': 'PCA', 'entity': 'Algorithm'}, {'span': 'linear discriminant analysis', 'entity': 'Algorithm'}, {'span': 'LDA', 'entity': 'Algorithm'}, {'span': 'canonical correlation analysis', 'entity': 'Algorithm'}, {'span': 'CCA', 'entity': 'Algorithm'}, {'span': 'non-negative matrix factorization', 'entity': 'Algorithm'}, {'span': 'NMF', 'entity': 'Algorithm'}, {'span': 'pre-processing step', 'entity': 'Misc'}, {'span': 'K-NN', 'entity': 'Algorithm'}, {'span': 'feature vectors', 'entity': 'Misc'}]'

Example #5: Feature extraction and dimension reduction can be combined in one step using Principal Component Analysis (PCA), linear discriminant analysis (LDA), or canonical correlation analysis (CCA) techniques as a pre-processing step, followed by clustering by k -NN on feature vectors in reduced-dimension space.
Expected output: 'entities: [{'span': 'Feature extraction', 'entity': 'Task'}, {'span': 'dimension reduction', 'entity': 'Task'}, {'span': 'Principal Component Analysis', 'entity': 'Algorithm'}, {'span': 'PCA', 'entity': 'Algorithm'}, {'span': 'linear discriminant analysis', 'entity': 'Algorithm'}, {'span': 'LDA', 'entity': 'Algorithm'}, {'span': 'canonical correlation analysis', 'entity': 'Algorithm'}, {'span': 'CCA', 'entity': 'Algorithm'}, {'span': 'pre-processing', 'entity': 'Misc'}, {'span': 'k -NN', 'entity': 'Algorithm'}]'

