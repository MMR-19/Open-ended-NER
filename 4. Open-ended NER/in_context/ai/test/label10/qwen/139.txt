Example #1: The term computational linguistics itself was first coined by David Hays, a founding member of both the Association for Computational Linguistics and the International Committee on Computational Linguistics (ICCL).
Expected output: 'entities: [{'span': 'computational linguistics', 'entity': 'Field'}, {'span': 'David Hays', 'entity': 'Researcher'}, {'span': 'Association for Computational Linguistics', 'entity': 'Conference'}, {'span': 'International Committee on Computational Linguistics', 'entity': 'Organisation'}, {'span': 'ICCL', 'entity': 'Organisation'}]'

Example #2: Between 2009 and 2012, the recurrent neural network s and deep feedforward neural network s developed in the research group of Jürgen Schmidhuber at the Swiss AI Lab IDSIA have won eight international competitions in pattern recognition and machine learning.
Expected output: 'entities: [{'span': 'recurrent neural network', 'entity': 'Algorithm'}, {'span': 'deep feedforward neural network', 'entity': 'Algorithm'}, {'span': 'Jürgen Schmidhuber', 'entity': 'Researcher'}, {'span': 'Swiss AI Lab IDSIA', 'entity': 'Organisation'}, {'span': 'pattern recognition', 'entity': 'Field'}, {'span': 'machine learning', 'entity': 'Field'}]'

Example #3: Self-organizing maps differ from other artificial neural networks as they apply competitive learning as opposed to error-correction learning such as backpropagation with gradient descent), and in the sense that they use a neighborhood function to preserve the topological properties of the input space.
Expected output: 'entities: [{'span': 'artificial neural networks', 'entity': 'Algorithm'}, {'span': 'competitive learning', 'entity': 'Algorithm'}, {'span': 'error-correction learning', 'entity': 'Algorithm'}, {'span': 'backpropagation', 'entity': 'Algorithm'}, {'span': 'gradient descent', 'entity': 'Algorithm'}, {'span': 'topological properties', 'entity': 'Misc'}]'

Example #4: CycL in computer science and artificial intelligence is an ontology language used by Doug Lenat's Cyc artificial project.
Expected output: 'entities: [{'span': 'CycL', 'entity': 'Programming Language'}, {'span': 'computer science', 'entity': 'Field'}, {'span': 'artificial intelligence', 'entity': 'Field'}, {'span': 'ontology language', 'entity': 'Misc'}, {'span': 'Doug Lenat', 'entity': 'Researcher'}, {'span': 'Cyc artificial project', 'entity': 'Misc'}]'

Example #5: The National Science Foundation was an umbrella for the National Aeronautics and Space Administration (NASA), the US Department of Energy, the US Department of Commerce NIST, the US Department of Defense, Defense Advanced Research Projects Agency (DARPA), and the Office of Naval Research coordinated studies to inform strategic planners in their deliberations.
Expected output: 'entities: [{'span': 'National Science Foundation', 'entity': 'Organisation'}, {'span': 'National Aeronautics and Space Administration', 'entity': 'Organisation'}, {'span': 'NASA', 'entity': 'Organisation'}, {'span': 'US Department of Energy', 'entity': 'Organisation'}, {'span': 'US Department of Commerce NIST', 'entity': 'Organisation'}, {'span': 'US Department of Defense', 'entity': 'Organisation'}, {'span': 'Defense Advanced Research Projects Agency', 'entity': 'Organisation'}, {'span': 'DARPA', 'entity': 'Organisation'}, {'span': 'Office of Naval Research', 'entity': 'Organisation'}]'

Example #6: Boris Katz, (born October 5, 1947, Chișinău, Moldavian SSR, Soviet Union, (now Chișinău, Moldova)) is a principal American research scientist (computer scientist) at the MIT Computer Science and Artificial Intelligence Laboratory at the Massachusetts Institute of Technology in Cambridge and head of the Laboratory's InfoLab Group.
Expected output: 'entities: [{'span': 'Boris Katz', 'entity': 'Researcher'}, {'span': 'Chișinău', 'entity': 'Location'}, {'span': 'Moldavian SSR', 'entity': 'Location'}, {'span': 'Soviet Union', 'entity': 'Country'}, {'span': 'Moldova', 'entity': 'Country'}, {'span': 'MIT Computer Science and Artificial Intelligence Laboratory', 'entity': 'Organisation'}, {'span': 'Massachusetts Institute of Technology', 'entity': 'Organisation'}, {'span': 'Cambridge', 'entity': 'University'}, {'span': 'InfoLab Group', 'entity': 'Organisation'}]'

Example #7: The first palletizing robot was introduced in 1963 by the Fuji Yusoki Kogyo Company. by KUKA robotics in Germany, and the Programmable Universal Machine for Assembly was invented by Victor Scheinman in 1976, and the design was sold to Unimation.
Expected output: 'entities: [{'span': 'palletizing robot', 'entity': 'Product'}, {'span': 'Fuji Yusoki Kogyo Company.', 'entity': 'Organisation'}, {'span': 'KUKA robotics', 'entity': 'Organisation'}, {'span': 'Germany', 'entity': 'Country'}, {'span': 'Programmable Universal Machine for Assembly', 'entity': 'Product'}, {'span': 'Victor Scheinman', 'entity': 'Researcher'}, {'span': 'Unimation', 'entity': 'Organisation'}]'

Example #8: In 2002 Hutter, with Jürgen Schmidhuber and Shane Legg, developed and published a mathematical theory of artificial general intelligence based on idealised intelligent agents and reward-motivated reinforcement learning.
Expected output: 'entities: [{'span': 'Hutter', 'entity': 'Researcher'}, {'span': 'Jürgen Schmidhuber', 'entity': 'Researcher'}, {'span': 'Shane Legg', 'entity': 'Researcher'}, {'span': 'artificial general intelligence', 'entity': 'Field'}, {'span': 'intelligent agents', 'entity': 'Misc'}, {'span': 'reinforcement learning', 'entity': 'Field'}]'

Example #9: The first attempt at end-to-end ASR was with Connectionist Temporal Classification (CTC) -based systems introduced by Alex Graves of Google DeepMind and Navdeep Jaitly of the University of Toronto in 2014.
Expected output: 'entities: [{'span': 'end-to-end ASR', 'entity': 'Task'}, {'span': 'Connectionist Temporal Classification', 'entity': 'Algorithm'}, {'span': 'CTC', 'entity': 'Algorithm'}, {'span': 'Alex Graves', 'entity': 'Researcher'}, {'span': 'Google DeepMind', 'entity': 'Organisation'}, {'span': 'Navdeep Jaitly', 'entity': 'Researcher'}, {'span': 'University of Toronto', 'entity': 'University'}]'

Example #10: Since 2002, perceptron training has become popular in the field of natural language processing for such tasks as part-of-speech tagging and syntactic parsing (Collins, 2002).
Expected output: 'entities: [{'span': 'natural language processing', 'entity': 'Field'}, {'span': 'part-of-speech tagging', 'entity': 'Task'}, {'span': 'syntactic parsing', 'entity': 'Task'}, {'span': 'Collins', 'entity': 'Researcher'}]'

