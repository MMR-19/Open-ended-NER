Example #1: With the emergence of conversational assistants such as Apple's Siri, Amazon Alexa, Google Assistant, Microsoft Cortana, and Samsung's Bixby, Voice Portals can now be accessed through mobile devices and Far Field voice smart speakers such as the Amazon Echo and Google Home.
Expected output: 'entities: [{'span': "Apple's Siri", 'entity': 'Product'}, {'span': 'Amazon Alexa', 'entity': 'Product'}, {'span': 'Google Assistant', 'entity': 'Product'}, {'span': 'Microsoft Cortana', 'entity': 'Product'}, {'span': "Samsung's Bixby", 'entity': 'Product'}, {'span': 'Voice Portals', 'entity': 'Product'}, {'span': 'Far Field voice smart speakers', 'entity': 'Product'}, {'span': 'Amazon Echo', 'entity': 'Product'}, {'span': 'Google Home', 'entity': 'Product'}]'

Example #2: The speech synthesis is verging on being completely indistinguishable from a real human's voice with the 2016 introduction of the voice editing and generation software Adobe Voco, a prototype slated to be a part of the Adobe Creative Suite and DeepMind WaveNet, a prototype from Google.
Expected output: 'entities: [{'span': 'speech synthesis', 'entity': 'Task'}, {'span': 'Adobe Voco', 'entity': 'Product'}, {'span': 'Adobe Creative Suite', 'entity': 'Product'}, {'span': 'DeepMind', 'entity': 'Organisation'}, {'span': 'WaveNet', 'entity': 'Product'}, {'span': 'Google', 'entity': 'Organisation'}]'

Example #3: Speech recognition and speech synthesis deal with how spoken language can be understood or created using computers.
Expected output: 'entities: [{'span': 'Speech recognition', 'entity': 'Task'}, {'span': 'speech synthesis', 'entity': 'Task'}]'

Example #4: The input is called speech recognition and the output is called speech synthesis.
Expected output: 'entities: [{'span': 'speech recognition', 'entity': 'Task'}, {'span': 'speech synthesis', 'entity': 'Task'}]'

Example #5: Modern Windows desktop systems can use SAPI 4 and SAPI 5 components to support speech synthesis and speech.
Expected output: 'entities: [{'span': 'Windows desktop systems', 'entity': 'Product'}, {'span': 'SAPI 4', 'entity': 'Product'}, {'span': 'SAPI 5', 'entity': 'Product'}, {'span': 'speech synthesis', 'entity': 'Task'}, {'span': 'speech', 'entity': 'Task'}]'

Example #6: The sigmoid function s and derivatives used in the package were originally included in the package, from version 0.8.0 onwards, these were released in a separate R package sigmoid, with the intention to enable more general use.
Expected output: 'entities: [{'span': 'sigmoid function', 'entity': 'Algorithm'}, {'span': 'R', 'entity': 'Programming Language'}, {'span': 'sigmoid', 'entity': 'Algorithm'}]'

Example #7: Some less widely spoken languages use the open-source eSpeak synthesizer for their speech; producing a robotic, awkward voice that may be difficult to understand.
Expected output: 'entities: [{'span': 'eSpeak synthesizer', 'entity': 'Product'}]'

Example #8: During the 1990s, encouraged by successes in speech recognition and speech synthesis, research began into speech translation with the development of the German Verbmobil project.
Expected output: 'entities: [{'span': 'speech recognition', 'entity': 'Task'}, {'span': 'speech synthesis', 'entity': 'Task'}, {'span': 'speech translation', 'entity': 'Task'}, {'span': 'German', 'entity': 'Misc'}, {'span': 'Verbmobil project', 'entity': 'Misc'}]'

Example #9: Voice user interfaces that interpret and manage conversational state are challenging to design due to the inherent difficulty of integrating complex natural language processing tasks like coreference resolution, named-entity recognition, information retrieval, and dialog management.
Expected output: 'entities: [{'span': 'Voice user interfaces', 'entity': 'Product'}, {'span': 'natural language processing', 'entity': 'Field'}, {'span': 'coreference resolution', 'entity': 'Task'}, {'span': 'named-entity recognition', 'entity': 'Task'}, {'span': 'information retrieval', 'entity': 'Task'}, {'span': 'dialog management', 'entity': 'Task'}]'

Example #10: It also created flexible intelligent AGV applications, designing the Motivity control system used by RMT Robotics to develop its ADAM iAGV (Self-Guided Vehicle), used for complex pick and place operations, in conjunction with gantry systems and industrial robot arms, used in first-tier auto supply factories to move products from process to process in non-linear layouts.
Expected output: 'entities: [{'span': 'AGV', 'entity': 'Product'}, {'span': 'Motivity control system', 'entity': 'Product'}, {'span': 'RMT Robotics', 'entity': 'Organisation'}, {'span': 'ADAM iAGV', 'entity': 'Product'}, {'span': 'gantry systems', 'entity': 'Product'}, {'span': 'industrial robot arms', 'entity': 'Product'}, {'span': 'non-linear layouts', 'entity': 'Misc'}]'

