Example #1: If we use least squares to fit a function in the form of a hyperplane ŷ = a + β supT / sup x to the data (x sub i / sub, y sub i / sub) sub 1 ≤ i ≤ n / sub, we could then assess the fit using the mean squared error (MSE).
Expected output: 'entities: [{'span': 'least squares', 'entity': 'Algorithm'}, {'span': 'mean squared error', 'entity': 'Metrics'}, {'span': 'MSE', 'entity': 'Metrics'}]'

Example #2: One can use the OSD algorithm to derive math O (\ sqrt { T }) / math regret bounds for the online version of Support vector machine for classification, which use the hinge loss math v _ t (w) = \ max \ { 0, 1 - y _ t (w \ cdot x _ t) \ } / math
Expected output: 'entities: [{'span': 'OSD algorithm', 'entity': 'Algorithm'}, {'span': 'Support vector machine', 'entity': 'Algorithm'}, {'span': 'classification', 'entity': 'Task'}, {'span': 'hinge loss', 'entity': 'Metrics'}]'

Example #3: tity contains a collection of visualization tools and algorithms for data analysis and predictive modeling, together with graphical user interfaces for easy access to these functions. but the more recent fully Java -based version (Weka 3), for which development started in 1997, is now used in many different application areas, in particular for educational purposes and research.
Expected output: 'entities: [{'span': 'tity', 'entity': 'Product'}, {'span': 'data analysis', 'entity': 'Field'}, {'span': 'predictive modeling', 'entity': 'Task'}, {'span': 'graphical user interfaces', 'entity': 'Misc'}, {'span': 'Java', 'entity': 'Programming Language'}, {'span': 'Weka 3', 'entity': 'Product'}]'

Example #4: Self-organizing maps differ from other artificial neural networks as they apply competitive learning as opposed to error-correction learning such as backpropagation with gradient descent), and in the sense that they use a neighborhood function to preserve the topological properties of the input space.
Expected output: 'entities: [{'span': 'artificial neural networks', 'entity': 'Algorithm'}, {'span': 'competitive learning', 'entity': 'Algorithm'}, {'span': 'error-correction learning', 'entity': 'Algorithm'}, {'span': 'backpropagation', 'entity': 'Algorithm'}, {'span': 'gradient descent', 'entity': 'Algorithm'}, {'span': 'topological properties', 'entity': 'Misc'}]'

Example #5: The sigmoid function s and derivatives used in the package were originally included in the package, from version 0.8.0 onwards, these were released in a separate R package sigmoid, with the intention to enable more general use.
Expected output: 'entities: [{'span': 'sigmoid function', 'entity': 'Algorithm'}, {'span': 'R', 'entity': 'Programming Language'}, {'span': 'sigmoid', 'entity': 'Algorithm'}]'

Example #6: In the late 1980s, two Netherlands universities, University of Groningen and University of Twente, jointly began a project called Knowledge Graphs, which are semantic networks but with the added constraint that edges are restricted to be from a limited set of possible relations, to facilitate algebras on the graph.
Expected output: 'entities: [{'span': 'Netherlands', 'entity': 'Country'}, {'span': 'University of Groningen', 'entity': 'University'}, {'span': 'University of Twente', 'entity': 'University'}, {'span': 'Knowledge Graphs', 'entity': 'Product'}, {'span': 'semantic networks', 'entity': 'Algorithm'}]'

Example #7: Also in regression analysis, mean squared error, often referred to as mean squared prediction error or out-of-sample mean squared error, can refer to the mean value of the squared deviations of the predictions from the TRUE values, over an out-of-sample test space, generated by a model estimated over a particular sample space.
Expected output: 'entities: [{'span': 'regression analysis', 'entity': 'Task'}, {'span': 'mean squared error', 'entity': 'Metrics'}, {'span': 'mean squared prediction error', 'entity': 'Metrics'}, {'span': 'out-of-sample mean squared error', 'entity': 'Metrics'}, {'span': 'squared deviations', 'entity': 'Misc'}]'

Example #8: 59, pp. 2547-2553, Oct. 2011 In one dimensional polynomial-based memory (or memoryless) DPD, in order to solve for the digital pre-distorter polynomials coefficients and minimize the mean squared error (MSE), the distorted output of the nonlinear system must be over-sampled at a rate that enables the capture of the nonlinear products of the order of the digital pre-distorter.
Expected output: 'entities: [{'span': 'one dimensional polynomial-based memory', 'entity': 'Misc'}, {'span': 'DPD', 'entity': 'Misc'}, {'span': 'mean squared error', 'entity': 'Metrics'}, {'span': 'MSE', 'entity': 'Metrics'}]'

Example #9: As for the results, the C-HOG and R-HOG block descriptors perform comparably, with the C-HOG descriptors maintaining a slight advantage in the detection miss rate at fixed FALSE positive rate s across both data sets.
Expected output: 'entities: [{'span': 'C-HOG', 'entity': 'Algorithm'}, {'span': 'R-HOG', 'entity': 'Algorithm'}, {'span': 'C-HOG descriptors', 'entity': 'Algorithm'}, {'span': 'FALSE positive rate', 'entity': 'Metrics'}]'

Example #10: With long-time collaborator Laurent Cohen, a neurologist at the Pitié-Salpêtrière Hospital in Paris, Dehaene also identified patients with lesions in different regions of the parietal lobe with impaired multiplication, but preserved subtraction (associated with lesions of the inferior parietal lobule) and others with impaired subtraction, but preserved multiplication (associated with lesions to the intraparietal sulcus).
Expected output: 'entities: [{'span': 'Laurent Cohen', 'entity': 'Researcher'}, {'span': 'Pitié-Salpêtrière Hospital', 'entity': 'Organisation'}, {'span': 'Paris', 'entity': 'Location'}, {'span': 'Dehaene', 'entity': 'Researcher'}, {'span': 'parietal lobe', 'entity': 'Misc'}, {'span': 'inferior parietal lobule', 'entity': 'Misc'}, {'span': 'intraparietal sulcus', 'entity': 'Misc'}]'

