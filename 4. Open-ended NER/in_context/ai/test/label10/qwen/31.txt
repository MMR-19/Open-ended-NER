Example #1: During the 1990s, encouraged by successes in speech recognition and speech synthesis, research began into speech translation with the development of the German Verbmobil project.
Expected output: 'entities: [{'span': 'speech recognition', 'entity': 'Task'}, {'span': 'speech synthesis', 'entity': 'Task'}, {'span': 'speech translation', 'entity': 'Task'}, {'span': 'German', 'entity': 'Misc'}, {'span': 'Verbmobil project', 'entity': 'Misc'}]'

Example #2: Haralick is a Fellow of IEEE for his contributions in computer vision and image processing and a Fellow of the International Association for Pattern Recognition (IAPR) for his contributions in pattern recognition, image processing, and for service to IAPR.
Expected output: 'entities: [{'span': 'Haralick', 'entity': 'Researcher'}, {'span': 'IEEE', 'entity': 'Organisation'}, {'span': 'computer vision', 'entity': 'Field'}, {'span': 'image processing', 'entity': 'Field'}, {'span': 'International Association for Pattern Recognition', 'entity': 'Organisation'}, {'span': 'IAPR', 'entity': 'Organisation'}, {'span': 'pattern recognition', 'entity': 'Field'}]'

Example #3: Linear predictive coding (LPC), a form of speech coding, began development with the work Fumitada Itakura of Nagoya University and Shuzo Saito of Nippon Telegraph and Telephone (NTT) in 1966.
Expected output: 'entities: [{'span': 'Linear predictive coding', 'entity': 'Algorithm'}, {'span': 'LPC', 'entity': 'Algorithm'}, {'span': 'speech coding', 'entity': 'Task'}, {'span': 'Fumitada Itakura', 'entity': 'Researcher'}, {'span': 'Nagoya University', 'entity': 'University'}, {'span': 'Shuzo Saito', 'entity': 'Researcher'}, {'span': 'Nippon Telegraph and Telephone', 'entity': 'University'}, {'span': 'NTT', 'entity': 'University'}]'

Example #4: Since 2002, perceptron training has become popular in the field of natural language processing for such tasks as part-of-speech tagging and syntactic parsing (Collins, 2002).
Expected output: 'entities: [{'span': 'natural language processing', 'entity': 'Field'}, {'span': 'part-of-speech tagging', 'entity': 'Task'}, {'span': 'syntactic parsing', 'entity': 'Task'}, {'span': 'Collins', 'entity': 'Researcher'}]'

Example #5: The speech synthesis is verging on being completely indistinguishable from a real human's voice with the 2016 introduction of the voice editing and generation software Adobe Voco, a prototype slated to be a part of the Adobe Creative Suite and DeepMind WaveNet, a prototype from Google.
Expected output: 'entities: [{'span': 'speech synthesis', 'entity': 'Task'}, {'span': 'Adobe Voco', 'entity': 'Product'}, {'span': 'Adobe Creative Suite', 'entity': 'Product'}, {'span': 'DeepMind', 'entity': 'Organisation'}, {'span': 'WaveNet', 'entity': 'Product'}, {'span': 'Google', 'entity': 'Organisation'}]'

Example #6: The technique used in creating eigenfaces and using them for recognition is also used outside of face recognition: handwriting recognition, lip reading, voice recognition, sign language / hand gestures interpretation and medical imaging analysis.
Expected output: 'entities: [{'span': 'eigenfaces', 'entity': 'Misc'}, {'span': 'face recognition', 'entity': 'Task'}, {'span': 'handwriting recognition', 'entity': 'Task'}, {'span': 'lip reading', 'entity': 'Task'}, {'span': 'voice recognition', 'entity': 'Task'}, {'span': 'sign language', 'entity': 'Task'}, {'span': 'hand gestures interpretation', 'entity': 'Task'}, {'span': 'medical imaging analysis', 'entity': 'Field'}]'

Example #7: Several of these programs are available online, such as Google Translate and the SYSTRAN system that powers AltaVista's BabelFish (now Yahoo's Babelfish as of 9 May 2008).
Expected output: 'entities: [{'span': 'Google Translate', 'entity': 'Product'}, {'span': 'SYSTRAN system', 'entity': 'Product'}, {'span': 'AltaVista', 'entity': 'Organisation'}, {'span': 'BabelFish', 'entity': 'Product'}, {'span': 'Yahoo', 'entity': 'Organisation'}, {'span': 'Babelfish', 'entity': 'Product'}]'

Example #8: He holds a D.Sc. degree in electrical and computer engineering (2000) from Inria and the University of Nice Sophia Antipolis, and has held permanent positions at Siemens Corporate Technology, École des ponts ParisTech as well as visiting positions at Rutgers University, Yale University and University of Houston.
Expected output: 'entities: [{'span': 'D.Sc. degree', 'entity': 'Misc'}, {'span': 'electrical and computer engineering', 'entity': 'Field'}, {'span': 'Inria', 'entity': 'Organisation'}, {'span': 'University of Nice Sophia Antipolis', 'entity': 'University'}, {'span': 'Siemens Corporate Technology', 'entity': 'Organisation'}, {'span': 'École des ponts ParisTech', 'entity': 'University'}, {'span': 'Rutgers University', 'entity': 'University'}, {'span': 'Yale University', 'entity': 'University'}, {'span': 'University of Houston', 'entity': 'University'}]'

Example #9: The input is called speech recognition and the output is called speech synthesis.
Expected output: 'entities: [{'span': 'speech recognition', 'entity': 'Task'}, {'span': 'speech synthesis', 'entity': 'Task'}]'

Example #10: Beginning at the 2019 Toronto International Film Festival, films may now be restricted from screening at Scotiabank Theatre Toronto - one of the festival's main venues - and screened elsewhere (such as TIFF Bell Lightbox and other local cinemas) if distributed by a service such as Netflix.
Expected output: 'entities: [{'span': '2019 Toronto International Film Festival', 'entity': 'Misc'}, {'span': 'Scotiabank Theatre Toronto', 'entity': 'Location'}, {'span': 'TIFF Bell Lightbox', 'entity': 'Location'}, {'span': 'Netflix', 'entity': 'Organisation'}]'

