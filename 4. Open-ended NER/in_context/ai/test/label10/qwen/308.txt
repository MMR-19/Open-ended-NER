Example #1: Linear predictive coding (LPC), a form of speech coding, began development with the work Fumitada Itakura of Nagoya University and Shuzo Saito of Nippon Telegraph and Telephone (NTT) in 1966.
Expected output: 'entities: [{'span': 'Linear predictive coding', 'entity': 'Algorithm'}, {'span': 'LPC', 'entity': 'Algorithm'}, {'span': 'speech coding', 'entity': 'Task'}, {'span': 'Fumitada Itakura', 'entity': 'Researcher'}, {'span': 'Nagoya University', 'entity': 'University'}, {'span': 'Shuzo Saito', 'entity': 'Researcher'}, {'span': 'Nippon Telegraph and Telephone', 'entity': 'University'}, {'span': 'NTT', 'entity': 'University'}]'

Example #2: The term computational linguistics itself was first coined by David Hays, a founding member of both the Association for Computational Linguistics and the International Committee on Computational Linguistics (ICCL).
Expected output: 'entities: [{'span': 'computational linguistics', 'entity': 'Field'}, {'span': 'David Hays', 'entity': 'Researcher'}, {'span': 'Association for Computational Linguistics', 'entity': 'Conference'}, {'span': 'International Committee on Computational Linguistics', 'entity': 'Organisation'}, {'span': 'ICCL', 'entity': 'Organisation'}]'

Example #3: Haralick is a Fellow of IEEE for his contributions in computer vision and image processing and a Fellow of the International Association for Pattern Recognition (IAPR) for his contributions in pattern recognition, image processing, and for service to IAPR.
Expected output: 'entities: [{'span': 'Haralick', 'entity': 'Researcher'}, {'span': 'IEEE', 'entity': 'Organisation'}, {'span': 'computer vision', 'entity': 'Field'}, {'span': 'image processing', 'entity': 'Field'}, {'span': 'International Association for Pattern Recognition', 'entity': 'Organisation'}, {'span': 'IAPR', 'entity': 'Organisation'}, {'span': 'pattern recognition', 'entity': 'Field'}]'

Example #4: tity contains a collection of visualization tools and algorithms for data analysis and predictive modeling, together with graphical user interfaces for easy access to these functions. but the more recent fully Java -based version (Weka 3), for which development started in 1997, is now used in many different application areas, in particular for educational purposes and research.
Expected output: 'entities: [{'span': 'tity', 'entity': 'Product'}, {'span': 'data analysis', 'entity': 'Field'}, {'span': 'predictive modeling', 'entity': 'Task'}, {'span': 'graphical user interfaces', 'entity': 'Misc'}, {'span': 'Java', 'entity': 'Programming Language'}, {'span': 'Weka 3', 'entity': 'Product'}]'

Example #5: The first attempt at end-to-end ASR was with Connectionist Temporal Classification (CTC) -based systems introduced by Alex Graves of Google DeepMind and Navdeep Jaitly of the University of Toronto in 2014.
Expected output: 'entities: [{'span': 'end-to-end ASR', 'entity': 'Task'}, {'span': 'Connectionist Temporal Classification', 'entity': 'Algorithm'}, {'span': 'CTC', 'entity': 'Algorithm'}, {'span': 'Alex Graves', 'entity': 'Researcher'}, {'span': 'Google DeepMind', 'entity': 'Organisation'}, {'span': 'Navdeep Jaitly', 'entity': 'Researcher'}, {'span': 'University of Toronto', 'entity': 'University'}]'

Example #6: He holds a D.Sc. degree in electrical and computer engineering (2000) from Inria and the University of Nice Sophia Antipolis, and has held permanent positions at Siemens Corporate Technology, École des ponts ParisTech as well as visiting positions at Rutgers University, Yale University and University of Houston.
Expected output: 'entities: [{'span': 'D.Sc. degree', 'entity': 'Misc'}, {'span': 'electrical and computer engineering', 'entity': 'Field'}, {'span': 'Inria', 'entity': 'Organisation'}, {'span': 'University of Nice Sophia Antipolis', 'entity': 'University'}, {'span': 'Siemens Corporate Technology', 'entity': 'Organisation'}, {'span': 'École des ponts ParisTech', 'entity': 'University'}, {'span': 'Rutgers University', 'entity': 'University'}, {'span': 'Yale University', 'entity': 'University'}, {'span': 'University of Houston', 'entity': 'University'}]'

Example #7: Since 2002, perceptron training has become popular in the field of natural language processing for such tasks as part-of-speech tagging and syntactic parsing (Collins, 2002).
Expected output: 'entities: [{'span': 'natural language processing', 'entity': 'Field'}, {'span': 'part-of-speech tagging', 'entity': 'Task'}, {'span': 'syntactic parsing', 'entity': 'Task'}, {'span': 'Collins', 'entity': 'Researcher'}]'

Example #8: The National Science Foundation was an umbrella for the National Aeronautics and Space Administration (NASA), the US Department of Energy, the US Department of Commerce NIST, the US Department of Defense, Defense Advanced Research Projects Agency (DARPA), and the Office of Naval Research coordinated studies to inform strategic planners in their deliberations.
Expected output: 'entities: [{'span': 'National Science Foundation', 'entity': 'Organisation'}, {'span': 'National Aeronautics and Space Administration', 'entity': 'Organisation'}, {'span': 'NASA', 'entity': 'Organisation'}, {'span': 'US Department of Energy', 'entity': 'Organisation'}, {'span': 'US Department of Commerce NIST', 'entity': 'Organisation'}, {'span': 'US Department of Defense', 'entity': 'Organisation'}, {'span': 'Defense Advanced Research Projects Agency', 'entity': 'Organisation'}, {'span': 'DARPA', 'entity': 'Organisation'}, {'span': 'Office of Naval Research', 'entity': 'Organisation'}]'

Example #9: Logo was created in 1967 at Bolt, Beranek and Newman (BBN), a Cambridge, Massachusetts research firm, by Wally Feurzeig, Cynthia Solomon, and Seymour Papert.
Expected output: 'entities: [{'span': 'Logo', 'entity': 'Programming Language'}, {'span': 'Bolt, Beranek and Newman', 'entity': 'Organisation'}, {'span': 'BBN', 'entity': 'Organisation'}, {'span': 'Cambridge', 'entity': 'University'}, {'span': 'Massachusetts', 'entity': 'University'}, {'span': 'Wally Feurzeig', 'entity': 'Researcher'}, {'span': 'Cynthia Solomon', 'entity': 'Researcher'}, {'span': 'Seymour Papert', 'entity': 'Researcher'}]'

Example #10: As with BLEU, the basic unit of evaluation is the sentence, the algorithm first creates an alignment (see illustrations) between two sentence s, the candidate translation string, and the reference translation string.
Expected output: 'entities: [{'span': 'BLEU', 'entity': 'Metrics'}]'

