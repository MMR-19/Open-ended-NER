{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "9f41c990",
            "metadata": {},
            "source": [
                "Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b513c9fe",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Libraries\n",
                "import numpy as np\n",
                "import sys\n",
                "import os\n",
                "import json\n",
                "from pathlib import Path\n",
                "import re\n",
                "import litellm \n",
                "from pydantic import BaseModel\n",
                "import pandas as pd\n",
                "\n",
                "# add path to the dataset entities\n",
                "sys.path.append(os.path.abspath(\"../0. Helpers\"))\n",
                "sys.path.append(os.path.abspath(\"../2. Data Processing/_dataset_entities\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a38a8d5d",
            "metadata": {},
            "source": [
                "Topic"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6ec88d30",
            "metadata": {},
            "outputs": [],
            "source": [
                "potential_tokens_folder_output = \"adapted\" # output\n",
                "potential_tokens_folder_input = \"adapted\" # input\n",
                "\n",
                "class Config:\n",
                "    def __init__(self, lang, n):\n",
                "        self.lang = lang\n",
                "        self.n = n\n",
                "\n",
                "all_configs = {\n",
                "    \"ai\": Config(\"en\", 10),\n",
                "    \"literature\": Config(\"en\", 10),\n",
                "    \"music\": Config(\"en\", 10),\n",
                "    \"politics\": Config(\"en\", 20),\n",
                "    \"science\": Config(\"en\", 20),\n",
                "    \"multinerd_en\": Config(\"en\", 20),\n",
                "    \"multinerd_pt\": Config(\"pt\", 20),\n",
                "    \"ener\": Config(\"en\", 20),\n",
                "    \"lener\": Config(\"pt\", 20),\n",
                "    \"neuralshift\": Config(\"pt\", 20)\n",
                "}"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "628bf226",
            "metadata": {},
            "source": [
                "Prompt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "186a96d2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare LLM environment\n",
                "os.environ[\"AZURE_API_KEY\"] = \"...\"\n",
                "os.environ[\"AZURE_API_BASE\"] = \"...\"\n",
                "\n",
                "class LLM_Entity(BaseModel):\n",
                "  span: str\n",
                "  entity: str\n",
                "\n",
                "class LLM_Entity_List(BaseModel):\n",
                "  reflection: str\n",
                "  entities: list[LLM_Entity]\n",
                "\n",
                "# class LLM_Output(BaseModel):\n",
                "#   candidate_token: str\n",
                "#   rationale: str\n",
                "#   updates: str\n",
                "#   entities: list[LLM_Entity]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7ff13a1e",
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_prompt_prefix(topic):\n",
                "\n",
                "    entity_info = \"\"\n",
                "\n",
                "    # Entity info = point + span\n",
                "    point_dict = json.load(open(f\"entity_info/point_entities/span/{topic}/train/_point_span_4.json\", \"r\", encoding=\"utf-8\"))\n",
                "    for entity, clusters in point_dict.items():\n",
                "        entity_info += f\"- \\\"{entity}\\\" e.g. {', '.join(clusters)}\\n\"\n",
                "\n",
                "    # Split by topic\n",
                "\n",
                "    # AI\n",
                "    if topic == \"ai\":\n",
                "        prompt_prefix = f\"\"\"Use the following set of possible entity labels:\n",
                "{entity_info}\n",
                "\n",
                "Dates, times, abstract concepts, adjectives and verbs are NOT entities.\n",
                "Be sure to prioritize more specific entities, such as \"researcher\" over \"person\", \"conference\" over \"location\" and \"university\" over \"organisation\", when it makes sense.\n",
                "\"\"\"\n",
                "    # LITERATURE\n",
                "    elif topic == \"literature\":\n",
                "        prompt_prefix = f\"\"\"Use the following set of possible entity labels:\n",
                "{entity_info}\n",
                "\n",
                "Dates, times, abstract concepts, adjectives and verbs are NOT entities.\n",
                "Be sure to prioritize more specific entities, such as \"writer\" over \"person\", when it makes sense.\n",
                "\"\"\"\n",
                "    \n",
                "    # MUSIC\n",
                "    elif topic == \"music\":\n",
                "        prompt_prefix = f\"\"\"Use the following set of possible entity labels:\n",
                "{entity_info}\n",
                "\n",
                "Dates, times, abstract concepts, adjectives and verbs are NOT entities.\n",
                "Be sure to prioritize more specific entities, such as \"musical artist\" over \"person\" and \"band\" over \"organisation\", when it makes sense.\n",
                "\"\"\"\n",
                "\n",
                "    # POLITICS\n",
                "    elif topic == \"politics\":\n",
                "        prompt_prefix = f\"\"\"Use the following set of possible entity labels:\n",
                "{entity_info}\n",
                "\n",
                "Dates, times, abstract concepts, adjectives and verbs are NOT entities.\n",
                "Be sure to prioritize more specific entities, such as \"politician\" over \"person\" and \"political party\" over \"organisation\", when it makes sense.\n",
                "\"\"\"\n",
                "        \n",
                "    # SCIENCE\n",
                "    elif topic == \"science\":\n",
                "        prompt_prefix = f\"\"\"Use the following set of possible entity labels:\n",
                "{entity_info}\n",
                "\n",
                "Dates, times, abstract concepts, adjectives and verbs are NOT entities.\n",
                "Abstract scientific concepts can be entities if they have a name associated with them.\n",
                "Be sure to prioritize more specific entities, such as \"scientist\" over \"person\" and \"university\" over \"organisation\" or \"location\", when it makes sense.\n",
                "\"\"\"\n",
                "\n",
                "    # MULTINERD PT\n",
                "    elif topic == \"multinerd_pt\":\n",
                "        prompt_prefix = f\"\"\"Usa o seguinte conjunto de tipos possíveis de entidade:\n",
                "{entity_info}\n",
                "\n",
                "Datas, horas, conceitos abstratos, adjetivos e verbos NÃO são entidades.\n",
                "Se uma entidade não se encaixar em nenhum dos tipos acima, não a incluas na resposta.\n",
                "\"\"\"\n",
                "        \n",
                "    # MULTINERD EN\n",
                "    elif topic == \"multinerd_en\":\n",
                "        prompt_prefix = f\"\"\"Use the following set of possible entity labels:\n",
                "{entity_info}\n",
                "Dates, times, abstract concepts, adjectives and verbs are NOT entities.\n",
                "\"\"\"\n",
                "        \n",
                "    # E-NER\n",
                "    elif topic == \"ener\":\n",
                "        prompt_prefix = f\"\"\"Use the following set of possible entity labels:\n",
                "{entity_info}\n",
                "Dates, times, abstract concepts, adjectives and verbs are NOT entities.\n",
                "\"\"\"\n",
                "        \n",
                "    # LeNER-Br + NEURALSHIFT\n",
                "    elif (topic == \"lener\" or topic == \"neuralshift\"):\n",
                "        prompt_prefix = f\"\"\"Usa o seguinte conjunto de tipos possíveis de entidade:\n",
                "{entity_info}\n",
                "\n",
                "Conceitos abstratos, adjetivos e verbos NÃO são entidades.\n",
                "Se uma entidade não se encaixar em nenhum dos tipos acima, não a incluas na resposta.\n",
                "\"\"\"\n",
                "        \n",
                "    # validation\n",
                "    if entity_info == \"\" or prompt_prefix == \"\":\n",
                "        raise ValueError(f\"Error retrieving entity info for topic {topic}.\")\n",
                "\n",
                "    return prompt_prefix\n",
                "\n",
                "\n",
                "################## FINAL PROMPT ##################\n",
                "\n",
                "def final_prompt(topic, lang, sentence, token, positive_examples):\n",
                "\n",
                "    positive_examples_string = \"\"\n",
                "    for example in positive_examples:\n",
                "        positive_examples_string += f\"Input: {example['sentence']}\\nOutput entity: '{example['entity']}'\\n\\n\"\n",
                "\n",
                "    if lang == \"en\":\n",
                "        return f\"\"\"\n",
                "<input_text>\n",
                "{sentence}\n",
                "</input_text>\n",
                "\n",
                "<candidate_token>\n",
                "{token}\n",
                "</candidate_token>\n",
                "\n",
                "1. Evaluate the candidate token \"{token}\" to determine if it should be categorized as an entity or not.\n",
                "\n",
                "2. {get_prompt_prefix(topic)}\n",
                "\n",
                "3. The candidate token is a token that was not recognized as part of an entity by the NER system. However, based on training data, it has a high likelihood of being part of an entity.\n",
                "Note that the training data might not be representative, therefore use your best judgment to determine whether the token (and surroundings) should be considered as part of a new entity.\n",
                "\n",
                "4. Consider the following examples provided carefully:\n",
                "<examples>\n",
                "{positive_examples_string}\n",
                "</examples>\n",
                "\n",
                "Output Format:\n",
                "- reflection: short reflection on whether the token should be considered part of an entity or not\n",
                "- entities: list of entities, or empty list if none \n",
                "\n",
                "Return a JSON, focusing on the token \"{token}\" and its surrounding context within the input text:\n",
                "\"{sentence}\"\n",
                "\"\"\"\n",
                "    elif lang == \"pt\":\n",
                "        return f\"\"\"\n",
                "<input_text>\n",
                "{sentence}\n",
                "</input_text>\n",
                "\n",
                "<candidate_token>\n",
                "{token}\n",
                "</candidate_token>\n",
                "\n",
                "1. Analisa o token candidato \"{token}\" para determinar se deve ser considerado parte de uma nova entidade ou não.\n",
                "\n",
                "2. {get_prompt_prefix(topic)}\n",
                "\n",
                "3. O token candidato é um token que não foi reconhecido como parte de uma entidade pelo sistema NER. No entanto, com base nos dados de treino, apresenta uma elevada probabilidade de fazer parte de uma entidade. Nota que os dados de treino podem não ser representativos, portanto utiliza o teu melhor julgamento para determinar se o token (e o contexto envolvente) deve ser considerado parte de uma nova entidade.\n",
                "\n",
                "4. Analisa cuidadosamente os seguintes exemplos:\n",
                "<examples>\n",
                "{positive_examples_string}\n",
                "</examples>\n",
                "\n",
                "Formato do Output:\n",
                "- reflection: pequena reflexão sobre se o token deve ser considerado parte de uma nova entidade ou não\n",
                "- entities: lista de entidades, ou lista vazia se nenhuma \n",
                "\n",
                "Return a JSON, focusing on the token \"{token}\" and its surrounding context within the input text:\n",
                "\"{sentence}\"\n",
                "\"\"\"\n",
                "    \n",
                "    else:\n",
                "        raise ValueError(f\"Language {lang} not supported.\")\n",
                "    \n",
                "# - reflection: short reflection on whether the token should be considered an entity or not\n",
                "# - reflection: breve reflexão sobre se o token deve ser considerado uma entidade ou não\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0c078cf4",
            "metadata": {},
            "source": [
                "LLM Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b5bc09fb",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Call LLM\n",
                "def safe_llm_call(prompt, system, instance):\n",
                "    try:\n",
                "\n",
                "        response = litellm.completion(\n",
                "            model = \"azure/gpt-4o-mini\",\n",
                "            messages = [\n",
                "                {\"role\": \"system\", \"content\": system},\n",
                "                {\"role\": \"user\", \"content\": prompt},\n",
                "            ],\n",
                "\n",
                "            temperature = 0.1,\n",
                "            response_format = LLM_Entity_List,\n",
                "\n",
                "            # stream = False,\n",
                "            # top_p = 1,\n",
                "        )\n",
                "\n",
                "        # extract LLM predictions\n",
                "        return response.choices[0].message[\"content\"]\n",
                "\n",
                "    except Exception as e:\n",
                "        print(f\"\\n❌❌ LLM call failed: {e}\\n\")\n",
                "        print(f\"\\nExample: {instance}\")\n",
                "        raise"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3f9de219",
            "metadata": {},
            "outputs": [],
            "source": [
                "def system_prompt(lang):\n",
                "    if lang == \"en\":\n",
                "        return \"You are a named entity recognition (NER) system. Your task is to reflect about either the <candidate_token> is part of an entity mentioned in the <input_text> or not. Always respond with JSON with the outcome of your reflection - new entity or empty.\"\n",
                "    elif lang == \"pt\":\n",
                "        return \"És um sistema de reconhecimento de entidades (NER). A tua tarefa é refletir sobre se o <candidate_token> faz parte de uma entidade mencionada no <input_text> ou não. Responde sempre no formato JSON com o resultado da tua reflexão - nova entidade ou nada.\""
            ]
        },
        {
            "cell_type": "markdown",
            "id": "42258627",
            "metadata": {},
            "source": [
                "Call LLM for each token (reflection)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c8941205",
            "metadata": {},
            "outputs": [],
            "source": [
                "def process_token(topic, lang, file_name, token_info, sentence):\n",
                "\n",
                "    token = token_info['token']\n",
                "    positive_examples = token_info['positive_examples']\n",
                "\n",
                "    prompt = final_prompt(topic, lang, sentence, token, positive_examples)\n",
                "\n",
                "    # Save prompt to txt file\n",
                "    prompt_file_path = f\"results/error_reflection/{topic}/false_negatives/{potential_tokens_folder_output}/prompts/prompt_{file_name}.txt\"\n",
                "    with open(prompt_file_path, \"w\", encoding=\"utf-8\") as f:\n",
                "        f.write(prompt)\n",
                "\n",
                "    # call the LLM\n",
                "    try:\n",
                "        llm_response = safe_llm_call(prompt, system_prompt(lang), sentence)\n",
                "\n",
                "        if llm_response is None:\n",
                "            print(f\"❌ LLM response is None for sentence: {sentence}\")\n",
                "            return\n",
                "        \n",
                "        # Save response to txt file\n",
                "        response_file_path = f\"results/error_reflection/{topic}/false_negatives/{potential_tokens_folder_output}/responses/response_{file_name}.txt\"\n",
                "        with open(response_file_path, \"a\", encoding=\"utf-8\") as f:\n",
                "            f.write(json.dumps(llm_response, ensure_ascii=False, indent=4))\n",
                "\n",
                "        # parse json and return new entities\n",
                "        llm_json = json.loads(llm_response)\n",
                "        llm_entities = llm_json.get('entities', [])\n",
                "\n",
                "        return llm_entities\n",
                "\n",
                "    except Exception as e:\n",
                "        print(f\"❌ Error on sentence: {e}\")\n",
                "        return"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0f7bd750",
            "metadata": {},
            "source": [
                "Evaluate all tokens in folder"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "42384f93",
            "metadata": {},
            "outputs": [],
            "source": [
                "for topic, config in all_configs.items():\n",
                "    \n",
                "    n = config.n\n",
                "    lang = config.lang\n",
                "    print(f\"\\n\\nProcessing topic: {topic} (n={n}, lang={lang})\")\n",
                "    \n",
                "    input_path = Path(f\"error_reflection/false_negatives/{topic}/{potential_tokens_folder_input}\")\n",
                "\n",
                "    # ensure folder exists\n",
                "    os.makedirs(f\"results/error_reflection/{topic}/false_negatives/{potential_tokens_folder_output}\", exist_ok=True)\n",
                "    os.makedirs(f\"results/error_reflection/{topic}/false_negatives/{potential_tokens_folder_output}/prompts\", exist_ok=True)\n",
                "    os.makedirs(f\"results/error_reflection/{topic}/false_negatives/{potential_tokens_folder_output}/responses\", exist_ok=True)\n",
                "\n",
                "    all_data = list(input_path.glob(\"*.json\"))\n",
                "    for i, file_path in enumerate(all_data):\n",
                "        \n",
                "        print(f\"\\r\\tProcessing instance {i+1}/{len(all_data)}\", end='', flush=True)\n",
                "\n",
                "        # load file\n",
                "        with open(file_path, mode='r', encoding=\"utf-8\") as f:\n",
                "            data = json.loads(f.read())\n",
                "\n",
                "        sentence = data[\"sentence\"]\n",
                "        false_negative_tokens = data[\"false_negative_tokens\"]\n",
                "\n",
                "        new_entities = []\n",
                "        for token_info in false_negative_tokens:\n",
                "            token = token_info[\"token\"]\n",
                "            reflected_token_entities = process_token(topic, lang, file_path.stem, token_info, sentence)\n",
                "            \n",
                "            if reflected_token_entities:\n",
                "                new_entities.extend(reflected_token_entities)\n",
                "\n",
                "        # save updated file with new entities\n",
                "        for entity in new_entities:\n",
                "            entity[\"candidate_token\"] = token\n",
                "\n",
                "        # save results to file\n",
                "        result_json = {\n",
                "            \"sentence\": sentence,\n",
                "            \"false_negative_tokens\": false_negative_tokens,\n",
                "            \"reflected_entities\": new_entities\n",
                "        }\n",
                "\n",
                "        result_file_path = f\"results/error_reflection/{topic}/false_negatives/{potential_tokens_folder_output}/{file_path.stem}.json\"\n",
                "        with open(result_file_path, \"w\", encoding=\"utf-8\") as f:\n",
                "            f.write(json.dumps(result_json, ensure_ascii=False, indent=4))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
