{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "7f6e5c22",
            "metadata": {},
            "source": [
                "Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f05205be",
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import json\n",
                "from pathlib import Path\n",
                "from collections import Counter\n",
                "\n",
                "from sentence_transformers import SentenceTransformer\n",
                "\n",
                "# add path \n",
                "sys.path.append(os.path.abspath(\"../0. Helpers\"))\n",
                "sys.path.append(os.path.abspath(\"../2. Data Processing/_dataset_entities\"))\n",
                "\n",
                "from datasets import load_dataset, load_from_disk\n",
                "from datasetProcessing import tokens_to_sentence, tokens_to_entities, join_datasets, recursive_fix"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b766d5b4",
            "metadata": {},
            "source": [
                "Define the model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "acbae4bc",
            "metadata": {},
            "outputs": [],
            "source": [
                "model = SentenceTransformer(\n",
                "    \"Qwen/Qwen3-Embedding-4B\",\n",
                "    # model_kwargs = {\n",
                "    #     # \"attn_implementation\": \"flash_attention_2\",\n",
                "    #     \"device_map\": \"auto\"},\n",
                "    tokenizer_kwargs = {\n",
                "        \"padding_side\": \"left\"},\n",
                ")\n",
                "\n",
                "# Move model to CPU\n",
                "model.to(\"cpu\")\n",
                "\n",
                "# Check model device\n",
                "print(model.device)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b5de3b9e",
            "metadata": {},
            "source": [
                "Test one similiarity"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "282c82bc",
            "metadata": {},
            "outputs": [],
            "source": [
                "# The queries and documents to embed\n",
                "queries = [\n",
                "    \"What is the capital of China?\",\n",
                "    \"Explain gravity\",\n",
                "]\n",
                "documents = [\n",
                "    \"The capital of China is Beijing.\",\n",
                "    \"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\",\n",
                "]\n",
                "\n",
                "# Encode the queries and documents. Note that queries benefit from using a prompt\n",
                "# Here we use the prompt called \"query\" stored under `model.prompts`, but you can\n",
                "# also pass your own prompt via the `prompt` argument\n",
                "query_embeddings = model.encode(queries, prompt_name=\"query\")\n",
                "document_embeddings = model.encode(documents)\n",
                "\n",
                "# Compute the (cosine) similarity between the query and document embeddings\n",
                "similarity = model.similarity(query_embeddings, document_embeddings)\n",
                "print(model.similarity_fn_name)\n",
                "print(similarity)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a4e8f7e2",
            "metadata": {},
            "source": [
                "Process whole dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e1e4d692",
            "metadata": {},
            "outputs": [],
            "source": [
                "topic = \"music\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e49ca6be",
            "metadata": {},
            "outputs": [],
            "source": [
                "if topic == \"lener\":\n",
                "    from entities_leNER import entity_names, entity_names_parsed\n",
                "    dataset = load_from_disk(\"...\")\n",
                "    lang = \"portuguese\"\n",
                "\n",
                "elif topic == \"neuralshift\":\n",
                "    from entities_neuralshift import entity_names, entity_names_parsed\n",
                "    dataset = load_from_disk(\"...\")\n",
                "    lang = \"portuguese\"\n",
                "\n",
                "elif topic == \"ener\":\n",
                "    from entities_eNER import entity_names, entity_names_parsed\n",
                "    dataset = load_from_disk(\"...\")\n",
                "    lang = \"english\"\n",
                "\n",
                "elif topic == \"multinerd_en\":\n",
                "    from entities_multinerd_en import entity_names, entity_names_parsed\n",
                "    dataset = load_from_disk(\"...\")\n",
                "    lang = \"english\"\n",
                "\n",
                "elif topic == \"multinerd_pt\":\n",
                "    from entities_multinerd_pt import entity_names, entity_names_parsed\n",
                "    dataset = load_from_disk(\"...\")\n",
                "    lang = \"portuguese\"\n",
                "\n",
                "else:\n",
                "    from entities_crossNER import entity_names, entity_names_parsed\n",
                "    dataset = load_dataset(\"...\")\n",
                "    lang = \"english\"\n",
                "\n",
                "# train_data\n",
                "train_data = dataset[\"train\"]\n",
                "test_data = dataset[\"test\"]\n",
                "\n",
                "# get the entity names\n",
                "start_of_entity_indices = [i for i in range(len(entity_names)) if (entity_names[i].startswith(\"B-\") or entity_names[i].startswith(\"U-\"))]\n",
                "entity_index_to_name = {i: entity_names[i].split(\"-\")[1] for i in range(len(entity_names)) if entity_names[i] != \"O\"}\n",
                "entity_index_to_name[0] = \"O\""
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c8180716",
            "metadata": {},
            "source": [
                "Get and save embeddings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "35319658",
            "metadata": {},
            "outputs": [],
            "source": [
                "data_splits = ['train', 'test']\n",
                "\n",
                "for split in data_splits:\n",
                "\n",
                "    split_path = f\"embeddings/{topic}/{split}\"\n",
                "    split_size = len(dataset[split])\n",
                "    \n",
                "    # Create folder\n",
                "    os.makedirs(split_path, exist_ok=True)\n",
                "\n",
                "    for i, instance in enumerate(dataset[split]):\n",
                "\n",
                "        id = None\n",
                "        sentence = None\n",
                "        embedding_qwen = None\n",
                "\n",
                "        file_path = f\"embeddings/{topic}/{split}/{i}.json\"\n",
                "\n",
                "        # read file if exists and complete it\n",
                "        if os.path.exists(file_path):\n",
                "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
                "\n",
                "                # handle problem with files\n",
                "                s = f.read()\n",
                "                boundary = s.rfind(\"}{\")\n",
                "\n",
                "                existing_data = json.loads(s[boundary + 1:]) if boundary != -1 else json.loads(s)\n",
                "                \n",
                "                id = existing_data.get(\"id\", None)\n",
                "                sentence = existing_data.get(\"sentence\", None)\n",
                "                embedding_qwen = existing_data.get(\"embedding_qwen\", None)\n",
                "        \n",
                "        if not id:\n",
                "            id = i\n",
                "\n",
                "        if not sentence:\n",
                "            sentence = tokens_to_sentence(instance['tokens'])\n",
                "        \n",
                "        if not embedding_qwen:\n",
                "            embedding_qwen = model.encode(sentence).tolist()\n",
                "\n",
                "        # Save to json file\n",
                "        result_json = {\n",
                "            \"id\": i,\n",
                "            \"sentence\": sentence,\n",
                "            \"embedding_qwen\": embedding_qwen,\n",
                "        }  \n",
                "\n",
                "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
                "            f.write(json.dumps(result_json, ensure_ascii=False, indent=4))\n",
                "\n",
                "        print(f\"âœ… {split} instance #{i+1}/{split_size} saved to {file_path}\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
