{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "409a19bc",
            "metadata": {},
            "source": [
                "Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c2f91dd6",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Libraries\n",
                "import numpy as np\n",
                "import sys\n",
                "import os\n",
                "import json\n",
                "from pathlib import Path\n",
                "import re\n",
                "\n",
                "# add path to the dataset entities\n",
                "sys.path.append(os.path.abspath(\"../0. Helpers\"))\n",
                "sys.path.append(os.path.abspath(\"../2. Data Processing/_dataset_entities\"))\n",
                "\n",
                "from datasetProcessing import Entity, recursive_fix\n",
                "from performance import Prediction, Performance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c09b66e4",
            "metadata": {},
            "outputs": [],
            "source": [
                "def process_instance(file_path, ignore_empty=False):\n",
                "    \n",
                "    try:\n",
                "        with open(file_path, mode='r', encoding=\"utf-8\") as f:\n",
                "            content = f.read()\n",
                "\n",
                "        if not content.strip():\n",
                "            print(f\"üóëÔ∏è Empty file detected, deleting: {file_path}\")\n",
                "            # file_path.unlink()\n",
                "            return None\n",
                "\n",
                "        # Fix JSON extra comma\n",
                "        content = re.sub(r',\\s*$', '', content)\n",
                "        data = json.loads(content)\n",
                "\n",
                "        # Apply encoding fix\n",
                "        data = recursive_fix(data)\n",
                "\n",
                "        # extract entities  \n",
                "        true_entities = data.get(\"true_entities\", [])\n",
                "        llm_entities = data.get(\"entities\", [])\n",
                "\n",
                "        # remove duplicates from llm entities\n",
                "        predicted_entities = []\n",
                "        for entity in llm_entities:\n",
                "\n",
                "            for added_entity in predicted_entities:\n",
                "                if entity[\"span\"].strip().lower() == added_entity[\"span\"].strip().lower() and entity[\"entity\"].strip().lower() == added_entity[\"entity\"].strip().lower():\n",
                "                    break\n",
                "            else:\n",
                "                predicted_entities.append(entity)\n",
                "\n",
                "        if not true_entities and ignore_empty:\n",
                "            print(f\"üóëÔ∏è No entities found in {file_path}, ignoring performance\")\n",
                "            return None\n",
                "        \n",
                "        # process true entities from dict to Entity objects\n",
                "        true_entities = [Entity.from_dict(entity) for entity in true_entities]\n",
                "\n",
                "        # create prediction object\n",
                "        prediction = Prediction(0, \"\")\n",
                "\n",
                "        # compute predictions\n",
                "        prediction.set_results(true_entities, predicted_entities)\n",
                "        prediction.compute_performance()\n",
                "        prediction.compute_relaxed_performance()\n",
                "\n",
                "        # write back to json\n",
                "        data[\"performance\"] = {\n",
                "            \"tp\": prediction.performance.tp,\n",
                "            \"fp\": prediction.performance.fp,\n",
                "            \"fn\": prediction.performance.fn,\n",
                "            \"precision\": prediction.performance.precision(),\n",
                "            \"recall\": prediction.performance.recall(),\n",
                "            \"f1\": prediction.performance.f1()\n",
                "        }\n",
                "        \n",
                "        data[\"relaxed_performance\"] = {\n",
                "            \"tp\": prediction.relaxed_performance.tp,\n",
                "            \"fp\": prediction.relaxed_performance.fp,\n",
                "            \"fn\": prediction.relaxed_performance.fn,\n",
                "            \"precision\": prediction.relaxed_performance.precision(),\n",
                "            \"recall\": prediction.relaxed_performance.recall(),\n",
                "            \"f1\": prediction.relaxed_performance.f1()\n",
                "        }\n",
                "\n",
                "        with open(file_path, mode='w', encoding=\"utf-8\") as f:\n",
                "            f.write(json.dumps(data, ensure_ascii=False, indent=4))\n",
                "\n",
                "        return prediction\n",
                "\n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Error reading {file_path}: {e}\")\n",
                "        # file_path.unlink()\n",
                "        return None"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "eae6a56e",
            "metadata": {},
            "source": [
                "Evaluate each model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6f97c797",
            "metadata": {},
            "outputs": [],
            "source": [
                "folder_prefix = \"results/demo_type\"\n",
                "folder_suffix = \"in_context_top\"\n",
                "\n",
                "all_configs = {\n",
                "    \"ai\": 10,\n",
                "    \"literature\": 10,\n",
                "    \"music\": 10,\n",
                "    \"politics\": 20,\n",
                "    \"science\": 20,\n",
                "    \"multinerd_en\": 20,\n",
                "    \"multinerd_pt\": 20,\n",
                "    \"ener\": 20,\n",
                "    \"lener\": 20,\n",
                "    \"neuralshift\": 20\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a4b3739a",
            "metadata": {},
            "outputs": [],
            "source": [
                "topic_performances = {}\n",
                "\n",
                "for topic, n in all_configs.items():\n",
                "    \n",
                "    print(f\"Processing topic: {topic}\")\n",
                "    topic_path = Path(f\"{folder_prefix}/{topic}/{folder_suffix}{n}\")\n",
                "    print(topic_path)\n",
                "\n",
                "    if not topic_path.exists():\n",
                "        print(f\"‚ùå Topic folder {topic} does not exist.\")\n",
                "        continue\n",
                "\n",
                "    # Process all instances in the folder\n",
                "    dataset_performance = []\n",
                "    for file_path in topic_path.glob(\"*.json\"):\n",
                "        result = process_instance(file_path)\n",
                "        dataset_performance.append(result)\n",
                "\n",
                "    # Filter out None results\n",
                "    dataset_performance = [instance for instance in dataset_performance if instance is not None]\n",
                "    if not dataset_performance:\n",
                "        print(f\"‚ö†Ô∏è No valid instances found in {topic_path.name}. Skipping...\")\n",
                "        continue\n",
                "    else:\n",
                "        print(f\"‚úÖ Processed {len(dataset_performance)} valid instances in {topic_path.name}.\")\n",
                "\n",
                "    # compute individual performance metrics\n",
                "    metrics_dict = {\n",
                "        \"total_samples\": len(dataset_performance),\n",
                "        \"true_0\": sum(1 for instance in dataset_performance if len(instance.true_entities) == 0),\n",
                "        \"llm_0\": sum(1 for instance in dataset_performance if len(instance.llm_entities) == 0),\n",
                "        \"performance\": {\n",
                "            \"tp\": [instance.performance.tp for instance in dataset_performance],\n",
                "            \"fp\": [instance.performance.fp for instance in dataset_performance],\n",
                "            \"fn\": [instance.performance.fn for instance in dataset_performance],\n",
                "        },\n",
                "        \"relaxed_performance\": {\n",
                "            \"tp\": [instance.relaxed_performance.tp for instance in dataset_performance],\n",
                "            \"fp\": [instance.relaxed_performance.fp for instance in dataset_performance],\n",
                "            \"fn\": [instance.relaxed_performance.fn for instance in dataset_performance],\n",
                "        }\n",
                "    }\n",
                "\n",
                "    # compute performance metrics\n",
                "    overall_performance = Performance(\n",
                "        tp=sum(metrics_dict[\"performance\"][\"tp\"]),\n",
                "        fp=sum(metrics_dict[\"performance\"][\"fp\"]),\n",
                "        fn=sum(metrics_dict[\"performance\"][\"fn\"])\n",
                "    )\n",
                "\n",
                "    print(\"topic:\", topic)\n",
                "    print(\"metrics:\", metrics_dict)\n",
                "    print(\"\\n\\n\")\n",
                "\n",
                "    topic_performances[topic] = overall_performance"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
