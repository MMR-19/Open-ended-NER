{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "7f6e5c22",
            "metadata": {},
            "source": [
                "Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f05205be",
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import json\n",
                "from pathlib import Path\n",
                "from collections import Counter\n",
                "\n",
                "# add path \n",
                "sys.path.append(os.path.abspath(\"../0. Helpers\"))\n",
                "sys.path.append(os.path.abspath(\"../2. Data Processing/_dataset_entities\"))\n",
                "\n",
                "from datasets import load_dataset, load_from_disk\n",
                "from datasetProcessing import tokens_to_sentence, tokens_to_entities, join_datasets, recursive_fix"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a4e8f7e2",
            "metadata": {},
            "source": [
                "Process whole dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e1e4d692",
            "metadata": {},
            "outputs": [],
            "source": [
                "topic = \"multinerd_pt\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e49ca6be",
            "metadata": {},
            "outputs": [],
            "source": [
                "if topic == \"lener\":\n",
                "    from entities_leNER import entity_names, entity_names_parsed\n",
                "    dataset = load_from_disk(\"...\")\n",
                "    lang = \"portuguese\"\n",
                "\n",
                "elif topic == \"neuralshift\":\n",
                "    from entities_neuralshift import entity_names, entity_names_parsed\n",
                "    dataset = load_from_disk(\"...\")\n",
                "    lang = \"portuguese\"\n",
                "\n",
                "elif topic == \"ener\":\n",
                "    from entities_eNER import entity_names, entity_names_parsed\n",
                "    dataset = load_from_disk(\"...\")\n",
                "    lang = \"english\"\n",
                "\n",
                "elif topic == \"multinerd_en\":\n",
                "    from entities_multinerd_en import entity_names, entity_names_parsed\n",
                "    dataset = load_from_disk(\"...\")\n",
                "    lang = \"english\"\n",
                "\n",
                "elif topic == \"multinerd_pt\":\n",
                "    from entities_multinerd_pt import entity_names, entity_names_parsed\n",
                "    dataset = load_from_disk(\"...\")\n",
                "    lang = \"portuguese\"\n",
                "\n",
                "else:\n",
                "    from entities_crossNER import entity_names, entity_names_parsed\n",
                "    dataset = load_dataset(\"...\")\n",
                "    lang = \"english\"\n",
                "\n",
                "# train_data\n",
                "train_data = dataset[\"train\"]\n",
                "test_data = dataset[\"test\"]\n",
                "\n",
                "# get the entity names\n",
                "start_of_entity_indices = [i for i in range(len(entity_names)) if (entity_names[i].startswith(\"B-\") or entity_names[i].startswith(\"U-\"))]\n",
                "entity_index_to_name = {i: entity_names[i].split(\"-\")[1] for i in range(len(entity_names)) if entity_names[i] != \"O\"}\n",
                "entity_index_to_name[0] = \"O\""
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1ed180c8",
            "metadata": {},
            "source": [
                "Token classification (entity, context, other)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d891e07f",
            "metadata": {},
            "outputs": [],
            "source": [
                "def classify_tokens(tokens, bio_tags, context_window=2, other_tag=0):\n",
                "    \n",
                "    n = len(tokens)\n",
                "    token_types = [''] * n\n",
                "\n",
                "    entity_indices = {i for i, tag in enumerate(bio_tags) if tag != other_tag}\n",
                "\n",
                "    # First, mark all entities\n",
                "    for i in entity_indices:\n",
                "        token_types[i] = 'entity'\n",
                "\n",
                "    # Then, iterate over all tokens\n",
                "    for i in range(n):\n",
                "        if token_types[i] == 'entity':\n",
                "            continue\n",
                "        \n",
                "        # Check if token is inside the window of any entity (context)\n",
                "        for entity_idx in entity_indices:\n",
                "            if (entity_idx - context_window <= i < entity_idx) or (entity_idx < i <= entity_idx + context_window):\n",
                "                token_types[i] = 'context'\n",
                "                break\n",
                "        \n",
                "        # Mark the rest as 'other'\n",
                "        if not token_types[i]:\n",
                "            token_types[i] = 'other'\n",
                "\n",
                "    # Create dictionary with spans\n",
                "    result = {\"entity\": [], \"context\": [], \"other\": []}\n",
                "    for i, token_type in enumerate(token_types):\n",
                "        result[token_type].append(tokens[i])\n",
                "\n",
                "    return result"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "08cd59fa",
            "metadata": {},
            "outputs": [],
            "source": [
                "total_entity_tokens = 0\n",
                "total_context_tokens = 0\n",
                "total_other_tokens = 0\n",
                "\n",
                "# Create directory if it doesn't exist\n",
                "os.makedirs(f\"classification/{topic}/train/data\", exist_ok=True)\n",
                "\n",
                "for i, example in enumerate(train_data):\n",
                "\n",
                "    # check if json file already exists and skip\n",
                "    file_path = f\"classification/{topic}/train/data/{i}.json\"\n",
                "    if os.path.exists(file_path):\n",
                "        print(f\"↪️ Example #{i} already processed\")\n",
                "        continue\n",
                "\n",
                "    tokens = example['tokens']\n",
                "    bio_tags = example['ner_tags']\n",
                "    sentence = tokens_to_sentence(example['tokens'])\n",
                "    true_entities = tokens_to_entities(example['tokens'], example['ner_tags'], entity_names_parsed, start_of_entity_indices, entity_index_to_name)\n",
                "    \n",
                "    # Classify tokens\n",
                "    classified_tokens = classify_tokens(tokens, bio_tags, context_window=2, other_tag=0)\n",
                "    \n",
                "    # Update totals\n",
                "    total_entity_tokens += len(classified_tokens['entity'])\n",
                "    total_context_tokens += len(classified_tokens['context'])   \n",
                "    total_other_tokens += len(classified_tokens['other'])\n",
                "\n",
                "    # Save to json file\n",
                "    result_json = {\n",
                "        \"id\": i,\n",
                "        \"sentence\": sentence,\n",
                "        \"classification\": classified_tokens,\n",
                "        \"tokens\": tokens,\n",
                "        \"ner_tags\": bio_tags,\n",
                "        \"true_entities\": [entity.to_dict() for entity in true_entities],\n",
                "    }  \n",
                "\n",
                "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
                "        f.write(json.dumps(result_json, ensure_ascii=False, indent=4))\n",
                "\n",
                "    print(f\"✅ Example #{i} processed and saved to {file_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "87a6eaee",
            "metadata": {},
            "source": [
                "Save totals"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ddf61f2b",
            "metadata": {},
            "outputs": [],
            "source": [
                "total_json = {\n",
                "    \"total_entity_tokens\": total_entity_tokens,\n",
                "    \"total_context_tokens\": total_context_tokens,\n",
                "    \"total_other_tokens\": total_other_tokens,\n",
                "}\n",
                "\n",
                "print(total_json)\n",
                "\n",
                "# Save to json file\n",
                "file_path = f\"classification/{topic}/train/_total.json\"\n",
                "with open(file_path, \"a\", encoding=\"utf-8\") as f:\n",
                "    f.write(json.dumps(total_json, ensure_ascii=False, indent=4))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c1e5d535",
            "metadata": {},
            "source": [
                "Build probability dictionary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "374d095a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build full dataset lists\n",
                "entity_tokens = []\n",
                "context_tokens = []\n",
                "other_tokens = []\n",
                "\n",
                "# Load all json files and append to lists\n",
                "folder = Path(f\"classification/{topic}/train/data\")\n",
                "for file_path in folder.glob(\"*.json\"):\n",
                "    if file_path.name == \"_total.json\":\n",
                "        continue  # Skip the total file\n",
                "\n",
                "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
                "        data = json.load(f)\n",
                "        entity_tokens.extend(data['classification']['entity'])\n",
                "        context_tokens.extend(data['classification']['context'])\n",
                "        other_tokens.extend(data['classification']['other'])\n",
                "\n",
                "print(f\"Total entity tokens: {len(entity_tokens)}\")\n",
                "print(f\"Total context tokens: {len(context_tokens)}\")\n",
                "print(f\"Total other tokens: {len(other_tokens)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ec977100",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Counters\n",
                "counter_e = Counter(entity_tokens)\n",
                "counter_c = Counter(context_tokens)\n",
                "counter_o = Counter(other_tokens)\n",
                "\n",
                "# Vocabulary = all tokens\n",
                "vocabulary = set(counter_e) | set(counter_c) | set(counter_o)\n",
                "\n",
                "# Build the probability table\n",
                "probs = {}\n",
                "for token in vocabulary:\n",
                "\n",
                "    token_total = counter_e[token] + counter_c[token] + counter_o[token]\n",
                "\n",
                "    p_e = counter_e[token] / token_total\n",
                "    p_c = counter_c[token] / token_total\n",
                "    p_o = counter_o[token] / token_total\n",
                "\n",
                "    probs[token] = {\"prob_e\": p_e, \"prob_c\": p_c, \"prob_o\": p_o}\n",
                "\n",
                "print(probs)\n",
                "\n",
                "# check if json file already exists and skip\n",
                "file_path = f\"classification/{topic}/train/_probs.json\"\n",
                "\n",
                "# Save to json file\n",
                "with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
                "    f.write(json.dumps(probs, ensure_ascii=False, indent=4))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
