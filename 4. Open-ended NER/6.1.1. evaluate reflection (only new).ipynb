{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "f8e93436",
            "metadata": {},
            "source": [
                "Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0cbbef77",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Libraries\n",
                "import numpy as np\n",
                "import sys\n",
                "import os\n",
                "import json\n",
                "from pathlib import Path\n",
                "import re\n",
                "import pandas as pd\n",
                "\n",
                "# add path to the dataset entities\n",
                "sys.path.append(os.path.abspath(\"../0. Helpers\"))\n",
                "sys.path.append(os.path.abspath(\"../2. Data Processing/_dataset_entities\"))\n",
                "\n",
                "from datasetProcessing import Entity, recursive_fix\n",
                "from performance import Prediction, Performance"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "82a63a13",
            "metadata": {},
            "source": [
                "Evaluate each model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "47ea21b1",
            "metadata": {},
            "outputs": [],
            "source": [
                "all_configs = {\n",
                "    \"ai\": 10,\n",
                "    \"literature\": 10,\n",
                "    \"music\": 10,\n",
                "    \"politics\": 20,\n",
                "    \"science\": 20,\n",
                "    \"multinerd_en\": 20,\n",
                "    \"multinerd_pt\": 20,\n",
                "    \"ener\": 20,\n",
                "    \"lener\": 20,\n",
                "    \"neuralshift\": 20\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ecf1adc2",
            "metadata": {},
            "outputs": [],
            "source": [
                "reflection_folders = [\"false_negatives/adapted\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f1643fae",
            "metadata": {},
            "outputs": [],
            "source": [
                "for topic, n in all_configs.items():\n",
                "    for reflection_folder in reflection_folders:\n",
                "\n",
                "        reflections_done_path = Path(f\"results/error_reflection/{topic}/{reflection_folder}\")\n",
                "\n",
                "        total_reflections = 0\n",
                "        predicted_new_entities = []\n",
                "        performance_list = []\n",
                "\n",
                "        # Process all examples in the folder\n",
                "        for file_path in reflections_done_path.glob(\"*.json\"):\n",
                "\n",
                "            total_reflections += 1\n",
                "\n",
                "            # extract file name\n",
                "            file_index = Path(file_path).stem\n",
                "\n",
                "            # read base result file\n",
                "            base_results_folder = f\"results/demo_type/{topic}/in_context_top{n}\"\n",
                "            with open(f\"{base_results_folder}/{file_index}.json\", \"r\", encoding=\"utf-8\") as f:\n",
                "                content = f.read()\n",
                "\n",
                "            # Fix JSON extra comma\n",
                "            content = re.sub(r',\\s*$', '', content)\n",
                "            old_data = json.loads(content)\n",
                "\n",
                "            # read new file\n",
                "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
                "                content = f.read()\n",
                "\n",
                "            # Fix JSON extra comma\n",
                "            content = re.sub(r',\\s*$', '', content)\n",
                "            new_data = json.loads(content)\n",
                "\n",
                "            # Apply encoding fix\n",
                "            old_data = recursive_fix(old_data)\n",
                "            new_data = recursive_fix(new_data)\n",
                "\n",
                "            # extract entities  \n",
                "            sentence = old_data.get(\"sentence\", \"\")\n",
                "            true_entities = old_data.get(\"true_entities\", [])\n",
                "            llm_entities = old_data.get(\"entities\", [])\n",
                "            new_entities = new_data.get(\"reflected_entities\", [])\n",
                "\n",
                "            # purge empty entities\n",
                "            new_entities = [entity for entity in new_entities if entity[\"entity\"]]\n",
                "\n",
                "            predicted_entities = []\n",
                "            for entity in llm_entities + new_entities:\n",
                "                \n",
                "                # remove duplicates\n",
                "                for added_entity in predicted_entities:\n",
                "                    if entity[\"span\"].lower() == added_entity[\"span\"].lower() and entity[\"entity\"].lower() == added_entity[\"entity\"].lower():\n",
                "                        break\n",
                "                else:\n",
                "                    predicted_entities.append(entity)\n",
                "\n",
                "            predicted_new_entities.append((file_index, len(new_entities)))\n",
                "\n",
                "            # previous perfomance\n",
                "            true_entities = [Entity.from_dict(entity) for entity in true_entities]\n",
                "\n",
                "            # create old prediction object\n",
                "            old_prediction = Prediction(0, \"\")\n",
                "            old_prediction.set_results(true_entities, llm_entities)\n",
                "            old_prediction.compute_performance()\n",
                "\n",
                "            # create new prediction object\n",
                "            new_prediction = Prediction(0, \"\")\n",
                "            new_prediction.set_results(true_entities, predicted_entities)\n",
                "            new_prediction.compute_performance()\n",
                "\n",
                "            performance_list.append((file_index, old_prediction.performance, new_prediction.performance))\n",
                "\n",
                "        # write to file\n",
                "        with open(f\"performance_reflection/{topic}_{reflection_folder.replace('/', '_')}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
                "\n",
                "            f.write(f\"total reflections: {total_reflections} \\n\")\n",
                "            f.write(\"new predicted entities: \\n\")\n",
                "            series = pd.Series([p[1] for p in predicted_new_entities]).value_counts().sort_index()\n",
                "            f.write(series.to_string())\n",
                "\n",
                "            f.write(\"\\n\\n\")\n",
                "            tp_diff_list = []\n",
                "            fp_diff_list = []\n",
                "            fn_diff_list = []\n",
                "            f1_diff_list = []\n",
                "\n",
                "            up_from_0 = 0\n",
                "            down_from_100 = 0\n",
                "\n",
                "            for performance in performance_list:\n",
                "                tp_diff_list.append(performance[2].tp - performance[1].tp)\n",
                "                fp_diff_list.append(performance[2].fp - performance[1].fp)\n",
                "                fn_diff_list.append(performance[2].fn - performance[1].fn)\n",
                "                f1_diff_list.append(performance[2].f1() - performance[1].f1())\n",
                "\n",
                "                tp_check = \"✅\" if performance[1].tp < performance[2].tp else \"➖\" if performance[1].tp == performance[2].tp else \"❌\"\n",
                "                fp_check = \"✅\" if performance[1].fp > performance[2].fp else \"➖\" if performance[1].fp == performance[2].fp else \"❌\"\n",
                "                fn_check = \"✅\" if performance[1].fn > performance[2].fn else \"➖\" if performance[1].fn == performance[2].fn else \"❌\"\n",
                "                f1_check = \"✅\" if performance[1].f1() < performance[2].f1() else \"➖\" if performance[1].f1() == performance[2].f1() else \"❌\"\n",
                "\n",
                "                if performance[1].f1() == 0 and performance[1].f1() < performance[2].f1():\n",
                "                    up_from_0 += 1\n",
                "\n",
                "                if performance[1].f1() == 100 and performance[1].f1() > performance[2].f1():\n",
                "                    down_from_100 += 1\n",
                "\n",
                "                f.write(f\"{performance[0]:>3} \\t tp {performance[1].tp} > {performance[2].tp} {tp_check} \\t fp {performance[1].fp} > {performance[2].fp} {fp_check} \\t fn {performance[1].fn} > {performance[2].fn} {fn_check} \\t f1 {round(performance[1].f1(), 2)} > {round(performance[2].f1(), 2)} {f1_check} \\n\")\n",
                "\n",
                "            f.write(\"\\n\\n\")\n",
                "            f.write(\"tp summary\\n\")\n",
                "            f.write(pd.Series(tp_diff_list).value_counts().sort_index().to_string())\n",
                "\n",
                "            f.write(\"\\n\\n\")\n",
                "            f.write(\"fp summary\\n\")\n",
                "            f.write(pd.Series(fp_diff_list).value_counts().sort_index().to_string())\n",
                "\n",
                "            f.write(\"\\n\\n\")\n",
                "            f.write(\"fn summary\\n\")\n",
                "            f.write(pd.Series(fn_diff_list).value_counts().sort_index().to_string())\n",
                "\n",
                "            f.write(\"\\n\\n\")\n",
                "            f.write(\"f1 summary\\n\")\n",
                "            f.write(pd.Series(f1_diff_list).describe().to_string())\n",
                "\n",
                "            f.write(\"\\n\\n\")\n",
                "            f.write(\"before vs after\\n\")\n",
                "            old_total_tp = sum([p[1].tp for p in performance_list])\n",
                "            old_total_fp = sum([p[1].fp for p in performance_list])\n",
                "            old_total_fn = sum([p[1].fn for p in performance_list])\n",
                "            old_micro_f1 = Performance(old_total_tp, old_total_fp, old_total_fn).f1()\n",
                "\n",
                "            new_total_tp = sum([p[2].tp for p in performance_list])\n",
                "            new_total_fp = sum([p[2].fp for p in performance_list])\n",
                "            new_total_fn = sum([p[2].fn for p in performance_list])\n",
                "            new_micro_f1 = Performance(new_total_tp, new_total_fp, new_total_fn).f1()\n",
                "\n",
                "            f.write(f\"TP: {old_total_tp} > {new_total_tp} \\n\")\n",
                "            f.write(f\"FP: {old_total_fp} > {new_total_fp} \\n\")\n",
                "            f.write(f\"FN: {old_total_fn} > {new_total_fn} \\n\")\n",
                "            f.write(f\"Micro F1: {old_micro_f1} > {new_micro_f1} \\n\")\n",
                "\n",
                "            f.write(\"\\n\\n\")\n",
                "            f.write(\"f1 sentence-level\\n\")\n",
                "            f.write(f\"better F1: {sum(1 for x in f1_diff_list if x > 0)}\\n\")\n",
                "            f.write(f\"same F1: {sum(1 for x in f1_diff_list if x == 0)}\\n\")\n",
                "            f.write(f\"worse F1: {sum(1 for x in f1_diff_list if x < 0)}\\n\")\n",
                "            f.write(f\"up from 0: {up_from_0}\\n\")\n",
                "            f.write(f\"down from 100: {down_from_100}\\n\")\n",
                "\n",
                "            print(f\"{topic}, {reflection_folder}: Micro F1: {old_micro_f1} > {new_micro_f1}\")\n",
                "\n",
                "    print(\"\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
