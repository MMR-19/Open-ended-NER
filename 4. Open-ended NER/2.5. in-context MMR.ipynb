{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "7f6e5c22",
            "metadata": {},
            "source": [
                "Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f05205be",
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import json\n",
                "from pathlib import Path\n",
                "\n",
                "from concurrent.futures import ThreadPoolExecutor\n",
                "\n",
                "from sklearn.metrics.pairwise import cosine_similarity\n",
                "\n",
                "# add path \n",
                "sys.path.append(os.path.abspath(\"../0. Helpers\"))\n",
                "sys.path.append(os.path.abspath(\"../2. Data Processing/_dataset_entities\"))\n",
                "\n",
                "from datasets import load_dataset, load_from_disk\n",
                "from datasetProcessing import tokens_to_sentence, tokens_to_entities, join_datasets, recursive_fix"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a4e8f7e2",
            "metadata": {},
            "source": [
                "Process whole dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e1e4d692",
            "metadata": {},
            "outputs": [],
            "source": [
                "topic = \"music\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e49ca6be",
            "metadata": {},
            "outputs": [],
            "source": [
                "if topic == \"lener\":\n",
                "    from entities_leNER import entity_names, entity_names_parsed\n",
                "    dataset = load_from_disk(\"...\")\n",
                "    lang = \"portuguese\"\n",
                "\n",
                "elif topic == \"neuralshift\":\n",
                "    from entities_neuralshift import entity_names, entity_names_parsed\n",
                "    dataset = load_from_disk(\"...\")\n",
                "    lang = \"portuguese\"\n",
                "\n",
                "elif topic == \"ener\":\n",
                "    from entities_eNER import entity_names, entity_names_parsed\n",
                "    dataset = load_from_disk(\"...\")\n",
                "    lang = \"english\"\n",
                "\n",
                "elif topic == \"multinerd_en\":\n",
                "    from entities_multinerd_en import entity_names, entity_names_parsed\n",
                "    dataset = load_from_disk(\"...\")\n",
                "    lang = \"english\"\n",
                "\n",
                "elif topic == \"multinerd_pt\":\n",
                "    from entities_multinerd_pt import entity_names, entity_names_parsed\n",
                "    dataset = load_from_disk(\"...\")\n",
                "    lang = \"portuguese\"\n",
                "\n",
                "else:\n",
                "    from entities_crossNER import entity_names, entity_names_parsed\n",
                "    dataset = load_dataset(\"...\")\n",
                "    lang = \"english\"\n",
                "\n",
                "# train_data\n",
                "train_data = dataset[\"train\"]\n",
                "test_data = dataset[\"test\"]\n",
                "\n",
                "# get the entity names\n",
                "start_of_entity_indices = [i for i in range(len(entity_names)) if (entity_names[i].startswith(\"B-\") or entity_names[i].startswith(\"U-\"))]\n",
                "entity_index_to_name = {i: entity_names[i].split(\"-\")[1] for i in range(len(entity_names)) if entity_names[i] != \"O\"}\n",
                "entity_index_to_name[0] = \"O\""
            ]
        },
        {
            "cell_type": "markdown",
            "id": "42449b3b",
            "metadata": {},
            "source": [
                "Run for all test instances"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8d69c11b",
            "metadata": {},
            "outputs": [],
            "source": [
                "test_len = len(dataset[\"test\"])\n",
                "train_len = len(dataset[\"train\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1168893a",
            "metadata": {},
            "source": [
                "Get top n demos"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "76d13eb8",
            "metadata": {},
            "outputs": [],
            "source": [
                "lambda_mmr = 0.5"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7a2fbc33",
            "metadata": {},
            "outputs": [],
            "source": [
                "all_n = [5]\n",
                "\n",
                "# Ensure result dir exists\n",
                "for n in all_n:\n",
                "    os.makedirs(f\"in_context/{topic}/test/mmr{str(n)}/qwen\", exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "367b1c4e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load train embeddings and sentences\n",
                "train_embeddings, train_files = [], []\n",
                "\n",
                "for train_index in range(train_len):\n",
                "\n",
                "    db_file_path = f\"classification/{topic}/train/data/{train_index}.json\"\n",
                "    db_file = json.load(open(db_file_path, \"r\", encoding=\"utf-8\"))\n",
                "\n",
                "    # Get embeddings from folder\n",
                "    with open(f\"embeddings/{topic}/train/{train_index}.json\", \"r\", encoding=\"utf-8\") as f:\n",
                "        train_embedding_data = json.load(f)\n",
                "        db_embedding_qwen = train_embedding_data['embedding_qwen']\n",
                "\n",
                "    # Get the entities\n",
                "    true_entities = tokens_to_entities(db_file['tokens'], db_file['ner_tags'], entity_names_parsed, start_of_entity_indices, entity_index_to_name)\n",
                "\n",
                "    # Append to train objects\n",
                "    train_embeddings.append(db_embedding_qwen)\n",
                "\n",
                "    train_files.append({\n",
                "        'index': train_index,\n",
                "        'embedding_qwen': db_embedding_qwen,\n",
                "        \"sentence\": db_file['sentence'],\n",
                "        \"true_entities\": [entity.to_dict() for entity in true_entities],\n",
                "    })\n",
                "\n",
                "train_embeddings = np.array(train_embeddings)\n",
                "print(f\"✅ Cached {len(train_embeddings)} training embeddings\")\n",
                "\n",
                "# Precompute cosine similarity matrix once\n",
                "train_sim_matrix = cosine_similarity(train_embeddings)\n",
                "print(f\"✅ Computed {train_sim_matrix.shape[0]} x {train_sim_matrix.shape[1]} training embeddings\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d13f743e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Loop test\n",
                "for test_index in range(test_len):\n",
                "\n",
                "    print(f\"\\rtest {test_index+1}/{test_len}\", end='', flush=True)\n",
                "\n",
                "    # check if mmr already computed\n",
                "    mmr_n_done = []\n",
                "    for n in all_n:\n",
                "        output_file = f\"in_context/{topic}/test/mmr{str(n)}/qwen/{test_index}.txt\"\n",
                "        mmr_n_done.append(os.path.exists(output_file))\n",
                "\n",
                "    if all(mmr_n_done):\n",
                "        print(f\"MMR for test example {test_index} already computed, skipping...\")\n",
                "        continue\n",
                "\n",
                "    test_example = dataset[\"test\"][test_index]\n",
                "    test_sentence = tokens_to_sentence(test_example['tokens'])\n",
                "\n",
                "    # Get test embeddings from folder\n",
                "    with open(f\"embeddings/{topic}/test/{test_index}.json\", \"r\", encoding=\"utf-8\") as f:\n",
                "        test_embedding_data = json.load(f)\n",
                "        test_embedding_qwen = test_embedding_data['embedding_qwen']\n",
                "\n",
                "    # Calculate cosine similarity of documents with the query\n",
                "    query_similarities = cosine_similarity([test_embedding_qwen], train_embeddings)[0]\n",
                "\n",
                "    # Initialize variables\n",
                "    selected_indices = []\n",
                "    remaining_indices = np.arange(len(train_embeddings))\n",
                "\n",
                "    # MMR selection process (select top n)\n",
                "    for _ in range(max(all_n)):\n",
                "\n",
                "        if not selected_indices:\n",
                "            mmr_scores = query_similarities[remaining_indices]\n",
                "        else:\n",
                "            diversity_scores = np.max(train_sim_matrix[np.ix_(remaining_indices, selected_indices)], axis=1)\n",
                "            mmr_scores = (lambda_mmr * query_similarities[remaining_indices] - (1 - lambda_mmr) * diversity_scores)\n",
                "\n",
                "        best_idx_local = np.argmax(mmr_scores)\n",
                "        selected_indices.append(remaining_indices[best_idx_local])\n",
                "        remaining_indices = np.delete(remaining_indices, best_idx_local)\n",
                "\n",
                "        # mmr_scores = []\n",
                "        \n",
                "        # for i in remaining_indices:\n",
                "\n",
                "        #     # Calculate diversity term\n",
                "        #     diversity_score = max(\n",
                "        #         cosine_similarity([train_embeddings[i]], [train_embeddings[j]])[0][0]\n",
                "        #         for j in selected_indices\n",
                "        #     ) if selected_indices else 0\n",
                "            \n",
                "        #     # MMR formula\n",
                "        #     mmr_score = lambda_mmr * query_similarities[i] - (1 - lambda_mmr) * diversity_score\n",
                "        #     mmr_scores.append((i, mmr_score))\n",
                "        \n",
                "        # # Select instance with highest MMR score\n",
                "        # best_train_instance = max(mmr_scores, key = lambda x: x[1])\n",
                "        # selected_indices.append(best_train_instance[0])\n",
                "        # remaining_indices.remove(best_train_instance[0])\n",
                "\n",
                "    for n in all_n:\n",
                "\n",
                "        qwen_example_txt = \"\"\n",
                "        for i in range(n):\n",
                "            idx = selected_indices[i]\n",
                "            db_file = train_files[idx]\n",
                "            qwen_example_txt += f\"Example #{i+1}: {db_file['sentence']}\\n\"\n",
                "            qwen_example_txt += f\"Expected output: 'entities: {db_file['true_entities']}'\\n\\n\"\n",
                "\n",
                "        output_file = f\"in_context/{topic}/test/mmr{str(n)}/qwen/{test_index}.txt\"\n",
                "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
                "            f.write(qwen_example_txt)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
