{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "7f6a429e",
            "metadata": {},
            "source": [
                "Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "53b725b1",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Libraries\n",
                "import sys\n",
                "import os\n",
                "import json\n",
                "import litellm \n",
                "from pydantic import BaseModel\n",
                "from enum import Enum\n",
                "from datasets import load_dataset, load_from_disk(\"...\")\n",
                "\n",
                "# add path to the dataset entities\n",
                "sys.path.append(os.path.abspath(\"../0. Helpers\"))\n",
                "sys.path.append(os.path.abspath(\"../2. Data Processing/_dataset_entities\"))\n",
                "\n",
                "from datasetProcessing import tokens_to_sentence, tokens_to_entities, join_datasets, recursive_fix\n",
                "from performance import Prediction"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ed408ba5",
            "metadata": {},
            "source": [
                "Configuations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e66ffec5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare LLM environment\n",
                "os.environ[\"AZURE_API_KEY\"] = \"...\"\n",
                "os.environ[\"AZURE_API_BASE\"] = \"...\"\n",
                "\n",
                "class LLM_Entity(BaseModel):\n",
                "    span: str\n",
                "    entity: str\n",
                "\n",
                "class LLM_Entity_List(BaseModel):\n",
                "    entities: list[LLM_Entity]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5ccd02d9",
            "metadata": {},
            "outputs": [],
            "source": [
                "results_folder = \"results/entity_info\"\n",
                "\n",
                "class Config:\n",
                "    def __init__(self, lang, prompt_type, prompt_subtype=None):\n",
                "        self.lang = lang\n",
                "        self.prompt_type = prompt_type\n",
                "        self.prompt_subtype = prompt_subtype\n",
                "\n",
                "    def __str__(self):\n",
                "        \n",
                "        prompt_str = self.prompt_type \n",
                "        if self.prompt_subtype:\n",
                "            prompt_str += \"_\" + self.prompt_subtype\n",
                "\n",
                "        return f\"{prompt_str}\"\n",
                "    \n",
                "all_configs = {\n",
                "    \"ai\": [\n",
                "        Config(\"en\", \"list\"), Config(\"en\", \"description\", \"general\"), Config(\"en\", \"description\", \"in-context\"), Config(\"en\", \"point\", \"token\"), Config(\"en\", \"point\", \"span\")\n",
                "    ],\n",
                "    \"literature\": [\n",
                "        Config(\"en\", \"list\"), Config(\"en\", \"description\", \"general\"), Config(\"en\", \"description\", \"in-context\"), Config(\"en\", \"point\", \"token\"), Config(\"en\", \"point\", \"span\")\n",
                "    ],\n",
                "    \"music\": [\n",
                "        Config(\"en\", \"list\"), Config(\"en\", \"description\", \"general\"), Config(\"en\", \"description\", \"in-context\"), Config(\"en\", \"point\", \"token\"), Config(\"en\", \"point\", \"span\")\n",
                "    ],\n",
                "    \"politics\": [\n",
                "        Config(\"en\", \"list\"), Config(\"en\", \"description\", \"general\"), Config(\"en\", \"description\", \"in-context\"), Config(\"en\", \"point\", \"token\"), Config(\"en\", \"point\", \"span\")\n",
                "    ],\n",
                "    \"science\": [\n",
                "        Config(\"en\", \"list\"), Config(\"en\", \"description\", \"general\"), Config(\"en\", \"description\", \"in-context\"), Config(\"en\", \"point\", \"token\"), Config(\"en\", \"point\", \"span\")\n",
                "    ],\n",
                "    \"multinerd_pt\": [\n",
                "        Config(\"pt\", \"list\"), Config(\"pt\", \"description\", \"general\"), Config(\"pt\", \"description\", \"in-context\"), Config(\"pt\", \"point\", \"token\"), Config(\"pt\", \"point\", \"span\")\n",
                "    ],\n",
                "    \"multinerd_en\": [\n",
                "        Config(\"en\", \"list\"), Config(\"en\", \"description\", \"general\"), Config(\"en\", \"description\", \"in-context\"), Config(\"en\", \"point\", \"token\"), Config(\"en\", \"point\", \"span\")\n",
                "    ],\n",
                "    \"ener\": [\n",
                "        Config(\"en\", \"list\"), Config(\"en\", \"description\", \"general\"), Config(\"en\", \"description\", \"in-context\"), Config(\"en\", \"point\", \"token\"), Config(\"en\", \"point\", \"span\")\n",
                "    ],\n",
                "    \"lener\": [\n",
                "        Config(\"pt\", \"list\"), Config(\"pt\", \"description\", \"general\"), Config(\"pt\", \"description\", \"in-context\"), Config(\"pt\", \"point\", \"token\"), Config(\"pt\", \"point\", \"span\")\n",
                "    ],\n",
                "    \"neuralshift\": [\n",
                "        Config(\"pt\", \"list\"), Config(\"pt\", \"description\", \"general\"), Config(\"pt\", \"description\", \"in-context\"), Config(\"pt\", \"point\", \"token\"), Config(\"pt\", \"point\", \"span\")\n",
                "    ]\n",
                "}"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "154e09d1",
            "metadata": {},
            "source": [
                "Prompt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7e768dbc",
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_prompt_prefix(topic, lang, prompt_type, prompt_sub_type=\"\"):\n",
                "    entity_info = \"\"\n",
                "\n",
                "    # Split by prompt type\n",
                "\n",
                "    if prompt_type == \"list\":\n",
                "        entity_info = open(f\"entity_info/list/{topic}.txt\", \"r\", encoding=\"utf-8\").read()\n",
                "\n",
                "    elif prompt_type == \"description\":\n",
                "        entity_info = open(f\"entity_info/description/{prompt_sub_type}/{topic}.txt\", \"r\", encoding=\"utf-8\").read()\n",
                "\n",
                "    elif prompt_type == \"point\":\n",
                "\n",
                "        if prompt_sub_type == \"span\":\n",
                "            point_file = \"_point_span_4\"\n",
                "        elif prompt_sub_type == \"token\":\n",
                "            point_file = \"_point_token_6\"\n",
                "\n",
                "        point_dict = json.load(open(f\"entity_info/point_entities/{prompt_sub_type}/{topic}/train/{point_file}.json\", \"r\", encoding=\"utf-8\"))\n",
                "        for entity, clusters in point_dict.items():\n",
                "            entity_info += f\"- \\\"{entity}\\\" e.g. {', '.join(clusters)}\\n\"\n",
                "\n",
                "    # Split by topic\n",
                "\n",
                "    # AI\n",
                "    if topic == \"ai\":\n",
                "        prompt_prefix = f\"\"\"In this context, an *entity* refers to any real-world object or concept that is specifically named or referred to in the domain of artificial intelligence.\n",
                "Dates, times, abstract concepts, adjectives and verbs are NOT entities.\n",
                "\n",
                "Use the following set of possible entity labels:\n",
                "{entity_info}\n",
                "\n",
                "If an entity does not fit the types above it is considered \"misc\".\n",
                "Be sure to prioritize more specific entities, such as \"researcher\" over \"person\", \"conference\" over \"location\" and \"university\" over \"organisation\", when it makes sense.\n",
                "\"\"\"\n",
                "    # LITERATURE\n",
                "    elif topic == \"literature\":\n",
                "        prompt_prefix = f\"\"\"In this context, an *entity* refers to any real-world object or concept that is specifically named or referred to in the domain of literature.\n",
                "Dates, times, abstract concepts, adjectives and verbs are NOT entities.\n",
                "\n",
                "Use the following set of possible entity labels:\n",
                "{entity_info}\n",
                "\n",
                "If an entity does not fit the types above it is considered \"misc\".\n",
                "Be sure to prioritize more specific entities, such as \"writer\" over \"person\", when it makes sense.\n",
                "\"\"\"\n",
                "    \n",
                "    # MUSIC\n",
                "    elif topic == \"music\":\n",
                "        prompt_prefix = f\"\"\"In this context, an *entity* refers to any real-world object or concept that is specifically named or referred to in the domain of music.\n",
                "Dates, times, abstract concepts, adjectives and verbs are NOT entities.\n",
                "\n",
                "Use the following set of possible entity labels:\n",
                "{entity_info}\n",
                "\n",
                "If an entity does not fit the types above it is considered \"misc\".\n",
                "Be sure to prioritize more specific entities, such as \"musical artist\" over \"person\" and \"band\" over \"organisation\", when it makes sense.\n",
                "\"\"\"\n",
                "\n",
                "    # POLITICS\n",
                "    elif topic == \"politics\":\n",
                "        prompt_prefix = f\"\"\"In this context, an *entity* refers to any real-world object or concept that is specifically named or referred to in the domain of politics.\n",
                "Dates, times, abstract concepts, adjectives and verbs are NOT entities.\n",
                "\n",
                "Use the following set of possible entity labels:\n",
                "{entity_info}\n",
                "\n",
                "If an entity does not fit the types above it is considered \"misc\".\n",
                "Be sure to prioritize more specific entities, such as \"politician\" over \"person\" and \"political party\" over \"organisation\", when it makes sense.\n",
                "\"\"\"\n",
                "        \n",
                "    # SCIENCE\n",
                "    elif topic == \"science\":\n",
                "        prompt_prefix = f\"\"\"In this context, an *entity* refers to any real-world object or concept that is specifically named or referred to in the domain of science.\n",
                "Dates, times, abstract concepts, adjectives and verbs are NOT entities.\n",
                "\n",
                "Use the following set of possible entity labels:\n",
                "{entity_info}\n",
                "\n",
                "Abstract scientific concepts can be entities if they have a name associated with them.\n",
                "\n",
                "If an entity does not fit the types above it is considered \"misc\".\n",
                "Be sure to prioritize more specific entities, such as \"scientist\" over \"person\" and \"university\" over \"organisation\" or \"location\", when it makes sense.\n",
                "\"\"\"\n",
                "\n",
                "    # MULTINERD PT\n",
                "    elif topic == \"multinerd_pt\":\n",
                "        prompt_prefix = f\"\"\"Neste contexto, uma *entidade* refere-se a qualquer objeto ou conceito do mundo real que seja especificamente mencionado ou referido.\n",
                "Datas, horas, conceitos abstratos, adjetivos e verbos NÃO são entidades.\n",
                "\n",
                "Usa o seguinte conjunto de tipos possíveis de entidade:\n",
                "{entity_info}\n",
                "\n",
                "Se uma entidade não se encaixar em nenhum dos tipos acima, não a incluas na resposta.\n",
                "\"\"\"\n",
                "        \n",
                "    # MULTINERD EN\n",
                "    elif topic == \"multinerd_en\":\n",
                "        prompt_prefix = f\"\"\"In this context, an *entity* refers to any real-world object or concept that is specifically named or referred to.\n",
                "Dates, times, abstract concepts, adjectives and verbs are NOT entities.\n",
                "\n",
                "Use the following set of possible entity labels:\n",
                "{entity_info}\n",
                "\"\"\"\n",
                "        \n",
                "    # E-NER\n",
                "    elif topic == \"ener\":\n",
                "        prompt_prefix = f\"\"\"In this context, an *entity* refers to any real-world object or concept that is specifically named or referred to in the legal domain.\n",
                "Dates, times, abstract concepts, adjectives and verbs are NOT entities.\n",
                "\n",
                "Use the following set of possible entity labels:\n",
                "{entity_info}\n",
                "\n",
                "If an entity does not fit the types above it is considered \"misc\".\n",
                "\"\"\"\n",
                "        \n",
                "    # LeNER-Br + NEURALSHIFT\n",
                "    elif (topic == \"lener\" or topic == \"neuralshift\"):\n",
                "        prompt_prefix = f\"\"\"Neste contexto, uma *entidade* refere-se a qualquer objeto ou conceito do mundo real que seja especificamente mencionado ou referido no domínio legal.\n",
                "Conceitos abstratos, adjetivos e verbos NÃO são entidades.\n",
                "\n",
                "Usa o seguinte conjunto de tipos possíveis de entidade:\n",
                "{entity_info}\n",
                "\n",
                "Se uma entidade não se encaixar em nenhum dos tipos acima, não a incluas na resposta.\n",
                "\"\"\"\n",
                "        \n",
                "################## FINAL PROMPT ##################\n",
                "\n",
                "    # validation\n",
                "    if entity_info==\"\" or prompt_prefix == \"\":\n",
                "        raise ValueError(f\"Error retrieving entity info for topic {topic}, prompt type {prompt_type} and prompt sub type {prompt_sub_type}.\")\n",
                "\n",
                "    # final prompt instruction\n",
                "    if lang == \"en\":\n",
                "        return f\"\"\"{prompt_prefix}\n",
                "\n",
                "Return the entities in a structured JSON format with the following fields:\n",
                "- \"span\": the exact span of the entity as it appears in the input\n",
                "- \"entity\": the category/type of the entity\n",
                "\n",
                "Now extract entities from the following text:\n",
                "\"\"\"\n",
                "    elif lang == \"pt\":\n",
                "        return f\"\"\"{prompt_prefix}\n",
                "\n",
                "Retorna as entidades num formato JSON estruturado com os seguintes campos:\n",
                "- \"span\": o span exato da entidade conforme aparece no texto de input\n",
                "- \"entity\": a classe/tipo da entidade\n",
                "\n",
                "Agora extrai as entidades do seguinte texto:\n",
                "\"\"\"\n",
                "    else:\n",
                "        raise ValueError(f\"Language {lang} not supported.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7c764fc0",
            "metadata": {},
            "source": [
                "LLM functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b358bf5e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Call LLM\n",
                "def safe_llm_call(prompt, system, instance):\n",
                "    try:\n",
                "        \n",
                "        response = litellm.completion(\n",
                "            model = \"azure/gpt-4o-mini\",\n",
                "            messages = [\n",
                "                {\"role\": \"system\", \"content\": system},\n",
                "                {\"role\": \"user\", \"content\": prompt},\n",
                "            ],\n",
                "\n",
                "            temperature = 0.1,\n",
                "            response_format = LLM_Entity_List,\n",
                "\n",
                "            # stream = False,\n",
                "            # top_p = 1,\n",
                "        )\n",
                "\n",
                "        # extract LLM predictions\n",
                "        return response.choices[0].message[\"content\"]\n",
                "\n",
                "    except Exception as e:\n",
                "        print(f\"\\n❌❌ LLM call failed: {e}\\n\")\n",
                "        print(f\"\\nInstance: {instance}\")\n",
                "        raise\n",
                "\n",
                "def process_instance(topic, config: Config, i, instance, system_prompt, entity_names_parsed, start_of_entity_indices, entity_index_to_name):\n",
                "    \n",
                "    results_path = f\"{results_folder}/{topic}/{str(config)}/{i}.json\"\n",
                "\n",
                "    # Check if the results file already exists\n",
                "    if os.path.exists(results_path):\n",
                "        print(f\" >>> Results for sentence #{i+1} already exist. Skipping...\")\n",
                "        return\n",
                "    \n",
                "    # get the instance\n",
                "    sentence = tokens_to_sentence(instance['tokens'])\n",
                "    true_entities = tokens_to_entities(instance['tokens'], instance['ner_tags'], entity_names_parsed, start_of_entity_indices, entity_index_to_name)\n",
                "\n",
                "    # set prompt\n",
                "    prompt = get_prompt_prefix(topic, config.lang, config.prompt_type, config.prompt_subtype)\n",
                "    prompt = f\"{prompt}\\n{sentence}\"\n",
                "\n",
                "    # Save prompt to txt file\n",
                "    prompt_file_path = f\"{results_folder}/{topic}/{str(config)}/prompts/prompt_{i}.txt\"\n",
                "    with open(prompt_file_path, \"w\", encoding=\"utf-8\") as f:\n",
                "        f.write(prompt)\n",
                "\n",
                "    # create prediction object\n",
                "    prediction = Prediction(i, sentence)\n",
                "\n",
                "    # call the LLM\n",
                "    try:\n",
                "        llm_response = safe_llm_call(prompt, system_prompt, sentence)\n",
                "\n",
                "        if llm_response is None:\n",
                "            print(f\"❌ LLM response is None for sentence #{i}: {sentence}\")\n",
                "            return\n",
                "\n",
                "        llm_json = json.loads(llm_response)\n",
                "        llm_entities = llm_json.get('entities', [])\n",
                "\n",
                "    except Exception as e:\n",
                "        print(f\"❌ Error on sentence #{i}: {e}\")\n",
                "        return\n",
                "\n",
                "    # compute predictions\n",
                "    prediction.set_results(true_entities, llm_entities)\n",
                "    prediction.compute_performance()\n",
                "    prediction.compute_relaxed_performance()\n",
                "\n",
                "    # write json to file\n",
                "    result_json = {\n",
                "        \"id\": i,\n",
                "        \"sentence\": sentence,\n",
                "        \"true_entities\": [entity.to_dict() for entity in true_entities],\n",
                "        \"entities\": llm_entities,\n",
                "         \"performance\": {\n",
                "            \"tp\": prediction.performance.tp,\n",
                "            \"fp\": prediction.performance.fp,\n",
                "            \"fn\": prediction.performance.fn,\n",
                "            \"precision\": prediction.performance.precision(),\n",
                "            \"recall\": prediction.performance.recall(),\n",
                "            \"f1\": prediction.performance.f1()\n",
                "        },\n",
                "        \"relaxed_performance\": {\n",
                "            \"tp\": prediction.relaxed_performance.tp,\n",
                "            \"fp\": prediction.relaxed_performance.fp,\n",
                "            \"fn\": prediction.relaxed_performance.fn,\n",
                "            \"precision\": prediction.relaxed_performance.precision(),\n",
                "            \"recall\": prediction.relaxed_performance.recall(),\n",
                "            \"f1\": prediction.relaxed_performance.f1()\n",
                "        },\n",
                "        \"tokens\": instance['tokens'],\n",
                "        \"ner_tags\": instance['ner_tags']\n",
                "    }  \n",
                "\n",
                "    # save results to file\n",
                "    with open(results_path, \"a\", encoding=\"utf-8\") as f:\n",
                "        f.write(json.dumps(result_json, ensure_ascii=False, indent=4))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2a118941",
            "metadata": {},
            "source": [
                "Run for each config"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "04d0f8aa",
            "metadata": {},
            "outputs": [],
            "source": [
                "def system_prompt(lang):\n",
                "    if lang == \"en\":\n",
                "        return \"You are a named entity recognition (NER) system. Your task is to extract all entities mentioned in the input text. Always respond with JSON containing named entities.\"\n",
                "    elif lang == \"pt\":\n",
                "        return \"És um sistema de reconhecimento de entidades (NER). A tua tarefa é extrair todas as entidades mencionadas no texto de input. Responde sempre no formato JSON com as entidades.\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "430a7f6a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# system based on lang\n",
                "for topic, configs in all_configs.items():\n",
                "\n",
                "    # Load the dataset\n",
                "    if topic == \"lener\":\n",
                "        from entities_leNER import entity_names, entity_names_parsed\n",
                "        dataset = load_from_disk(\"...\")\n",
                "\n",
                "    elif topic == \"neuralshift\":\n",
                "        from entities_neuralshift import entity_names, entity_names_parsed\n",
                "        dataset = load_from_disk(\"...\")\n",
                "\n",
                "    elif topic == \"ener\":\n",
                "        from entities_eNER import entity_names, entity_names_parsed\n",
                "        dataset = load_from_disk(\"...\")\n",
                "\n",
                "    elif topic == \"multinerd_en\":\n",
                "        from entities_multinerd_en import entity_names, entity_names_parsed\n",
                "        dataset = load_from_disk(\"...\")\n",
                "\n",
                "    elif topic == \"multinerd_pt\":\n",
                "        from entities_multinerd_pt import entity_names, entity_names_parsed\n",
                "        dataset = load_from_disk(\"...\")\n",
                "\n",
                "    else:\n",
                "        from entities_crossNER import entity_names, entity_names_parsed\n",
                "        dataset = load_dataset(\"...\")\n",
                "\n",
                "    # all_data = test\n",
                "    all_data = dataset['test']\n",
                "\n",
                "    # get the entity names\n",
                "    start_of_entity_indices = [i for i in range(len(entity_names)) if (entity_names[i].startswith(\"B-\") or entity_names[i].startswith(\"U-\"))]\n",
                "    entity_index_to_name = {i: entity_names[i].split(\"-\")[1] for i in range(len(entity_names)) if entity_names[i] != \"O\"}\n",
                "    entity_index_to_name[0] = \"O\"\n",
                "\n",
                "    # Run for each config\n",
                "    for config in configs:\n",
                "\n",
                "        lang = config.lang\n",
                "        config_folder = str(config)\n",
                "\n",
                "        # Ensure the results directory exists\n",
                "        os.makedirs(f\"{results_folder}/{topic}/{config_folder}\", exist_ok=True)\n",
                "        os.makedirs(f\"{results_folder}/{topic}/{config_folder}/prompts\", exist_ok=True)\n",
                "\n",
                "        # Run through all instances\n",
                "        print(\"\\n\\nRunning config:\", topic, config_folder)\n",
                "        for i, instance in enumerate(all_data):\n",
                "            print(f\"\\r\\tProcessing instance {i+1}/{len(all_data)}\", end='', flush=True)\n",
                "            process_instance(topic, config, i, instance, system_prompt(lang), entity_names_parsed, start_of_entity_indices, entity_index_to_name)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
