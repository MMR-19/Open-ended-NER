{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "f8e93436",
            "metadata": {},
            "source": [
                "Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0cbbef77",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Libraries\n",
                "import numpy as np\n",
                "import sys\n",
                "import os\n",
                "import json\n",
                "from pathlib import Path\n",
                "import re\n",
                "\n",
                "# add path to the dataset entities\n",
                "sys.path.append(os.path.abspath(\"../0. Helpers\"))\n",
                "sys.path.append(os.path.abspath(\"../2. Data Processing/_dataset_entities\"))\n",
                "\n",
                "from datasetProcessing import Entity, recursive_fix\n",
                "from performance import Prediction, Performance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a21c40c8",
            "metadata": {},
            "outputs": [],
            "source": [
                "def process_instance(file_path, topic_reflection_folder, ignore_empty=False):\n",
                "    \n",
                "    try:\n",
                "        with open(file_path, mode='r', encoding=\"utf-8\") as f:\n",
                "            content = f.read()\n",
                "\n",
                "        if not content.strip():\n",
                "            print(f\"üóëÔ∏è Empty file detected, deleting: {file_path}\")\n",
                "            # file_path.unlink()\n",
                "            return None\n",
                "\n",
                "        # Fix JSON extra comma\n",
                "        content = re.sub(r',\\s*$', '', content)\n",
                "        old_data = json.loads(content)\n",
                "\n",
                "        # Apply encoding fix\n",
                "        old_data = recursive_fix(old_data)\n",
                "\n",
                "        # extract entities\n",
                "        true_entities = old_data.get(\"true_entities\", [])\n",
                "        llm_entities = old_data.get(\"entities\", [])\n",
                "        \n",
                "        # check for reflection\n",
                "        reflection_file_path = f\"results/error_reflection/{topic_reflection_folder}/{file_path.name}\"\n",
                "        if os.path.exists(reflection_file_path):\n",
                "            with open(reflection_file_path, mode='r', encoding=\"utf-8\") as rf:\n",
                "                reflection_content = rf.read()\n",
                "            \n",
                "            # Fix JSON extra comma\n",
                "            reflection_content = re.sub(r',\\s*$', '', reflection_content)\n",
                "            new_data = json.loads(reflection_content)\n",
                "\n",
                "            # Apply encoding fix\n",
                "            new_data = recursive_fix(new_data)\n",
                "\n",
                "            # extract entities\n",
                "            new_entities = new_data.get(\"reflected_entities\", [])\n",
                "        else:\n",
                "            new_entities = []   \n",
                "\n",
                "        predicted_entities = []\n",
                "        for entity in llm_entities + new_entities:\n",
                "            \n",
                "            # remove duplicates\n",
                "            for added_entity in predicted_entities:\n",
                "                if entity[\"span\"].strip().lower() == added_entity[\"span\"].strip().lower() and entity[\"entity\"].strip().lower() == added_entity[\"entity\"].strip().lower():\n",
                "                    break\n",
                "            else:\n",
                "                predicted_entities.append(entity)\n",
                "\n",
                "        if not true_entities and ignore_empty:\n",
                "            print(f\"üóëÔ∏è No entities found in {file_path}, ignoring performance\")\n",
                "            return None\n",
                "        \n",
                "        # process true entities from dict to Entity objects\n",
                "        true_entities = [Entity.from_dict(entity) for entity in true_entities]\n",
                "\n",
                "        # create prediction object\n",
                "        prediction = Prediction(0, \"\")\n",
                "\n",
                "        # compute predictions\n",
                "        prediction.set_results(true_entities, predicted_entities)\n",
                "        prediction.compute_performance()\n",
                "        prediction.compute_relaxed_performance()\n",
                "\n",
                "        return prediction\n",
                "\n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Error reading {file_path}: {e}\")\n",
                "        # file_path.unlink()\n",
                "        return None"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "82a63a13",
            "metadata": {},
            "source": [
                "Evaluate each model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "47ea21b1",
            "metadata": {},
            "outputs": [],
            "source": [
                "all_configs = {\n",
                "    \"ai\": 10,\n",
                "    \"literature\": 10,\n",
                "    \"music\": 10,\n",
                "    \"politics\": 20,\n",
                "    \"science\": 20,\n",
                "    \"multinerd_en\": 20,\n",
                "    \"multinerd_pt\": 20,\n",
                "    \"ener\": 20,\n",
                "    \"lener\": 20,\n",
                "    \"neuralshift\": 20\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ecf1adc2",
            "metadata": {},
            "outputs": [],
            "source": [
                "results_category = \"error_reflection\"\n",
                "\n",
                "base_category = \"demo_type\"\n",
                "base_folder = \"in_context_top\"\n",
                "\n",
                "reflection_folders = [\"unseen/adapted\", \"unseen/adapted_noCoT\"]\n",
                "\n",
                "def custom_sort_key(name):\n",
                "    if   name == \"error_reflection_unseen_adapted\":\n",
                "        return 2\n",
                "    elif name == \"error_reflection_unseen_adapted_noCoT\":\n",
                "        return 3\n",
                "    else:\n",
                "        return 0\n",
                "    \n",
                "latex_mapping = {\n",
                "    \"error_reflection_unseen_adapted\":        \"Adapted       \",\n",
                "    \"error_reflection_unseen_adapted_noCoT\":  \"Adapted no CoT\",\n",
                "}\n",
                "\n",
                "# reflection_folders = [\"false_negatives/adapted\", \"false_negatives/adapted_noCoT\"]\n",
                "\n",
                "# def custom_sort_key(name):\n",
                "#     if   name == \"error_reflection_false_negatives_adapted\":\n",
                "#         return 2\n",
                "#     elif name == \"error_reflection_false_negatives_adapted_noCoT\":\n",
                "#         return 3\n",
                "#     else:\n",
                "#         return 0\n",
                "    \n",
                "# latex_mapping = {\n",
                "#     \"error_reflection_false_negatives_adapted\":        \"FN & \\checkmark  \",\n",
                "#     \"error_reflection_false_negatives_adapted_noCoT\":  \"FN & -           \",\n",
                "# }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "10dd31a4",
            "metadata": {},
            "outputs": [],
            "source": [
                "for topic, n in all_configs.items():\n",
                "    print(f\"Processing topic: {topic}\")\n",
                "    config_folder = Path(f\"results/{base_category}/{topic}/{base_folder}{n}\")\n",
                "\n",
                "    if not config_folder.exists():\n",
                "        print(f\"‚ùå Topic folder {topic} does not exist.\")\n",
                "        continue\n",
                "\n",
                "    for reflection_folder in reflection_folders:\n",
                "        \n",
                "        if config_folder.is_dir():\n",
                "            print(f\"Processing configuration: {topic} - {config_folder.name} - {reflection_folder}\")\n",
                "\n",
                "            # Process all instances in the folder\n",
                "            dataset_performance = []\n",
                "            for file_path in config_folder.glob(\"*.json\"):\n",
                "                result = process_instance(file_path, f\"{topic}/{reflection_folder}\")\n",
                "                dataset_performance.append(result)\n",
                "\n",
                "            # Filter out None results\n",
                "            dataset_performance = [instance for instance in dataset_performance if instance is not None]\n",
                "            if not dataset_performance:\n",
                "                print(f\"‚ö†Ô∏è No valid instances found in {config_folder.name}. Skipping...\")\n",
                "                continue\n",
                "            else:\n",
                "                print(f\"‚úÖ Processed {len(dataset_performance)} valid instances in {config_folder.name}.\")\n",
                "\n",
                "            # compute individual performance metrics\n",
                "            metrics_dict = {\n",
                "                \"total_samples\": len(dataset_performance),\n",
                "                \"true_0\": sum(1 for instance in dataset_performance if len(instance.true_entities) == 0),\n",
                "                \"llm_0\": sum(1 for instance in dataset_performance if len(instance.llm_entities) == 0),\n",
                "                \"performance\": {\n",
                "                    \"tp\": [instance.performance.tp for instance in dataset_performance],\n",
                "                    \"fp\": [instance.performance.fp for instance in dataset_performance],\n",
                "                    \"fn\": [instance.performance.fn for instance in dataset_performance],\n",
                "                },\n",
                "                \"relaxed_performance\": {\n",
                "                    \"tp\": [instance.relaxed_performance.tp for instance in dataset_performance],\n",
                "                    \"fp\": [instance.relaxed_performance.fp for instance in dataset_performance],\n",
                "                    \"fn\": [instance.relaxed_performance.fn for instance in dataset_performance],\n",
                "                }\n",
                "            }\n",
                "\n",
                "            # compute performance metrics\n",
                "            overall_performance = Performance(\n",
                "                tp=sum(metrics_dict[\"performance\"][\"tp\"]),\n",
                "                fp=sum(metrics_dict[\"performance\"][\"fp\"]),\n",
                "                fn=sum(metrics_dict[\"performance\"][\"fn\"])\n",
                "            )\n",
                "\n",
                "            overall_relaxed_performance = Performance(\n",
                "                tp=sum(metrics_dict[\"relaxed_performance\"][\"tp\"]),\n",
                "                fp=sum(metrics_dict[\"relaxed_performance\"][\"fp\"]),\n",
                "                fn=sum(metrics_dict[\"relaxed_performance\"][\"fn\"])\n",
                "            )\n",
                "\n",
                "            # write dataset performance to file\n",
                "            overall_json = {    \n",
                "                \"total_samples\": metrics_dict[\"total_samples\"],\n",
                "                \"true_0\": metrics_dict[\"true_0\"],\n",
                "                \"llm_0\": metrics_dict[\"llm_0\"],\n",
                "                \"performance\": {\n",
                "                    \"tp\": overall_performance.tp,\n",
                "                    \"fp\": overall_performance.fp,\n",
                "                    \"fn\": overall_performance.fn,\n",
                "                    \"precision\": overall_performance.precision(),\n",
                "                    \"recall\": overall_performance.recall(),\n",
                "                    \"f1\": overall_performance.f1()\n",
                "                },\n",
                "                \"relaxed_performance\": {\n",
                "                    \"tp\": overall_relaxed_performance.tp,\n",
                "                    \"fp\": overall_relaxed_performance.fp,\n",
                "                    \"fn\": overall_relaxed_performance.fn,\n",
                "                    \"precision\": overall_relaxed_performance.precision(),\n",
                "                    \"recall\": overall_relaxed_performance.recall(),\n",
                "                    \"f1\": overall_relaxed_performance.f1()\n",
                "                }\n",
                "            }\n",
                "\n",
                "            print(\"  exact f1 \", round(overall_json[\"performance\"][\"f1\"], 3), round(overall_json[\"performance\"][\"f1\"], 1))\n",
                "            print(\"relaxed f1 \", round(overall_json[\"relaxed_performance\"][\"f1\"], 3), round(overall_json[\"relaxed_performance\"][\"f1\"], 1))\n",
                "\n",
                "            # Save the overall performance to a JSON file\n",
                "            save_path = f\"performance/{topic}\"\n",
                "            os.makedirs(save_path, exist_ok=True)\n",
                "\n",
                "            performance_file = f\"{save_path}/{results_category}_{reflection_folder.replace('/', '_')}.json\"\n",
                "            with open(performance_file, \"w\", encoding=\"utf-8\") as f:\n",
                "                f.write(json.dumps(overall_json, ensure_ascii=False, indent=4))\n",
                "            \n",
                "            print(f\"‚úÖ Performance saved to {performance_file}\")\n",
                "            print(\"\\n\\n\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "585dee96",
            "metadata": {},
            "outputs": [],
            "source": [
                "# include_categories = [\"demo_type\", \"demo_criterium\"]\n",
                "include_categories = [f.replace(\"/\", \"_\") for f in reflection_folders]\n",
                "topic_all_f1 = {}\n",
                "\n",
                "for topic in all_configs.keys():\n",
                "\n",
                "    all_f1 = []\n",
                "    performance_folder = Path(f\"performance/{topic}\")\n",
                "    for file in performance_folder.glob(\"*.json\"):\n",
                "        if results_category in file.name and any(cat in file.name for cat in include_categories):\n",
                "            with open(file, \"r\") as f:\n",
                "                overall_json = json.load(f)\n",
                "                all_f1.append((file.name.removesuffix('.json'), overall_json[\"performance\"][\"f1\"]))\n",
                "\n",
                "    # all_f1.sort(key=lambda x: x[1], reverse=True)\n",
                "    all_f1.sort(key = lambda x : custom_sort_key(x[0]))\n",
                "    \n",
                "    # add to topic all f1\n",
                "    topic_all_f1[topic] = all_f1\n",
                "\n",
                "    print(f\"Topic: {topic}\")\n",
                "    # print only best\n",
                "    for name, f1 in all_f1:\n",
                "        print(f\"{name}: {f1:.2f}\")\n",
                "\n",
                "    print()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
