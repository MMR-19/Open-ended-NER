{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f6e5c22",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05205be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# add path \n",
    "sys.path.append(os.path.abspath(\"../0. Helpers\"))\n",
    "sys.path.append(os.path.abspath(\"../2. Data Processing/_dataset_entities\"))\n",
    "\n",
    "from datasets import load_dataset, load_from_disk(\"...\")\n",
    "from datasetProcessing import tokens_to_sentence, tokens_to_entities, join_datasets, recursive_fix\n",
    "from reflection_helpers import word_only_punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b766d5b4",
   "metadata": {},
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbae4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\n",
    "    \"Qwen/Qwen3-Embedding-4B\",\n",
    "    # model_kwargs = {\n",
    "    #     # \"attn_implementation\": \"flash_attention_2\",\n",
    "    #     \"device_map\": \"auto\"},\n",
    "    tokenizer_kwargs = {\n",
    "        \"padding_side\": \"left\"},\n",
    ")\n",
    "\n",
    "# Move model to CPU\n",
    "model.to(\"cpu\")\n",
    "\n",
    "# Check model device\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3d31a6",
   "metadata": {},
   "source": [
    "Process whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0ea3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"lener\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee52165d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if topic == \"lener\":\n",
    "    from entities_leNER import entity_names, entity_names_parsed\n",
    "    dataset = load_from_disk(\"...\")\n",
    "    lang = \"portuguese\"\n",
    "\n",
    "elif topic == \"neuralshift\":\n",
    "    from entities_neuralshift import entity_names, entity_names_parsed\n",
    "    dataset = load_from_disk(\"...\")\n",
    "    lang = \"portuguese\"\n",
    "\n",
    "elif topic == \"ener\":\n",
    "    from entities_eNER import entity_names, entity_names_parsed\n",
    "    dataset = load_from_disk(\"...\")\n",
    "    lang = \"english\"\n",
    "\n",
    "elif topic == \"multinerd_en\":\n",
    "    from entities_multinerd_en import entity_names, entity_names_parsed\n",
    "    dataset = load_from_disk(\"...\")\n",
    "    lang = \"english\"\n",
    "\n",
    "elif topic == \"multinerd_pt\":\n",
    "    from entities_multinerd_pt import entity_names, entity_names_parsed\n",
    "    dataset = load_from_disk(\"...\")\n",
    "    lang = \"portuguese\"\n",
    "\n",
    "else:\n",
    "    from entities_crossNER import entity_names, entity_names_parsed\n",
    "    dataset = load_dataset(\"...\")\n",
    "    lang = \"english\"\n",
    "\n",
    "# train_data\n",
    "train_data = dataset[\"train\"]\n",
    "\n",
    "# get the entity names\n",
    "start_of_entity_indices = [i for i in range(len(entity_names)) if (entity_names[i].startswith(\"B-\") or entity_names[i].startswith(\"U-\"))]\n",
    "entity_index_to_name = {i: entity_names[i].split(\"-\")[1] for i in range(len(entity_names)) if entity_names[i] != \"O\"}\n",
    "entity_index_to_name[0] = \"O\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1ff506",
   "metadata": {},
   "source": [
    "Entity gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88332c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = list(set(stopwords.words(lang))) + [\"'s\"]\n",
    "\n",
    "def collect_entities(dataset_split, remove_duplicates = True):\n",
    "    \n",
    "    entities_by_type = defaultdict(list)\n",
    "    \n",
    "    for instance in dataset_split:\n",
    "        extracted_entities = tokens_to_entities(instance[\"tokens\"], instance[\"ner_tags\"], entity_names_parsed, start_of_entity_indices, entity_index_to_name)\n",
    "        \n",
    "        for entity in extracted_entities:\n",
    "            tokens = entity.tokens\n",
    "\n",
    "            # remove punctuation tokens\n",
    "            tokens = [token for token in tokens if not word_only_punctuation(token) and token.lower() not in STOPWORDS]\n",
    "            \n",
    "            entities_by_type[entity.entity].extend(tokens)\n",
    "    \n",
    "    if remove_duplicates:\n",
    "        for entity_type in entities_by_type:\n",
    "            entities_by_type[entity_type] = list(set(entities_by_type[entity_type]))\n",
    "            \n",
    "    return dict(entities_by_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cb6e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_tokens_by_type = collect_entities(train_data)\n",
    "print(entity_tokens_by_type)\n",
    "\n",
    "print()\n",
    "for entity_type, entities in entity_tokens_by_type.items():\n",
    "    print(f\"{entity_type}: {len(entities)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a77f92",
   "metadata": {},
   "source": [
    "Compute point/center entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef918abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_point_entities(entities, model, k=4):\n",
    "    \n",
    "    if not entities:\n",
    "        return []\n",
    "    \n",
    "    # If fewer entities than clusters, return unique mentions\n",
    "    if len(entities) <= k:\n",
    "        return list(set(entities))\n",
    "    \n",
    "    # Get embeddings\n",
    "    embeddings = model.encode(entities, convert_to_numpy=True, normalize_embeddings=True)\n",
    "    \n",
    "    # KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=k, random_state = 73, n_init = 10)\n",
    "    kmeans.fit(embeddings)\n",
    "    \n",
    "    centroids = kmeans.cluster_centers_\n",
    "    labels = kmeans.labels_\n",
    "    \n",
    "    point_entities = []\n",
    "\n",
    "    for cluster_id in range(k):\n",
    "        cluster_indices = np.where(labels == cluster_id)[0]\n",
    "        cluster_embeddings = embeddings[cluster_indices]\n",
    "\n",
    "        sims = cosine_similarity([centroids[cluster_id]], cluster_embeddings)[0]\n",
    "        \n",
    "        best_idx = cluster_indices[np.argmax(sims)]\n",
    "        point_entities.append(entities[best_idx])\n",
    "    \n",
    "    return point_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5889afc8",
   "metadata": {},
   "source": [
    "Run for all entity types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a2c9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=6\n",
    "point_entities_dict = {}\n",
    "\n",
    "for entity_type, entities in entity_tokens_by_type.items():\n",
    "    point_entities = extract_point_entities(entities, model, k)\n",
    "    point_entities_dict[entity_type] = point_entities\n",
    "    print(entity_type, \">\", point_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afff873b",
   "metadata": {},
   "source": [
    "Save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf61f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(point_entities_dict)\n",
    "\n",
    "point_type = \"token\"\n",
    "\n",
    "# Make sure folder exists\n",
    "Path(f\"entity_info/point_entities/{point_type}/{topic}/train/data\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save to json file\n",
    "file_path = f\"entity_info/point_entities/{point_type}/{topic}/train/data/all_entities.json\"\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(entity_tokens_by_type, ensure_ascii=False, indent=4))\n",
    "\n",
    "# Save to json file\n",
    "file_path = f\"entity_info/point_entities/{point_type}/{topic}/train/_point_{point_type}_{k}.json\"\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(point_entities_dict, ensure_ascii=False, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
